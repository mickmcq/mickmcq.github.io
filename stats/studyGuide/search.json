[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Study Guide: Statistics for Informatics",
    "section": "",
    "text": "Preface\nThis is the study guide for a University of Texas at Austin, School of Information course in the undergraduate Informatics major: I306 Statistics for Informatics. It was developed during the Spring 2023 semester, so some of the material is already dated and the instructor will point out that material during class sessions.\nThis study guide supplements our main textbook, Diez, Çetinkaya-Rundel, and Barr (2019) and our secondary textbook, Wickham, Çetinkaya-Rundel, and Grolemund (2023). We will go over the study guide in class instead of slideshows, which will only be used on the first and last day of class.\nThis book was created using the Quarto document publishing system, which is the same system you will use to complete all homework and the take-home final exam in this course. Consequently, the guide describes many details of Quarto, some of which you will need to complete homework and some of which are optional. Quarto is an example of tools for reproducible research and literate programming, two important concepts in science that will be discussed in class sessions. Quarto can be used to produce books, websites, presentations, documents, and more. This is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.\nNote that the display of a Quarto book has three parts: (1) a left sidebar, containing a list of book chapters and a search box that can return results from any chapter, (2) the body, containing the text of the current chapter, and (3) a right sidebar, containing the table of contents for just the current chapter."
  },
  {
    "objectID": "index.html#why-quarto",
    "href": "index.html#why-quarto",
    "title": "Study Guide: Statistics for Informatics",
    "section": "Why Quarto?",
    "text": "Why Quarto?\nQuarto is free and is being actively developed by Posit, the same company developing RStudio. It provides us with an audit trail of our work, and a platform that allows you to think aloud in a sense, adding code and thoughts and pictures to a document that slowly takes shape as you explore data. You gradually sharpen your ideas and the document along with them and, when you are finished, the document contains all the code, thoughts, pictures, equations, and whatever else you wish to share with your audience."
  },
  {
    "objectID": "index.html#why-r-and-rstudio",
    "href": "index.html#why-r-and-rstudio",
    "title": "Study Guide: Statistics for Informatics",
    "section": "Why R and RStudio?",
    "text": "Why R and RStudio?\nR is free and is the leading platform for statisticians in the world today. It is both a language and software that implements that language and is where most statisticians implement their ideas in code. Other statistics platforms such as SPSS are more consumer oriented and don’t receive the amount of debugging and new features that R offers.\nRStudio is free (for our use, there is also a paid version that allows you to collaborate, kind of like Google docs but for data analysis). It is the most prominent IDE for the use of R. It is also an IDE for Python, by the way, although we won’t use Python in this course. It is produced, and the paid version is marketed, by Posit, a company based in Austin that employs as its chief scientist, Hadley Wickham, the leading developer of R software in the world today."
  },
  {
    "objectID": "index.html#why-use-our-textbooks",
    "href": "index.html#why-use-our-textbooks",
    "title": "Study Guide: Statistics for Informatics",
    "section": "Why use our textbooks?",
    "text": "Why use our textbooks?\nBoth our main and secondary textbooks are free, open educational resources. The main textbook covers introductory statistics and the secondary textbook covers R in the context of data science, which is closely related to statistics. I will also mention other valuable texts in the course of the study guide. These are listed in the References section at the end of the book.\n\n\n\n\nDiez, David, Mine Çetinkaya-Rundel, and Cristopher D Barr. 2019. OpenIntro Statistics, Fourth Edition. self-published. https://openintro.org/os.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd ed. O’Reilly Media, Inc. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "week01.html#introducing-r-and-rstudio",
    "href": "week01.html#introducing-r-and-rstudio",
    "title": "1  Introduction to the Course",
    "section": "1.1 Introducing R and RStudio",
    "text": "1.1 Introducing R and RStudio\nFirst we introduce R and RStudio and, finally, Quarto. First, either use the RStudio server at https://rstudio.ischool.utexas.edu or install R and RStudio, in that order. Some people have trouble installing, especially Windows or Mac. Some Mac users were opening the .dmg file for RStudio as a readonly volume, then open the app on that volume. Instead, you have to drag the RStudio icon to the Applications folder and open it from there. The telltale sign of this problem is that you can’t save any files. Windows users have a different problem. Some Windows users try to install RStudio and R on OneDrive. RStudio won’t run from OneDrive and some Windows users can’t tell the difference between installing on a local hard drive and installing in the cloud on OneDrive."
  },
  {
    "objectID": "week01.html#console",
    "href": "week01.html#console",
    "title": "1  Introduction to the Course",
    "section": "1.2 Console",
    "text": "1.2 Console\nThe first thing we can try (after installing if you chose to have it on your machine) is to use the console. By default, that is in the lower left of the RStudio window (you can move everything around, though) and it has a command prompt that looks like &gt;. There enter the following function to verify that you can download R packages, which are collections of functions.\n\nlibrary(MASS)\n\nIf this works, you won’t get any output from the library() function but the command prompt will reappear. That function loads the package from the library into our environment so we can use it in the current session. If you screw around with RStudio and particularly if you follow a lot of hints on Stack Overflow, you may end up with several libraries of packages, all out of sync with each other. If you have trouble loading packages from the library, you may want to call the following function to see how many libraries are on your computer and where they are. This function will return the list of library folders on the server if you call it there.\n\n.libPaths()\n\n[1] \"/Users/mcq/Library/R/x86_64/4.3/library\"                              \n[2] \"/Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library\"\n\n\nThis function returns a list of folders containing libraries, one library per folder. You can then use the terminal or a file explorer outside of R to delete some duplicate packages. The important library for this class is /opt/R/4.3.1/lib/R/library. Only I can install packages there. If you need a package that is not installed, it is best to ask me to install it there. You also probably have a personal library where you can install packages, but on this machine, it is better not to do so.\nAfter loading the relative small package known as MASS, go on to install a package that is actually a large set of packages, collectively known as the Tidyverse. This is a set of packages we will use in our homework.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::select() masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThis takes a while because there are so many packages involved."
  },
  {
    "objectID": "week01.html#the-mtcars-data-set",
    "href": "week01.html#the-mtcars-data-set",
    "title": "1  Introduction to the Course",
    "section": "1.3 The mtcars data set",
    "text": "1.3 The mtcars data set\nThere is a lot of data built into R by default. We look at one such data set, called mtcars. Run a function that looks at the first few lines of the data set, head(mtcars), then checked the help screen for the data set, saying help(mtcars), then produce a linear model of the mpg column being predicted by the disp column, saying summary(lm(mpg~disp,data=mtcars)). This linear model is the heart of regression analysis and one of the main things we’ll learn in this course is how to read the summary.\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nDescriptive statistics deals with numerical analysis of data, such as finding the mean, median etc of values in the dataset. We will find mean, median and mode using the weight column in mtcars with the functions mean() and median(). R doesn’t have a built-in function for mode so we calculate it explicitly.\n\nmean_wt &lt;- mean(mtcars$wt)\nprint(paste0(\"Mean = \", mean_wt))\n\n[1] \"Mean = 3.21725\"\n\n\n\nmedian_wt &lt;- median(mtcars$wt)\nprint(paste0(\"Median = \", median_wt))\n\n[1] \"Median = 3.325\"\n\n\n\nmode_wt &lt;- as.numeric(names(sort(table(mtcars$wt), decreasing = TRUE)[1]))\nprint(paste0(\"Mode = \", mode_wt))\n\n[1] \"Mode = 3.44\"\n\n\nHistograms show distribution of data. Let’s create a histogram using the data in mtcars. First load the dataset using data(mtcars). Then we use the hist() function in R to create a histogram for the mpg variable.\n\ndata(mtcars)\nhist(mtcars$mpg, main = \"Miles per Gallon Distribution\", xlab = \"Miles per Gallon\", ylab = \"Frequency\")\n\n\n\n\nThen we calculate the range of miles per gallon from the histogram using the range() function.\n\nmpg_range &lt;- range(mtcars$mpg)\nmpg_range\n\n[1] 10.4 33.9\n\n\nNow, we will produce a linear model of the mpg column being predicted by the disp column, saying summary(lm(mpg~disp,data=mtcars)). This linear model is the heart of regression analysis and one of the main things we’ll learn in this course is how to read the summary.\n\nsummary(lm(mpg~disp,data=mtcars))\n\n\nCall:\nlm(formula = mpg ~ disp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8922 -2.2022 -0.9631  1.6272  7.2305 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 29.599855   1.229720  24.070  &lt; 2e-16 ***\ndisp        -0.041215   0.004712  -8.747 9.38e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.251 on 30 degrees of freedom\nMultiple R-squared:  0.7183,    Adjusted R-squared:  0.709 \nF-statistic: 76.51 on 1 and 30 DF,  p-value: 9.38e-10"
  },
  {
    "objectID": "week01.html#textbook-data-sets",
    "href": "week01.html#textbook-data-sets",
    "title": "1  Introduction to the Course",
    "section": "1.4 Textbook data sets",
    "text": "1.4 Textbook data sets\nOur textbook, Openintro Stats, contains references to a lot of data sets, many of which I’ve downloaded and put into the folder /home/mm223266/data/. You can just use them from this location or go to the URL https://openintro.org/data, but if you can’t remember that, you can just google openintro stats and navigate to the data sets. Look at the metadata for the loan50 data set, which is used in Chapter 1 of the textbook. You can download it in four different formats, the best of which is the .rda file or R Data file. It’s the best because it preserves the data types, in this case dbl, int, and fctr. If we instead import the .csv file, we have to then specify the data types in R, which is an extra step we’d like to avoid when possible.\nWhen we download a file, R doesn’t know where it is automatically. We do one of three things.\n\nChange R to address the folder where we downloaded it\nMove it to the folder R is currently addressing\nKeep R addressing your homework folder, but reach out for the data sets where I’ve downloaded them (only works if you’re using the RStudio Server).\n\nHow you do this depends on the operating system but, in any operating system we use the following three functions.\n\ngetwd()\n\n[1] \"/Users/mcq/courses/stats/i306studyGuide\"\n\n#. setwd(\"/home/mm223266/i306/\")\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/loan50.rda\"))\n\nThe first function tells us which folder (or directory if you prefer) R is addressing. The second one changes to the folder / directory we would like to use. (I’ve commented it out for this study guide.) The third one addresses the data where I’ve put it but leaves your working directory where it is. This is very convenient because it means that (1) you don’t have to download data sets, and (2) I don’t have to modify your homework file in order to check it.\nMy suggestion is that you create a folder for this class and use the third option. You can make your folder your default in RStudio, using Tools &gt; Global Options &gt; General &gt; Default working directory.\nOnce we load loan50.rda, look at it and try to predict total_credit_limit using annual_income. Keep in mind that, if the file is in the current working directory / folder, R will autocomplete its name when you say lo and then press the tab key (assuming there are no other files starting with the letters lo in the same folder). You just have to enter enough letters to make the name unique before you press the tab key. If nothing happens when you press the tab key, you are either in the wrong folder or you have other files starting with the same letters.\n\nhead(loan50)\n\n  state emp_length term homeownership annual_income verified_income\n1    NJ          3   60          rent         59000    Not Verified\n2    CA         10   36          rent         60000    Not Verified\n3    SC         NA   36      mortgage         75000        Verified\n4    CA          0   36          rent         75000    Not Verified\n5    OH          4   60      mortgage        254000    Not Verified\n6    IN          6   36      mortgage         67000 Source Verified\n  debt_to_income total_credit_limit total_credit_utilized\n1      0.5575254              95131                 32894\n2      1.3056833              51929                 78341\n3      1.0562800             301373                 79221\n4      0.5743467              59890                 43076\n5      0.2381496             422619                 60490\n6      1.0770448             349825                 72162\n  num_cc_carrying_balance       loan_purpose loan_amount grade interest_rate\n1                       8 debt_consolidation       22000     B         10.90\n2                       2        credit_card        6000     B          9.92\n3                      14 debt_consolidation       25000     E         26.30\n4                      10        credit_card        6000     B          9.92\n5                       2   home_improvement       25000     B          9.43\n6                       4   home_improvement        6400     B          9.92\n  public_record_bankrupt loan_status has_second_income total_income\n1                      0     Current             FALSE        59000\n2                      1     Current             FALSE        60000\n3                      0     Current             FALSE        75000\n4                      0     Current             FALSE        75000\n5                      0     Current             FALSE       254000\n6                      0     Current             FALSE        67000\n\nsummary(lm(total_credit_limit~annual_income,data=loan50))\n\n\nCall:\nlm(formula = total_credit_limit ~ annual_income, data = loan50)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-303384  -91959  -38226   89869  239503 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.649e+04  3.156e+04   1.156    0.253    \nannual_income 1.997e+00  3.055e-01   6.535  3.8e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 123100 on 48 degrees of freedom\nMultiple R-squared:  0.4708,    Adjusted R-squared:  0.4598 \nF-statistic: 42.71 on 1 and 48 DF,  p-value: 3.796e-08"
  },
  {
    "objectID": "week01.html#the-migraine-data-set",
    "href": "week01.html#the-migraine-data-set",
    "title": "1  Introduction to the Course",
    "section": "1.5 The migraine data set",
    "text": "1.5 The migraine data set\nNext we load the migraine.rda file from the same place as above and reproduce a figure from the textbook by using the table() function.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/migraine.rda\"))\nhead(migraine)\n\n# A tibble: 6 × 2\n  group     pain_free\n  &lt;fct&gt;     &lt;fct&gt;    \n1 treatment yes      \n2 treatment yes      \n3 treatment yes      \n4 treatment yes      \n5 treatment yes      \n6 treatment yes      \n\nwith(migraine,table(pain_free,group))\n\n         group\npain_free control treatment\n      no       44        33\n      yes       2        10\n\n\nWe could have done this graphically.\n\ntbl &lt;- with(migraine,table(pain_free,group))\nmosaicplot(tbl)\n\n\n\n\nExamine the mosaic plot and the table to see how the sizes of the rectangles compare to the numbers.\nWe could also more precisely reproduce the figure from the textbook by adding row and column sums.\n\naddmargins(tbl)\n\n         group\npain_free control treatment Sum\n      no       44        33  77\n      yes       2        10  12\n      Sum      46        43  89\n\n\nWe could make it prettier by using the pander package.\n\nlibrary(pander)\npander(addmargins(tbl))\n\n\n\n\n\n\n\n\n\n\n \ncontrol\ntreatment\nSum\n\n\n\n\nno\n44\n33\n77\n\n\nyes\n2\n10\n12\n\n\nSum\n46\n43\n89\n\n\n\n\n\npander has a lot of options we could use to make it even prettier, but we’ll skip that for now. There are also a lot of other packages similar to pander for prettifying R output.\nWe could display proportions instead of the raw numbers, but it looks ugly, so we’ll then use the options() function to make it look better.\n\nprop.table(tbl)\n\n         group\npain_free    control  treatment\n      no  0.49438202 0.37078652\n      yes 0.02247191 0.11235955\n\noptions(digits=1)\nprop.table(tbl)\n\n         group\npain_free control treatment\n      no     0.49      0.37\n      yes    0.02      0.11\n\n\nBear in mind that digits=1 is a suggestion to R and that R will determine the exact number of digits on its own, depending on the value of the variables to be displayed."
  },
  {
    "objectID": "week02.html#recap-week-01",
    "href": "week02.html#recap-week-01",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.1 Recap Week 01",
    "text": "2.1 Recap Week 01\nWe looked at four things:\n\nAn example experiment (stents)\ndata basics\nsampling\nexperiments\n\nThe best way to read the textbook is to do some of the exercises. I expect you to spend 6 to 9 hours doing so outside of class each week. This is based on the popular rule of thumb that three class hours implies six to nine study hours.\nLet’s look at some of the Chapter 1 exercises briefly."
  },
  {
    "objectID": "week02.html#textbook-section-1.1-an-example-experiment-stents",
    "href": "week02.html#textbook-section-1.1-an-example-experiment-stents",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.2 Textbook Section 1.1, An Example Experiment (stents)",
    "text": "2.2 Textbook Section 1.1, An Example Experiment (stents)\nHere we looked at treatment groups and control groups.\n\n2.2.1 Textbook Exercise 1.1, the migraine data set\nIn class we loaded the data set and made a contingency table, with which we can answer the four questions in Exercise 1.1.\n\noptions(digits=1)\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/migraine.rda\"))\nhead(migraine)\n\n      group pain_free\n1 treatment       yes\n2 treatment       yes\n3 treatment       yes\n4 treatment       yes\n5 treatment       yes\n6 treatment       yes\n\ntbl &lt;- with(migraine,table(pain_free,group))\ntbl\n\n         group\npain_free control treatment\n      no       44        33\n      yes       2        10\n\n\nThe four questions are:\n\nWhat percent of patients in the treatment group were pain free 24 hours after receiving acupuncture?\nWhat percent were pain free in the control group?\nIn which group did a higher percent of patients become pain free 24 hours after receiving acupuncture?\nYour findings so far might suggest that acupuncture is an effective treatment for migraines for all people who suffer from migraines. However, this is not the only possible conclusion that can be drawn based on your findings so far. What is one other possible explanation for the observed difference between the percentages of patients that are pain free 24 hours after receiving acupuncture in the two groups?\n\nWe can use the tbl object we created to answer these questions.\n\nThe percentage of patients in the treatment group who were pain free was 23.3%.\nThe percentage of patients in the control group who were pain free was 4.3%.\nA higher percent of the treatment group became pain free.\nNot all migraine headaches are alike. It is possible that, due to chance, the patients in the treatment group had less severe migraine headaches that were easier to cure.\n\nExamine the .qmd file that renders into this .html file. You’ll see that, rather than specify the numbers in the answers above, we actually did the calculations inline. What is good about doing this? (Hint: what happens if you add patients to the data set and rerender the document?) What is bad about doing this? (Hint: it looks clumsy and we could just as easily run the calculations in an r chunk, assign the results to object, and name the objects inline.)\nAnother issue is the code for part c. I only accounted for the case where the treatment group has the greater percent. It would be much better to add an else clause to account for the possibility that the answer should be control and an else to account for the two to be equal. The best way to do that is to create a non-echoing chunk and use the results inline. For example, the following chunk only appears in the .qmd file, not the .html file, but its results can be used inline.\nNow we can say that the treatment group had the higher percentage. Only one problem remains and it is a software engineering problem. We haven’t tested the above code on a case where the control group or neither group had the higher percentages. We’ll leave that for now as a more advanced topic."
  },
  {
    "objectID": "week02.html#textbook-section-1.2-data-basics",
    "href": "week02.html#textbook-section-1.2-data-basics",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.3 Textbook Section 1.2, Data Basics",
    "text": "2.3 Textbook Section 1.2, Data Basics\nHere we looked at\n\nobservations (rows), variables (columns), and data matrices (data frames)\ntypes of variables (dbl or continuous, int or discrete, fctr or nominal, and ordered fctr or ordinal)\nrelationships between variables\nexplanatory (x or features or input) and response (y or targets or output) variables\nobservational studies and experiments (and we mentioned an in-between activity called quasi-experiments)\n\n\nData Type terminology: R vs the Textbook\n\n\nR\nOpenintro Stats textbook\n\n\n\n\ndbl, as.numeric()\nnumerical, continuous\n\n\nint, as.integer()\nnumerical, discrete\n\n\nfctr, factor()\ncategorical, nominal\n\n\nord, ordered()\ncategorical, ordinal\n\n\n\nOn the left of the above table, you see how R refers to data types. On the right is how the OpenIntro Stats textbook refers to data types. When you display a tibble (a tibble is a data frame with some extra information) using R, each variable column will be headed with dbl, int, fctr, or ord to indicate the four kinds of numbers. If a variable is not interpreted as a number, R will display chr as an abbreviation of character.\n\n2.3.1 Textbook Exercise 1.7\nWhat were the explanatory and response variables in the migraine study? The group was explanatory and pain_free was the response variable.\n\n\n2.3.2 Textbook Exercise 1.12\nThis is a hard question in two parts.\n\nList the variables used in creating this visualization.\nIndicate whether each variable in the study is numerical or categorical. If numerical, identify as contin- uous or discrete. If categorical, indicate if the variable is ordinal.\n\nThere is actually an r package underlying this question. If you visit https://github.com/dgrtwo/unvotes you will see the data represented as a tibble. If you recall, during week one we said that a tibble is a data frame that behaves well. Among its features is a list of the data types, so you can answer parts a and b by looking at a tibble of the data, where you’ll see that\n\nyear is stored as dbl although it is really discrete and could be stored as int\ncountry is stored as chr which means characters although it is really a nominal factor\npercent_yes is stored as a dbl which is appropriate\nissue is stored as chr although it is really a nominal factor\n\nLater we’ll learn how to produce a visualization like this, although you are welcome to try based on the code at the unvotes website mentioned above. If you want the actual code itself, you can slightly modify the code at https://rpubs.com/minebocek/unvotes to include Mexico."
  },
  {
    "objectID": "week02.html#textbook-section-1.3-sampling",
    "href": "week02.html#textbook-section-1.3-sampling",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.4 Textbook Section 1.3, Sampling",
    "text": "2.4 Textbook Section 1.3, Sampling\nHere we talked about random sampling, stratified sampling, cluster sampling, and observational studies.\n\n2.4.1 Textbook Exercise 1.15, Asthma\n\nWhat is the population of interest and the sample? Note that the population is NOT all asthma sufferers. The population of interest is all asthma patients aged 18-69 who rely on medication for asthma treatment. The sample consists of 600 such patients.\nIs the study generalizable? Can we establish cause and effect? The patients are probably not randomly sampled, so we need to know more to say whether they represent all asthma patients 18–69 who rely on medication. For example, they could all be from a high-pollution city. We would need to know that. The cause and effect determination is easier. An experiment can determine cause and effect, while an observational study only determines association."
  },
  {
    "objectID": "week02.html#textbook-section-1.4-experiments",
    "href": "week02.html#textbook-section-1.4-experiments",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.5 Textbook Section 1.4, Experiments",
    "text": "2.5 Textbook Section 1.4, Experiments\nHere we discussed four issues:\n\ncontrol\nrandomization\nreplication\nblocking\n\n\n2.5.1 Textbook Exercise 1.34, Exercise and mental health\nA researcher is interested in the effects of exercise on mental health and he proposes the following study: Use stratified random sampling to ensure representative proportions of 18-30, 31-40 and 41-55 year olds from the population. Next, randomly assign half the subjects from each age group to exercise twice a week, and instruct the rest not to exercise. Conduct a mental health exam at the beginning and at the end of the study, and compare the results.\n\nWhat type of study is this?\nWhat are the treatment and control groups in this study?\nDoes this study make use of blocking? If so, what is the blocking variable?\nDoes this study make use of blinding?\nComment on whether or not the results of the study can be used to establish a causal relationship between exercise and mental health, and indicate whether or not the conclusions can be generalized to the population at large.\nSuppose you are given the task of determining if this proposed study should get funding. Would you have any reservations about the study proposal?\n\n\n\n2.5.2 Answers\n\nThis is an experiment.\nThe treatment is exercise twice a week and control is no exercise.\nYes, the blocking variable is age.\nNo, the study is not blinded since the patients will know whether or not they are exercising.\nSince this is an experiment, we can make a causal statement. Since the sample is random, the causal statement can be generalized to the population at large. However, we should be cautious about making a causal statement because of a possible placebo effect.\nIt would be very difficult, if not impossible, to successfully conduct this study since randomly sampled people cannot be required to participate in a clinical trial"
  },
  {
    "objectID": "week02.html#textbook-chapter-2-summarizing-data",
    "href": "week02.html#textbook-chapter-2-summarizing-data",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.6 Textbook Chapter 2: Summarizing data",
    "text": "2.6 Textbook Chapter 2: Summarizing data\nThis week, we’ll look at numerical data, categorical data, and a case study.\n\n2.6.1 Numerical data\nThere are graphical and numerical methods for summarizing numerical data, including\n\nscatterplots\ndot plots\nmean\nhistograms\nvariance and standard deviation\nbox plots, quartiles, and the median\nrobust statistics\ncartographic maps and cartograms\n\nWe can draw a scatterplot of two variables of the loan50 data as follows.\n\noptions(repos=structure(c(CRAN=\"https://mirrors.nics.utk.edu/cran/\")))\nlibrary(tidyverse)\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/loan50.rda\"))\nloan50 |&gt;\n  ggplot(aes(annual_income,loan_amount)) +\n  geom_point()\n\n\n\n\nThe above is a very basic scatterplot. Later, we’ll learn to change colors, background, labels, legends, and more. Bear in mind that the scatterplot is meant to compare two numeric variables. You can’t use it for a numeric variable and a categorical variable.\nWe can create a basic dotplot as follows.\n\nloan50 |&gt;\n  ggplot(aes(homeownership)) +\n  geom_dotplot()\n\n\n\n\nA dotplot may be helpful to illustrate a categorical variable, but I seldom use them. Using them in concert with a boxplot may make more sense. We’ll look at boxplots later.\nThe mean is part of a good summary of data. We can find the mean of a variable by saying mean(variable_name) or as part of a summary. For instance\n\nwith(loan50,mean(annual_income))\n\n[1] 86170\n\nwith(loan50,summary(annual_income))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  28800   55750   74000   86170   99500  325000 \n\n\nNotice that the summary() function also gives us the minimum, the first quartile, the median, the 3rd quartile, and the maximum value of the variable, in addition to the mean. We’ll discuss all these statistics in the context of other ways to extract them. The problem with the mean that is demonstrated in the textbook is that two variables may have very different shapes but the same mean. So the textbook then describes histograms, which are a good way to identify the shape of a variable.\n\nloan50 |&gt;\n  ggplot(aes(annual_income)) +\n  geom_histogram()\n\n\n\n\nNotice that the x-axis labels are shown in scientific notation. We can fix this using the scales package. By the way, in case I haven’t mentioned it before, we always refer to the horizontal axis as the x-axis and the vertical axis as the y-axis. This is the default for r and many other languages that create graphical displays.\n\nlibrary(scales)\nloan50 |&gt;\n  ggplot(aes(annual_income)) +\n  geom_histogram() +\n  scale_x_continuous(labels = comma_format())\n\n\n\n\nYou may have noticed that Hadley Wickham, the inventor of the Tidyverse, dislikes the default number of bins in a histogram. So he programmed ggplot() to always show a warning message saying to pick a better number, depending on your data. We can fix that easily with a parameter to geom_histogram(). Then each bin (vertical stripe) will represent a thousand dollars.\n\nloan50 |&gt;\n  ggplot(aes(annual_income)) +\n  geom_histogram(binwidth=1000) +\n  scale_x_continuous(labels = comma_format())\n\n\n\n\nYou may notice that warning messages aren’t dealbreakers. An error message on the other hand, will often stop output dead in its tracks.\nThe next concepts covered in the textbook are variance and standard deviation. We can calculate them as follows. When you do this, you may notice that sd() is the square root of var(). Why would you prefer one over the other? Usually you use sd() because it’s in the same units as the data, dollars in the following case, unlike var(), which is in squared units, squared dollars in the following case.\n\nwith(loan50,var(annual_income))\n\n[1] 3e+09\n\nwith(loan50,sd(annual_income))\n\n[1] 57566\n\n\nTogether, the mean and standard deviation are often a good, yet compact, description of a data set.\nYou may want to find the means of all the columns in a data set. If you try to do that with the colMeans() function, you’ll get an error message as follows. (Actually, I’ve disabled the following code chunk by saying #| eval: false in the .qmd file because otherwise the rendering would halt.)\n\ncolMeans(loan50)\n\nThe remedy is to use a logical function to identify only the numeric columns.\n\ncolMeans(loan50[sapply(loan50, is.numeric)])\n\n             emp_length                    term           annual_income \n                     NA                   4e+01                   9e+04 \n         debt_to_income      total_credit_limit   total_credit_utilized \n                  7e-01                   2e+05                   6e+04 \nnum_cc_carrying_balance             loan_amount           interest_rate \n                  5e+00                   2e+04                   1e+01 \n public_record_bankrupt            total_income \n                  8e-02                   1e+05 \n\n\nThis is okay, but the results are in scientific notation. You can use the format() function to supress scientific notation as follows.\n\nformat(colMeans(loan50[sapply(loan50, is.numeric)]),scientific=FALSE)\n\n             emp_length                    term           annual_income \n            \"       NA\"             \"    42.72\"             \" 86170.00\" \n         debt_to_income      total_credit_limit   total_credit_utilized \n            \"     0.72\"             \"208546.64\"             \" 61546.54\" \nnum_cc_carrying_balance             loan_amount           interest_rate \n            \"     5.06\"             \" 17083.00\"             \"    11.57\" \n public_record_bankrupt            total_income \n            \"     0.08\"             \"105220.56\" \n\n\nNotice that you often wrap a function inside another function. The only problem is that it’s easy to lose track of all the parentheses.\nThe next topic in the textbook is the box plot. This also gives an opportunity to talk about the quartiles and the median. We can display a box plot as follows.\n\nloan50 |&gt;\n  ggplot(aes(annual_income)) +\n  geom_boxplot() +\n  scale_x_continuous(labels = comma_format())\n\n\n\n\nThe thick line in the middle of the box is the median, the middle value of the data set. The box itself is bound by the first and third quartiles, known as hinges. The full name of this construct is actually a box and whiskers plot and the lines extending horizontally from the box are called whiskers. The upper whisker extends from the hinge to the largest value no further than 1.5 * IQR from the hinge (where IQR is the inter-quartile range, or distance between the first and third quartiles). The lower whisker extends from the hinge to the smallest value at most 1.5 * IQR of the hinge. Data beyond the end of the whiskers are called “outlying” points and are plotted individually.\nWe often want to create several box plots and compare them. This is easy to do as follows.\n\nloan50 |&gt;\n  ggplot(aes(annual_income,homeownership)) +\n  geom_boxplot() +\n  scale_x_continuous(labels = comma_format())\n\n\n\n\nThe textbook’s next topic is Robust Statistics and we’re going to pretty much skip that for now, except to say that outliers, as shown in the textbook, can affect the value of some statistics more than others. The mean and median are a good example. The median is much more robust to outliers than is the mean, which can be dragged way up or down by the presence of just one or a few outliers, whereas the median can not. As a kind of thought experiment, consider the following data set and the addition of an outlier and the effect of the outlier on the mean and median.\n\nx&lt;-c(2,3,3,3,4,4,4,5,5,6,6)\nmean(x)\n\n[1] 4\n\nmedian(x)\n\n[1] 4\n\ny&lt;-c(2,3,3,3,4,4,4,5,5,6,6,800)\nmean(y)\n\n[1] 70\n\nmedian(y)\n\n[1] 4"
  },
  {
    "objectID": "week02.html#uncertainty-in-summaries",
    "href": "week02.html#uncertainty-in-summaries",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.7 Uncertainty in summaries",
    "text": "2.7 Uncertainty in summaries\nHow can we portray our uncertainty about estimates of parameters? Consider two different vectors:\n\nu &lt;- c(1,2,3,4,5,6,7,8,9)\nv &lt;- c(3,4,5,5,5,5,5,6,7)\n\nSuppose that these are samples from two different populations. For both of these vectors, the best estimate is 5. Both the mean and median are 5 in both cases. But when estimating, we’re much more sure of our estimate in the case of v than u. We quantify that with variance or its square root, standard deviation. But those numbers don’t mean much to most people. It’s easier to portray uncertainty graphically than numerically. The typical ways to do so are to make boxplots, violin plots, or barcharts with error bars if we’re comparing two or more groups. If we are estimating a continuous variable, say \\(y\\), at many different values of \\(x\\), we can show our uncertainty by estimating standard deviation for portions of the data and putting them together as follows in an example from the R Graph Gallery.\n\n#. load packages from the library\nlibrary(hrbrthemes)\n\n#. Create synthetic data\ndf &lt;- data.frame(\n  x = 1:100 + rnorm(100,sd=9),\n  y = 1:100 + rnorm(100,sd=16)\n)\n\n#. linear trend + confidence interval\nggplot(df, aes(x=x, y=y)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  theme_ipsum()\n\n\n\n\nNotice that the confidence interval is narrower at the center than at the extremes. Why? It’s because we’re using more data to estimate at the center and have less uncertainty at the center than at the ends, where the data is truncated.\nSuppose we’re doing something even more sophisticated, such as comparing groups of continuous \\(y\\) variables. A ridgeline plot is a popular solution to show variability, which is often similar to uncertainty, as shown in this example from the R Graph Gallery.\n\n#. packages\nlibrary(ggridges)\nlibrary(viridis)\n\n#. Plot\nggplot(lincoln_weather, aes(x = `Mean Temperature [F]`, y = `Month`, fill = ..x..)) +\n  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +\n  scale_fill_viridis(name = \"Temp. [F]\", option = \"C\") +\n  labs(title = 'Temperatures in Lincoln NE in 2016') +\n  theme_ipsum() +\n    theme(\n      legend.position=\"none\",\n      panel.spacing = unit(0.1, \"lines\"),\n      strip.text.x = element_text(size = 8)\n    )\n\nWarning: The dot-dot notation (`..x..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(x)` instead.\n\n\n\n\n\nNot only are the temperatures higher in the summer than in the winter, they are less variable, so we’re more certain of what the temperature might be on any given day in summer."
  },
  {
    "objectID": "week02.html#categorical-data",
    "href": "week02.html#categorical-data",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.8 Categorical Data",
    "text": "2.8 Categorical Data\nWe’ve already seen contingency tables and how to manipulate them, which are introduced in more detail in this section. We’ve also seen a mosaic plot. Another kind of plot introduced in this section is the bar plot. We’ll examine each of these.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/migraine.rda\"))\ntbl &lt;- with(migraine,table(pain_free,group))\ntbl &lt;- addmargins(tbl)\nlibrary(kableExtra)\ntbl |&gt;\n  kbl() |&gt;\n  kable_classic(full_width=F) |&gt;\n  row_spec(3, color = \"white\", background = \"#AAAAAA\") |&gt;\n  column_spec(4, color = \"white\", background = \"#AAAAAA\") |&gt;\n  column_spec(1, color = \"black\", background = \"white\")\n\n\n\n\n\ncontrol\ntreatment\nSum\n\n\n\n\nno\n44\n33\n77\n\n\nyes\n2\n10\n12\n\n\nSum\n46\n43\n89\n\n\n\n\n\n\n\nAs before, we can create a mosaic plot.\n\ntbl &lt;- with(migraine,table(pain_free,group))\nmosaicplot(tbl)\n\n\n\n\nWe can also create bar plots.\n\nloan50 |&gt;\n  ggplot(aes(homeownership)) +\n    geom_bar()\n\n\n\n\n\nloan50 |&gt;\n  ggplot(aes(homeownership,fill=loan_purpose)) +\n    geom_bar() +\n    scale_fill_brewer()\n\n\n\n\nThe above looks better but is misleading because it implies an ordinal relationship between the loan purposes and there is no such relationship. We would be better of specifying that the loan_purpose variable is qualitative.\n\nloan50 |&gt;\n  ggplot(aes(homeownership,fill=loan_purpose)) +\n    geom_bar() +\n    scale_fill_brewer(type=\"qual\",palette=\"Set1\")\n\n\n\n\nI find the above palette to be ugly but several others are available. Google colorbrewer for more info. By the way, these colors have been extensively psychologically tested to verify that people can easily distinguish between them. I’m uncertain about colorblind people because the most prevalent form of color blindness is red-green."
  },
  {
    "objectID": "week02.html#thursdays-class",
    "href": "week02.html#thursdays-class",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.9 Thursday’s class",
    "text": "2.9 Thursday’s class\n\n2.9.1 Statistical summary tools\n\nlibrary(ISLR2)\ndata(Auto)\nAuto &lt;- na.omit(Auto)\nwith(Auto,cylinders&lt;-as.factor(cylinders))\n\nHere are some statistical summary questions we can answer about the Auto data set.\n\nWhat is the range of each quantitative predictor? You can answer this using the range() function.\n\n\nwith(Auto,range(mpg))\n\n[1]  9 47\n\nwith(Auto,range(displacement))\n\n[1]  68 455\n\nsapply(Auto[,c(1,3:7)],range)\n\n     mpg displacement horsepower weight acceleration year\n[1,]   9           68         46   1613            8   70\n[2,]  47          455        230   5140           25   82\n\nsapply(Auto[,sapply(Auto,is.numeric)],range)\n\n     mpg cylinders displacement horsepower weight acceleration year origin\n[1,]   9         3           68         46   1613            8   70      1\n[2,]  47         8          455        230   5140           25   82      3\n\n\n\nWhat is the mean and standard deviation of each quantitative predictor?\n\n\nsapply(Auto[,c(1,3:7)],mean)\n\n         mpg displacement   horsepower       weight acceleration         year \n          23          194          104         2978           16           76 \n\nsapply(Auto[,c(1,3:7)],sd)\n\n         mpg displacement   horsepower       weight acceleration         year \n           8          105           38          849            3            4 \n\nas.data.frame(t(sapply(Auto[,c(1,3:7)],function(bla) list(means=mean(bla),sds=sd(bla),ranges=range(bla)))))\n\n             means sds     ranges\nmpg             23   8      9, 47\ndisplacement   194 105    68, 455\nhorsepower     104  38    46, 230\nweight        2978 849 1613, 5140\nacceleration    16   3      8, 25\nyear            76   4     70, 82\n\n\n\nNow remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\n\n\nas.data.frame(t(sapply(Auto[-10:-85,c(1,3:7)],function(bla) list(means=mean(bla),sds=sd(bla),ranges=range(bla)))))\n\n             means sds     ranges\nmpg             24   8     11, 47\ndisplacement   187 100    68, 455\nhorsepower     101  36    46, 230\nweight        2936 811 1649, 4997\nacceleration    16   3      8, 25\nyear            77   3     70, 82\n\n\n\n\n2.9.2 Numerical summary shortcuts\nSeveral functions allow you to check up on many columns at once. This is especially helpful in examining dataframes with large numbers of columns, such as amesHousing2011.csv.\n\ndf &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/amesHousing2011.csv\"))\nstr(df)\n\nspc_tbl_ [2,925 × 82] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Order        : num [1:2925] 1498 2738 2446 2667 2451 ...\n $ PID          : chr [1:2925] \"0908154080\" \"0905427030\" \"0528320060\" \"0902400110\" ...\n $ MSSubClass   : chr [1:2925] \"020\" \"075\" \"060\" \"075\" ...\n $ MSZoning     : chr [1:2925] \"RL\" \"RL\" \"RL\" \"RM\" ...\n $ LotFrontage  : num [1:2925] 123 60 118 90 114 87 NA 60 60 47 ...\n $ LotArea      : num [1:2925] 47007 19800 35760 22950 17242 ...\n $ Street       : chr [1:2925] \"Pave\" \"Pave\" \"Pave\" \"Pave\" ...\n $ Alley        : chr [1:2925] NA NA NA NA ...\n $ LotShape     : chr [1:2925] \"IR1\" \"Reg\" \"IR1\" \"IR2\" ...\n $ LandContour  : chr [1:2925] \"Lvl\" \"Lvl\" \"Lvl\" \"Lvl\" ...\n $ Utilities    : chr [1:2925] \"AllPub\" \"AllPub\" \"AllPub\" \"AllPub\" ...\n $ LotConfig    : chr [1:2925] \"Inside\" \"Inside\" \"CulDSac\" \"Inside\" ...\n $ LandSlope    : chr [1:2925] \"Gtl\" \"Gtl\" \"Gtl\" \"Gtl\" ...\n $ Neighborhood : chr [1:2925] \"Edwards\" \"Edwards\" \"NoRidge\" \"OldTown\" ...\n $ Condition1   : chr [1:2925] \"Norm\" \"Norm\" \"Norm\" \"Artery\" ...\n $ Condition2   : chr [1:2925] \"Norm\" \"Norm\" \"Norm\" \"Norm\" ...\n $ BldgType     : chr [1:2925] \"1Fam\" \"1Fam\" \"1Fam\" \"1Fam\" ...\n $ HouseStyle   : chr [1:2925] \"1Story\" \"2.5Unf\" \"2Story\" \"2.5Fin\" ...\n $ OverallQual  : num [1:2925] 5 6 10 10 9 7 8 6 10 8 ...\n $ OverallCond  : num [1:2925] 7 8 5 9 5 9 9 7 5 5 ...\n $ YearBuilt    : num [1:2925] 1959 1935 1995 1892 1993 ...\n $ YearRemod/Add: num [1:2925] 1996 1990 1996 1993 1994 ...\n $ RoofStyle    : chr [1:2925] \"Gable\" \"Gable\" \"Hip\" \"Gable\" ...\n $ RoofMatl     : chr [1:2925] \"CompShg\" \"CompShg\" \"CompShg\" \"WdShngl\" ...\n $ Exterior1st  : chr [1:2925] \"Plywood\" \"BrkFace\" \"HdBoard\" \"Wd Sdng\" ...\n $ Exterior2nd  : chr [1:2925] \"Plywood\" \"Wd Sdng\" \"HdBoard\" \"Wd Sdng\" ...\n $ MasVnrType   : chr [1:2925] \"None\" \"None\" \"BrkFace\" \"None\" ...\n $ MasVnrArea   : num [1:2925] 0 0 1378 0 738 ...\n $ ExterQual    : chr [1:2925] \"TA\" \"TA\" \"Gd\" \"Gd\" ...\n $ ExterCond    : chr [1:2925] \"TA\" \"TA\" \"Gd\" \"Gd\" ...\n $ Foundation   : chr [1:2925] \"Slab\" \"BrkTil\" \"PConc\" \"BrkTil\" ...\n $ BsmtQual     : chr [1:2925] NA \"TA\" \"Ex\" \"TA\" ...\n $ BsmtCond     : chr [1:2925] NA \"TA\" \"TA\" \"TA\" ...\n $ BsmtExposure : chr [1:2925] NA \"No\" \"Gd\" \"Mn\" ...\n $ BsmtFinType1 : chr [1:2925] NA \"Rec\" \"GLQ\" \"Unf\" ...\n $ BsmtFinSF1   : num [1:2925] 0 425 1387 0 292 ...\n $ BsmtFinType2 : chr [1:2925] NA \"Unf\" \"Unf\" \"Unf\" ...\n $ BsmtFinSF2   : num [1:2925] 0 0 0 0 1393 ...\n $ BsmtUnfSF    : num [1:2925] 0 1411 543 1107 48 ...\n $ TotalBsmtSF  : num [1:2925] 0 1836 1930 1107 1733 ...\n $ Heating      : chr [1:2925] \"GasA\" \"GasA\" \"GasA\" \"GasA\" ...\n $ HeatingQC    : chr [1:2925] \"TA\" \"Gd\" \"Ex\" \"Ex\" ...\n $ CentralAir   : chr [1:2925] \"Y\" \"Y\" \"Y\" \"Y\" ...\n $ Electrical   : chr [1:2925] \"SBrkr\" \"SBrkr\" \"SBrkr\" \"SBrkr\" ...\n $ 1stFlrSF     : num [1:2925] 3820 1836 1831 1518 1933 ...\n $ 2ndFlrSF     : num [1:2925] 0 1836 1796 1518 1567 ...\n $ LowQualFinSF : num [1:2925] 0 0 0 572 0 0 0 515 0 0 ...\n $ GrLivArea    : num [1:2925] 3820 3672 3627 3608 3500 ...\n $ BsmtFullBath : num [1:2925] NA 0 1 0 1 0 0 0 0 1 ...\n $ BsmtHalfBath : num [1:2925] NA 0 0 0 0 0 0 0 0 0 ...\n $ FullBath     : num [1:2925] 3 3 3 2 3 3 3 2 3 3 ...\n $ HalfBath     : num [1:2925] 1 1 1 1 1 0 1 0 1 1 ...\n $ BedroomAbvGr : num [1:2925] 5 5 4 4 4 3 4 8 5 4 ...\n $ KitchenAbvGr : num [1:2925] 1 1 1 1 1 1 1 2 1 1 ...\n $ KitchenQual  : chr [1:2925] \"Ex\" \"Gd\" \"Gd\" \"Ex\" ...\n $ TotRmsAbvGrd : num [1:2925] 11 7 10 12 11 10 11 14 10 12 ...\n $ Functional   : chr [1:2925] \"Typ\" \"Typ\" \"Typ\" \"Typ\" ...\n $ Fireplaces   : num [1:2925] 2 2 1 2 1 1 2 0 1 1 ...\n $ FireplaceQu  : chr [1:2925] \"Gd\" \"Gd\" \"TA\" \"TA\" ...\n $ GarageType   : chr [1:2925] \"Attchd\" \"Detchd\" \"Attchd\" \"Detchd\" ...\n $ GarageYrBlt  : num [1:2925] 1959 1993 1995 1993 1993 ...\n $ GarageFinish : chr [1:2925] \"Unf\" \"Unf\" \"Fin\" \"Unf\" ...\n $ GarageCars   : num [1:2925] 2 2 3 3 3 3 3 0 3 3 ...\n $ GarageArea   : num [1:2925] 624 836 807 840 959 ...\n $ GarageQual   : chr [1:2925] \"TA\" \"TA\" \"TA\" \"Ex\" ...\n $ GarageCond   : chr [1:2925] \"TA\" \"TA\" \"TA\" \"TA\" ...\n $ PavedDrive   : chr [1:2925] \"Y\" \"Y\" \"Y\" \"Y\" ...\n $ WoodDeckSF   : num [1:2925] 0 684 361 0 870 302 314 0 204 503 ...\n $ OpenPorchSF  : num [1:2925] 372 80 76 260 86 0 12 110 34 36 ...\n $ EnclosedPorch: num [1:2925] 0 32 0 0 0 0 0 0 0 0 ...\n $ 3SsnPorch    : num [1:2925] 0 0 0 0 0 0 0 0 0 0 ...\n $ ScreenPorch  : num [1:2925] 0 0 0 410 210 0 0 0 0 210 ...\n $ PoolArea     : num [1:2925] 0 0 0 0 0 0 0 0 0 0 ...\n $ PoolQC       : chr [1:2925] NA NA NA NA ...\n $ Fence        : chr [1:2925] NA NA NA \"GdPrv\" ...\n $ MiscFeature  : chr [1:2925] NA NA NA NA ...\n $ MiscVal      : num [1:2925] 0 0 0 0 0 0 0 0 0 0 ...\n $ MoSold       : num [1:2925] 7 12 7 6 5 5 5 3 9 6 ...\n $ YrSold       : num [1:2925] 2008 2006 2006 2006 2006 ...\n $ SaleType     : chr [1:2925] \"WD\" \"WD\" \"WD\" \"WD\" ...\n $ SaleCondition: chr [1:2925] \"Normal\" \"Normal\" \"Normal\" \"Normal\" ...\n $ SalePrice    : num [1:2925] 284700 415000 625000 475000 584500 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Order = col_double(),\n  ..   PID = col_character(),\n  ..   MSSubClass = col_character(),\n  ..   MSZoning = col_character(),\n  ..   LotFrontage = col_double(),\n  ..   LotArea = col_double(),\n  ..   Street = col_character(),\n  ..   Alley = col_character(),\n  ..   LotShape = col_character(),\n  ..   LandContour = col_character(),\n  ..   Utilities = col_character(),\n  ..   LotConfig = col_character(),\n  ..   LandSlope = col_character(),\n  ..   Neighborhood = col_character(),\n  ..   Condition1 = col_character(),\n  ..   Condition2 = col_character(),\n  ..   BldgType = col_character(),\n  ..   HouseStyle = col_character(),\n  ..   OverallQual = col_double(),\n  ..   OverallCond = col_double(),\n  ..   YearBuilt = col_double(),\n  ..   `YearRemod/Add` = col_double(),\n  ..   RoofStyle = col_character(),\n  ..   RoofMatl = col_character(),\n  ..   Exterior1st = col_character(),\n  ..   Exterior2nd = col_character(),\n  ..   MasVnrType = col_character(),\n  ..   MasVnrArea = col_double(),\n  ..   ExterQual = col_character(),\n  ..   ExterCond = col_character(),\n  ..   Foundation = col_character(),\n  ..   BsmtQual = col_character(),\n  ..   BsmtCond = col_character(),\n  ..   BsmtExposure = col_character(),\n  ..   BsmtFinType1 = col_character(),\n  ..   BsmtFinSF1 = col_double(),\n  ..   BsmtFinType2 = col_character(),\n  ..   BsmtFinSF2 = col_double(),\n  ..   BsmtUnfSF = col_double(),\n  ..   TotalBsmtSF = col_double(),\n  ..   Heating = col_character(),\n  ..   HeatingQC = col_character(),\n  ..   CentralAir = col_character(),\n  ..   Electrical = col_character(),\n  ..   `1stFlrSF` = col_double(),\n  ..   `2ndFlrSF` = col_double(),\n  ..   LowQualFinSF = col_double(),\n  ..   GrLivArea = col_double(),\n  ..   BsmtFullBath = col_double(),\n  ..   BsmtHalfBath = col_double(),\n  ..   FullBath = col_double(),\n  ..   HalfBath = col_double(),\n  ..   BedroomAbvGr = col_double(),\n  ..   KitchenAbvGr = col_double(),\n  ..   KitchenQual = col_character(),\n  ..   TotRmsAbvGrd = col_double(),\n  ..   Functional = col_character(),\n  ..   Fireplaces = col_double(),\n  ..   FireplaceQu = col_character(),\n  ..   GarageType = col_character(),\n  ..   GarageYrBlt = col_double(),\n  ..   GarageFinish = col_character(),\n  ..   GarageCars = col_double(),\n  ..   GarageArea = col_double(),\n  ..   GarageQual = col_character(),\n  ..   GarageCond = col_character(),\n  ..   PavedDrive = col_character(),\n  ..   WoodDeckSF = col_double(),\n  ..   OpenPorchSF = col_double(),\n  ..   EnclosedPorch = col_double(),\n  ..   `3SsnPorch` = col_double(),\n  ..   ScreenPorch = col_double(),\n  ..   PoolArea = col_double(),\n  ..   PoolQC = col_character(),\n  ..   Fence = col_character(),\n  ..   MiscFeature = col_character(),\n  ..   MiscVal = col_double(),\n  ..   MoSold = col_double(),\n  ..   YrSold = col_double(),\n  ..   SaleType = col_character(),\n  ..   SaleCondition = col_character(),\n  ..   SalePrice = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\ndf |&gt; select(where(is.character)) |&gt; map ( table )\n\n$PID\n\n0526301100 0526302030 0526302040 0526302110 0526302120 0526303060 0526350040 \n         1          1          1          1          1          1          1 \n0526351010 0526351030 0526351100 0526352080 0526352090 0526353030 0526353050 \n         1          1          1          1          1          1          1 \n0526354020 0526354070 0526355080 0526355170 0526355190 0527105010 0527105030 \n         1          1          1          1          1          1          1 \n0527105050 0527105060 0527105070 0527105130 0527105140 0527106010 0527106050 \n         1          1          1          1          1          1          1 \n0527106140 0527106150 0527107010 0527107020 0527107030 0527107040 0527107090 \n         1          1          1          1          1          1          1 \n0527107120 0527107130 0527107210 0527107240 0527108010 0527108020 0527108030 \n         1          1          1          1          1          1          1 \n0527108040 0527108060 0527108070 0527108090 0527110020 0527110080 0527110120 \n         1          1          1          1          1          1          1 \n0527110130 0527126030 0527127100 0527127140 0527127150 0527131030 0527131110 \n         1          1          1          1          1          1          1 \n0527132090 0527140090 0527145080 0527145090 0527146010 0527146030 0527146135 \n         1          1          1          1          1          1          1 \n0527158020 0527158090 0527161010 0527161040 0527161090 0527162080 0527162090 \n         1          1          1          1          1          1          1 \n0527162110 0527162120 0527162130 0527162140 0527162180 0527163010 0527163020 \n         1          1          1          1          1          1          1 \n0527163040 0527163070 0527163080 0527163100 0527163130 0527164020 0527164060 \n         1          1          1          1          1          1          1 \n0527164090 0527164120 0527165010 0527165020 0527165100 0527165130 0527165170 \n         1          1          1          1          1          1          1 \n0527165230 0527166010 0527166030 0527166040 0527175130 0527180040 0527180100 \n         1          1          1          1          1          1          1 \n0527182020 0527182040 0527182110 0527182170 0527182190 0527183030 0527183060 \n         1          1          1          1          1          1          1 \n0527184020 0527184110 0527190050 0527190220 0527208010 0527210030 0527210040 \n         1          1          1          1          1          1          1 \n0527210050 0527210060 0527212030 0527212040 0527212050 0527212060 0527214030 \n         1          1          1          1          1          1          1 \n0527214050 0527214060 0527216010 0527216050 0527216070 0527216080 0527225035 \n         1          1          1          1          1          1          1 \n0527226010 0527226020 0527226040 0527250040 0527252050 0527252070 0527252080 \n         1          1          1          1          1          1          1 \n0527252090 0527254020 0527254070 0527256030 0527256040 0527256120 0527258010 \n         1          1          1          1          1          1          1 \n0527258020 0527276040 0527276150 0527276160 0527301010 0527301080 0527301170 \n         1          1          1          1          1          1          1 \n0527301245 0527301280 0527301320 0527302020 0527302050 0527302060 0527302070 \n         1          1          1          1          1          1          1 \n0527302080 0527302090 0527302110 0527302175 0527302185 0527302210 0527325070 \n         1          1          1          1          1          1          1 \n0527325090 0527325110 0527325160 0527325240 0527326040 0527326130 0527326150 \n         1          1          1          1          1          1          1 \n0527327050 0527327080 0527328010 0527328020 0527328050 0527352150 0527353020 \n         1          1          1          1          1          1          1 \n0527353060 0527353080 0527354040 0527354050 0527354100 0527354160 0527354200 \n         1          1          1          1          1          1          1 \n0527355060 0527355150 0527355280 0527356020 0527356040 0527356050 0527356060 \n         1          1          1          1          1          1          1 \n0527357020 0527357110 0527357180 0527358060 0527358090 0527358120 0527358140 \n         1          1          1          1          1          1          1 \n0527358200 0527359010 0527359030 0527359050 0527359070 0527359080 0527359160 \n         1          1          1          1          1          1          1 \n0527359180 0527364030 0527365010 0527365030 0527366030 0527368010 0527368020 \n         1          1          1          1          1          1          1 \n0527375100 0527375160 0527375180 0527375210 0527376090 0527376100 0527376110 \n         1          1          1          1          1          1          1 \n0527377030 0527377110 0527378010 0527378020 0527378140 0527380240 0527401130 \n         1          1          1          1          1          1          1 \n0527401160 0527402060 0527402090 0527402130 0527402150 0527402200 0527402210 \n         1          1          1          1          1          1          1 \n0527402220 0527402240 0527402250 0527402380 0527403020 0527403040 0527403120 \n         1          1          1          1          1          1          1 \n0527403310 0527403360 0527404020 0527404030 0527404050 0527404100 0527404120 \n         1          1          1          1          1          1          1 \n0527404140 0527404150 0527404180 0527405130 0527405160 0527405180 0527425025 \n         1          1          1          1          1          1          1 \n0527425035 0527425060 0527425090 0527425110 0527425140 0527427090 0527427110 \n         1          1          1          1          1          1          1 \n0527427160 0527427200 0527427210 0527427230 0527450010 0527450030 0527450080 \n         1          1          1          1          1          1          1 \n0527450110 0527450150 0527450210 0527450220 0527450250 0527450280 0527450460 \n         1          1          1          1          1          1          1 \n0527450520 0527451020 0527451060 0527451110 0527451170 0527451180 0527451200 \n         1          1          1          1          1          1          1 \n0527451290 0527451320 0527451330 0527451380 0527451400 0527451410 0527451450 \n         1          1          1          1          1          1          1 \n0527451460 0527451520 0527451540 0527451550 0527451640 0527451650 0527452060 \n         1          1          1          1          1          1          1 \n0527452070 0527452100 0527452150 0527452190 0527452290 0527452310 0527453010 \n         1          1          1          1          1          1          1 \n0527453050 0527453060 0527453070 0527453130 0527453140 0527453150 0527453160 \n         1          1          1          1          1          1          1 \n0527454050 0527454120 0527454130 0527454200 0527455030 0527455050 0527455060 \n         1          1          1          1          1          1          1 \n0527455080 0527455090 0527455100 0527455240 0527455250 0527455270 0527455280 \n         1          1          1          1          1          1          1 \n0528102010 0528102030 0528102040 0528102050 0528102080 0528102090 0528102100 \n         1          1          1          1          1          1          1 \n0528102110 0528102120 0528102130 0528102140 0528104010 0528104050 0528104070 \n         1          1          1          1          1          1          1 \n0528104080 0528106010 0528106020 0528106040 0528106050 0528106060 0528106070 \n         1          1          1          1          1          1          1 \n0528106110 0528106120 0528108010 0528108020 0528108040 0528108070 0528108120 \n         1          1          1          1          1          1          1 \n0528108130 0528108140 0528108150 0528108160 0528110010 0528110020 0528110040 \n         1          1          1          1          1          1          1 \n0528110050 0528110070 0528110080 0528110090 0528110100 0528110110 0528112010 \n         1          1          1          1          1          1          1 \n0528112020 0528112040 0528112150 0528114010 0528114040 0528114050 0528116010 \n         1          1          1          1          1          1          1 \n0528116030 0528118010 0528118020 0528118030 0528118040 0528118050 0528118060 \n         1          1          1          1          1          1          1 \n0528118090 0528118110 0528120010 0528120020 0528120030 0528120060 0528120090 \n         1          1          1          1          1          1          1 \n0528120100 0528120110 0528120120 0528120130 0528120140 0528120150 0528120160 \n         1          1          1          1          1          1          1 \n0528120170 0528138010 0528138020 0528138030 0528138060 0528142010 0528142020 \n         1          1          1          1          1          1          1 \n0528142040 0528142050 0528142060 0528142070 0528142090 0528142110 0528142130 \n         1          1          1          1          1          1          1 \n0528142140 0528142150 0528144020 0528144030 0528144050 0528150040 0528150070 \n         1          1          1          1          1          1          1 \n0528150080 0528150090 0528150110 0528150120 0528164040 0528164060 0528166060 \n         1          1          1          1          1          1          1 \n0528166090 0528166120 0528166150 0528168030 0528168040 0528170040 0528170070 \n         1          1          1          1          1          1          1 \n0528172020 0528172030 0528172050 0528172060 0528172070 0528172075 0528172080 \n         1          1          1          1          1          1          1 \n0528172150 0528174010 0528174020 0528174030 0528174040 0528174050 0528174060 \n         1          1          1          1          1          1          1 \n0528174070 0528174080 0528175010 0528175050 0528175060 0528176010 0528176030 \n         1          1          1          1          1          1          1 \n0528176070 0528178010 0528178070 0528178090 0528180070 0528180080 0528180110 \n         1          1          1          1          1          1          1 \n0528180120 0528180130 0528180140 0528181010 0528181030 0528181040 0528181050 \n         1          1          1          1          1          1          1 \n0528181060 0528181070 0528181080 0528186010 0528186030 0528186050 0528186080 \n         1          1          1          1          1          1          1 \n0528186090 0528186100 0528186110 0528186120 0528186130 0528186160 0528186170 \n         1          1          1          1          1          1          1 \n0528186180 0528186190 0528188020 0528188040 0528188060 0528188090 0528188120 \n         1          1          1          1          1          1          1 \n0528188150 0528188160 0528190010 0528218010 0528218020 0528218030 0528218050 \n         1          1          1          1          1          1          1 \n0528218060 0528218080 0528218090 0528218100 0528218130 0528218140 0528218150 \n         1          1          1          1          1          1          1 \n0528220040 0528221010 0528221030 0528221040 0528221050 0528221060 0528221070 \n         1          1          1          1          1          1          1 \n0528221090 0528221100 0528222010 0528222020 0528222030 0528222040 0528222050 \n         1          1          1          1          1          1          1 \n0528222060 0528222080 0528222090 0528228270 0528228275 0528228280 0528228285 \n         1          1          1          1          1          1          1 \n0528228290 0528228295 0528228325 0528228340 0528228345 0528228360 0528228375 \n         1          1          1          1          1          1          1 \n0528228405 0528228415 0528228430 0528228435 0528228440 0528228445 0528228450 \n         1          1          1          1          1          1          1 \n0528228455 0528228460 0528228465 0528228540 0528228545 0528228550 0528228555 \n         1          1          1          1          1          1          1 \n0528228565 0528228575 0528228580 0528231010 0528231020 0528231030 0528231050 \n         1          1          1          1          1          1          1 \n0528235000 0528235010 0528235020 0528235050 0528235090 0528235120 0528235130 \n         1          1          1          1          1          1          1 \n0528235160 0528235170 0528235180 0528235190 0528235200 0528240020 0528240030 \n         1          1          1          1          1          1          1 \n0528240040 0528240050 0528240060 0528240070 0528240080 0528240130 0528240150 \n         1          1          1          1          1          1          1 \n0528250010 0528250020 0528250030 0528250040 0528250060 0528250070 0528250100 \n         1          1          1          1          1          1          1 \n0528253010 0528253020 0528275035 0528275060 0528275070 0528275080 0528275110 \n         1          1          1          1          1          1          1 \n0528275160 0528280100 0528280110 0528280130 0528280150 0528280180 0528280230 \n         1          1          1          1          1          1          1 \n0528290010 0528290030 0528290060 0528290090 0528290120 0528290140 0528290170 \n         1          1          1          1          1          1          1 \n0528290180 0528290190 0528292020 0528292030 0528292040 0528292060 0528292070 \n         1          1          1          1          1          1          1 \n0528292080 0528294010 0528294050 0528294060 0528294070 0528315030 0528315060 \n         1          1          1          1          1          1          1 \n0528315070 0528315080 0528315090 0528315110 0528320060 0528321010 0528322020 \n         1          1          1          1          1          1          1 \n0528326030 0528326060 0528326110 0528327010 0528327060 0528327070 0528328100 \n         1          1          1          1          1          1          1 \n0528340030 0528344020 0528344040 0528344060 0528344070 0528344100 0528346050 \n         1          1          1          1          1          1          1 \n0528348010 0528348030 0528354050 0528354060 0528354100 0528354110 0528358010 \n         1          1          1          1          1          1          1 \n0528358030 0528358040 0528360050 0528363020 0528363050 0528363070 0528363110 \n         1          1          1          1          1          1          1 \n0528363130 0528364110 0528365060 0528365070 0528365080 0528365090 0528366040 \n         1          1          1          1          1          1          1 \n0528366050 0528366070 0528376010 0528382020 0528382030 0528385050 0528386040 \n         1          1          1          1          1          1          1 \n0528387030 0528387050 0528390020 0528390100 0528390210 0528427010 0528427020 \n         1          1          1          1          1          1          1 \n0528427040 0528427070 0528427100 0528429050 0528429060 0528429070 0528429080 \n         1          1          1          1          1          1          1 \n0528429090 0528429100 0528429110 0528429120 0528429130 0528431020 0528431030 \n         1          1          1          1          1          1          1 \n0528431040 0528431050 0528431060 0528431070 0528431080 0528431110 0528431120 \n         1          1          1          1          1          1          1 \n0528435010 0528435020 0528435030 0528435040 0528435050 0528435060 0528435070 \n         1          1          1          1          1          1          1 \n0528439010 0528439030 0528439040 0528439050 0528439060 0528439080 0528441020 \n         1          1          1          1          1          1          1 \n0528441080 0528441090 0528445010 0528445020 0528445060 0528445070 0528456160 \n         1          1          1          1          1          1          1 \n0528456180 0528456190 0528456200 0528456220 0528456230 0528456240 0528458020 \n         1          1          1          1          1          1          1 \n0528458030 0528458040 0528458050 0528458060 0528458070 0528458080 0528458090 \n         1          1          1          1          1          1          1 \n0528458110 0528458120 0528458130 0528458140 0528458150 0528458160 0528458180 \n         1          1          1          1          1          1          1 \n0528462040 0528477010 0528477020 0528477030 0528477040 0528477050 0528477070 \n         1          1          1          1          1          1          1 \n0528477080 0528478030 0528478040 0528480020 0528480040 0528480060 0528480070 \n         1          1          1          1          1          1          1 \n0528480090 0528480100 0528480110 0528480130 0528480150 0528480160 0528480170 \n         1          1          1          1          1          1          1 \n0528480180 0528482010 0528482020 0528482030 0528482040 0528482090 0528482130 \n         1          1          1          1          1          1          1 \n0528488100 0528488110 0528488120 0528488130 0528488140 0528490040 0528490070 \n         1          1          1          1          1          1          1 \n0528490080 0531363010 0531363020 0531363030 0531363040 0531363050 0531363060 \n         1          1          1          1          1          1          1 \n0531363080 0531367010 0531367050 0531369010 0531369020 0531369060 0531371050 \n         1          1          1          1          1          1          1 \n0531371070 0531373060 0531375040 0531375050 0531375060 0531375070 0531375090 \n         1          1          1          1          1          1          1 \n0531375100 0531375110 0531375120 0531375130 0531375140 0531376010 0531376030 \n         1          1          1          1          1          1          1 \n0531376050 0531376060 0531376070 0531376090 0531379030 0531379050 0531380080 \n         1          1          1          1          1          1          1 \n0531380110 0531382070 0531382090 0531382110 0531382120 0531384030 0531384070 \n         1          1          1          1          1          1          1 \n0531385020 0531385060 0531385070 0531385130 0531450030 0531450040 0531450090 \n         1          1          1          1          1          1          1 \n0531450120 0531450170 0531451020 0531451030 0531451110 0531451150 0531451280 \n         1          1          1          1          1          1          1 \n0531451290 0531452010 0531452020 0531452050 0531452070 0531452080 0531452100 \n         1          1          1          1          1          1          1 \n0531452140 0531452180 0531452210 0531452260 0531453010 0531453040 0531453100 \n         1          1          1          1          1          1          1 \n0531453140 0531475220 0531477020 0531477040 0531477050 0531478010 0531479020 \n         1          1          1          1          1          1          1 \n0531479030 0532351050 0532351060 0532351090 0532351140 0532351150 0532353020 \n         1          1          1          1          1          1          1 \n0532353030 0532353050 0532353120 0532353130 0532353180 0532354010 0532354060 \n         1          1          1          1          1          1          1 \n0532354070 0532354090 0532354150 0532354160 0532354230 0532376040 0532376070 \n         1          1          1          1          1          1          1 \n0532376080 0532376090 0532376110 0532376160 0532376170 0532376180 0532376250 \n         1          1          1          1          1          1          1 \n0532377020 0532377050 0532377060 0532377100 0532377130 0532377140 0532378050 \n         1          1          1          1          1          1          1 \n0532378070 0532378110 0532378120 0532378130 0532378160 0532378180 0532378200 \n         1          1          1          1          1          1          1 \n0532378220 0532378240 0532476050 0532476080 0532477030 0532477040 0532478020 \n         1          1          1          1          1          1          1 \n0532479020 0532479050 0532479070 0532479120 0532479140 0533110130 0533120030 \n         1          1          1          1          1          1          1 \n0533125080 0533125120 0533125140 0533127080 0533128030 0533128090 0533130020 \n         1          1          1          1          1          1          1 \n0533130130 0533130160 0533130170 0533135020 0533205060 0533205110 0533206020 \n         1          1          1          1          1          1          1 \n0533206040 0533206050 0533206060 0533206070 0533208010 0533208020 0533208030 \n         1          1          1          1          1          1          1 \n0533208040 0533208050 0533208070 0533208080 0533208090 0533208140 0533210010 \n         1          1          1          1          1          1          1 \n0533210020 0533210060 0533212010 0533212020 0533212050 0533212060 0533212100 \n         1          1          1          1          1          1          1 \n0533212110 0533212120 0533212130 0533212140 0533212150 0533213010 0533213020 \n         1          1          1          1          1          1          1 \n0533213030 0533213040 0533213070 0533213120 0533213130 0533213140 0533215020 \n         1          1          1          1          1          1          1 \n0533215030 0533215040 0533215070 0533215080 0533217050 0533217060 0533221030 \n         1          1          1          1          1          1          1 \n0533221040 0533221060 0533221080 0533221090 0533221100 0533221110 0533223010 \n         1          1          1          1          1          1          1 \n0533223020 0533223030 0533223050 0533223080 0533223100 0533223110 0533234020 \n         1          1          1          1          1          1          1 \n0533236040 0533236050 0533236060 0533236070 0533236090 0533238045 0533241010 \n         1          1          1          1          1          1          1 \n0533241030 0533242030 0533242080 0533242090 0533244030 0533244040 0533250050 \n         1          1          1          1          1          1          1 \n0533250110 0533250130 0533250160 0533250190 0533251110 0533251120 0533251130 \n         1          1          1          1          1          1          1 \n0533252020 0533252040 0533252130 0533253030 0533253050 0533253060 0533253070 \n         1          1          1          1          1          1          1 \n0533253090 0533253160 0533253180 0533253210 0533254050 0533254100 0533254110 \n         1          1          1          1          1          1          1 \n0533254130 0533350020 0533350050 0533350090 0533350120 0533352075 0533352150 \n         1          1          1          1          1          1          1 \n0533352170 0534102025 0534104090 0534104100 0534125080 0534125120 0534125210 \n         1          1          1          1          1          1          1 \n0534126060 0534126090 0534127080 0534127130 0534127140 0534127170 0534127190 \n         1          1          1          1          1          1          1 \n0534127210 0534127230 0534127260 0534127270 0534128010 0534128020 0534128050 \n         1          1          1          1          1          1          1 \n0534128090 0534128100 0534128190 0534128210 0534129040 0534129060 0534129080 \n         1          1          1          1          1          1          1 \n0534129230 0534129370 0534129380 0534151090 0534151120 0534151175 0534151180 \n         1          1          1          1          1          1          1 \n0534152050 0534152070 0534152100 0534152120 0534175010 0534175080 0534175100 \n         1          1          1          1          1          1          1 \n0534175140 0534176060 0534176140 0534176230 0534176250 0534177180 0534177210 \n         1          1          1          1          1          1          1 \n0534177230 0534178010 0534200030 0534201040 0534201130 0534201230 0534201240 \n         1          1          1          1          1          1          1 \n0534201250 0534201260 0534201280 0534201300 0534202020 0534202030 0534202130 \n         1          1          1          1          1          1          1 \n0534202160 0534202170 0534202180 0534204030 0534204120 0534225110 0534225120 \n         1          1          1          1          1          1          1 \n0534226060 0534226120 0534250010 0534250300 0534250335 0534250370 0534250400 \n         1          1          1          1          1          1          1 \n0534250410 0534251030 0534251280 0534251320 0534252040 0534252060 0534252070 \n         1          1          1          1          1          1          1 \n0534252090 0534252110 0534252240 0534252270 0534275010 0534275060 0534275100 \n         1          1          1          1          1          1          1 \n0534275170 0534275220 0534276010 0534276040 0534276070 0534276160 0534276180 \n         1          1          1          1          1          1          1 \n0534276190 0534276230 0534276290 0534276360 0534277060 0534277070 0534277090 \n         1          1          1          1          1          1          1 \n0534278070 0534278150 0534278170 0534278190 0534278230 0534400030 0534400050 \n         1          1          1          1          1          1          1 \n0534400060 0534400120 0534400290 0534401110 0534401120 0534401130 0534401140 \n         1          1          1          1          1          1          1 \n0534401190 0534401200 0534402060 0534402140 0534402170 0534403040 0534403050 \n         1          1          1          1          1          1          1 \n0534403080 0534403100 0534403260 0534403280 0534403290 0534403360 0534403370 \n         1          1          1          1          1          1          1 \n0534403400 0534403410 0534403420 0534403440 0534425015 0534425080 0534425310 \n         1          1          1          1          1          1          1 \n0534426040 0534426110 0534427010 0534427040 0534427090 0534427140 0534428020 \n         1          1          1          1          1          1          1 \n0534428060 0534428100 0534429030 0534429130 0534429150 0534430050 0534430080 \n         1          1          1          1          1          1          1 \n0534430090 0534430110 0534431020 0534431030 0534431050 0534431070 0534431130 \n         1          1          1          1          1          1          1 \n0534450090 0534450110 0534450150 0534450180 0534451020 0534451050 0534451080 \n         1          1          1          1          1          1          1 \n0534451110 0534451120 0534451130 0534451150 0534451170 0534452060 0534453080 \n         1          1          1          1          1          1          1 \n0534453140 0534453150 0534455030 0534455080 0534475100 0534475160 0534475170 \n         1          1          1          1          1          1          1 \n0534475185 0534475250 0534476080 0534476100 0534476150 0534476260 0534476320 \n         1          1          1          1          1          1          1 \n0534477030 0534477080 0534477090 0534477110 0534477270 0534478140 0534478230 \n         1          1          1          1          1          1          1 \n0534479120 0534479130 0534479140 0534479150 0534479240 0534479300 0534479320 \n         1          1          1          1          1          1          1 \n0535101020 0535101110 0535101120 0535101150 0535102010 0535103050 0535103070 \n         1          1          1          1          1          1          1 \n0535104020 0535104120 0535105030 0535105060 0535105090 0535105100 0535105110 \n         1          1          1          1          1          1          1 \n0535105160 0535106140 0535106180 0535106190 0535125010 0535125060 0535125070 \n         1          1          1          1          1          1          1 \n0535125090 0535126010 0535126040 0535126100 0535126140 0535126180 0535150070 \n         1          1          1          1          1          1          1 \n0535150210 0535150240 0535150280 0535150300 0535151020 0535151040 0535151080 \n         1          1          1          1          1          1          1 \n0535151100 0535151110 0535151130 0535151160 0535152010 0535152050 0535152060 \n         1          1          1          1          1          1          1 \n0535152070 0535152130 0535152150 0535152200 0535152230 0535152250 0535152280 \n         1          1          1          1          1          1          1 \n0535153070 0535153140 0535153150 0535154050 0535154060 0535154080 0535154130 \n         1          1          1          1          1          1          1 \n0535155110 0535175030 0535175050 0535175070 0535175180 0535176050 0535176070 \n         1          1          1          1          1          1          1 \n0535176100 0535177020 0535177100 0535177110 0535178040 0535178060 0535178100 \n         1          1          1          1          1          1          1 \n0535178120 0535179020 0535179050 0535179060 0535179120 0535179160 0535180020 \n         1          1          1          1          1          1          1 \n0535180030 0535180070 0535180100 0535180120 0535180130 0535181030 0535181140 \n         1          1          1          1          1          1          1 \n0535300040 0535300120 0535301010 0535301060 0535301080 0535301150 0535301160 \n         1          1          1          1          1          1          1 \n0535301170 0535302070 0535302080 0535302120 0535302130 0535302140 0535302200 \n         1          1          1          1          1          1          1 \n0535303030 0535303050 0535303110 0535303150 0535304020 0535304060 0535304100 \n         1          1          1          1          1          1          1 \n0535304170 0535304180 0535305030 0535305120 0535305160 0535305180 0535325090 \n         1          1          1          1          1          1          1 \n0535325130 0535325280 0535325290 0535325320 0535325340 0535325350 0535325400 \n         1          1          1          1          1          1          1 \n0535325450 0535325460 0535327140 0535327160 0535327200 0535327210 0535327230 \n         1          1          1          1          1          1          1 \n0535350030 0535350040 0535351050 0535352080 0535352090 0535353040 0535353050 \n         1          1          1          1          1          1          1 \n0535353060 0535353130 0535353140 0535353160 0535353180 0535353190 0535353240 \n         1          1          1          1          1          1          1 \n0535354050 0535354070 0535354100 0535354130 0535354200 0535354260 0535355020 \n         1          1          1          1          1          1          1 \n0535355100 0535375010 0535375040 0535375050 0535375130 0535375140 0535375160 \n         1          1          1          1          1          1          1 \n0535376010 0535376060 0535376090 0535376100 0535377070 0535377090 0535377100 \n         1          1          1          1          1          1          1 \n0535377150 0535378020 0535378080 0535378120 0535379060 0535379100 0535379110 \n         1          1          1          1          1          1          1 \n0535380010 0535380050 0535380110 0535380130 0535380140 0535381040 0535381060 \n         1          1          1          1          1          1          1 \n0535382020 0535382100 0535382130 0535382150 0535383060 0535383070 0535383100 \n         1          1          1          1          1          1          1 \n0535383110 0535383120 0535401080 0535401140 0535402010 0535402070 0535402090 \n         1          1          1          1          1          1          1 \n0535402100 0535402120 0535402140 0535402180 0535402220 0535402230 0535402270 \n         1          1          1          1          1          1          1 \n0535402330 0535403010 0535403030 0535403040 0535403150 0535403190 0535403200 \n         1          1          1          1          1          1          1 \n0535403210 0535403280 0535404080 0535425010 0535425040 0535425050 0535425060 \n         1          1          1          1          1          1          1 \n0535425070 0535425080 0535426130 0535426150 0535426195 0535426260 0535426350 \n         1          1          1          1          1          1          1 \n0535427070 0535450070 0535450100 0535450160 0535450180 0535450190 0535450200 \n         1          1          1          1          1          1          1 \n0535450210 0535450220 0535450310 0535451010 0535451040 0535451050 0535451100 \n         1          1          1          1          1          1          1 \n0535451110 0535451170 0535451190 0535451210 0535451230 0535451250 0535452060 \n         1          1          1          1          1          1          1 \n0535452090 0535452100 0535452120 0535452140 0535453020 0535453040 0535453070 \n         1          1          1          1          1          1          1 \n0535453080 0535453100 0535453150 0535453160 0535453180 0535453200 0535453210 \n         1          1          1          1          1          1          1 \n0535454030 0535454050 0535454060 0535454070 0535454100 0535454150 0535454160 \n         1          1          1          1          1          1          1 \n0535455060 0535455080 0535455090 0535456010 0535456020 0535456050 0535456070 \n         1          1          1          1          1          1          1 \n0535456100 0535456110 0535457010 0535457020 0535457040 0535457050 0535457070 \n         1          1          1          1          1          1          1 \n0535457090 0535475010 0535475020 0535476110 0535476240 0535476350 0535476360 \n         1          1          1          1          1          1          1 \n0535476370 0535477020 0535477060 0535477090 0535477130 0535477150 0535477180 \n         1          1          1          1          1          1          1 \n0535478010 0535478040 0535478070 0535478090 0535478110 0902100010 0902100020 \n         1          1          1          1          1          1          1 \n0902100030 0902100110 0902100130 0902101010 0902101050 0902102060 0902102080 \n         1          1          1          1          1          1          1 \n0902102100 0902103090 0902103100 0902103120 0902103150 0902103170 0902104020 \n         1          1          1          1          1          1          1 \n0902104060 0902105010 0902105020 0902105050 0902105060 0902105110 0902105130 \n         1          1          1          1          1          1          1 \n0902106010 0902106040 0902106060 0902106070 0902106090 0902106120 0902106130 \n         1          1          1          1          1          1          1 \n0902106140 0902107030 0902107050 0902108060 0902109010 0902109080 0902109110 \n         1          1          1          1          1          1          1 \n0902109120 0902109130 0902109140 0902109160 0902110010 0902110080 0902110090 \n         1          1          1          1          1          1          1 \n0902110120 0902110130 0902111010 0902125020 0902125080 0902125120 0902125160 \n         1          1          1          1          1          1          1 \n0902128020 0902128075 0902128100 0902128150 0902128160 0902128170 0902134060 \n         1          1          1          1          1          1          1 \n0902134100 0902134120 0902135020 0902136110 0902201090 0902201110 0902201120 \n         1          1          1          1          1          1          1 \n0902201130 0902201140 0902202070 0902202090 0902202150 0902204060 0902204080 \n         1          1          1          1          1          1          1 \n0902204120 0902205010 0902205020 0902205030 0902205070 0902205090 0902205130 \n         1          1          1          1          1          1          1 \n0902205140 0902206020 0902206040 0902206090 0902206130 0902206220 0902206240 \n         1          1          1          1          1          1          1 \n0902206260 0902206270 0902207010 0902207080 0902207110 0902207120 0902207130 \n         1          1          1          1          1          1          1 \n0902207140 0902207150 0902207170 0902207220 0902300020 0902300040 0902300090 \n         1          1          1          1          1          1          1 \n0902300110 0902300215 0902300260 0902301030 0902301060 0902301080 0902301120 \n         1          1          1          1          1          1          1 \n0902301130 0902301140 0902301150 0902302150 0902303100 0902304040 0902304060 \n         1          1          1          1          1          1          1 \n0902305010 0902305080 0902305090 0902305110 0902306120 0902306130 0902325050 \n         1          1          1          1          1          1          1 \n0902325070 0902325100 0902325160 0902326030 0902326060 0902327040 0902327070 \n         1          1          1          1          1          1          1 \n0902328040 0902328100 0902329030 0902329070 0902329080 0902329090 0902330010 \n         1          1          1          1          1          1          1 \n0902330040 0902330090 0902331010 0902332030 0902400090 0902400110 0902401010 \n         1          1          1          1          1          1          1 \n0902401030 0902401060 0902401090 0902401120 0902401130 0902402010 0902402130 \n         1          1          1          1          1          1          1 \n0902402150 0902402250 0902402260 0902403070 0902404010 0902405070 0902405100 \n         1          1          1          1          1          1          1 \n0902405120 0902406020 0902406030 0902406070 0902406090 0902408080 0902427040 \n         1          1          1          1          1          1          1 \n0902427045 0902427140 0902427150 0902427180 0902456015 0902477120 0902477130 \n         1          1          1          1          1          1          1 \n0903200050 0903200080 0903201020 0903201030 0903201080 0903201090 0903202120 \n         1          1          1          1          1          1          1 \n0903202170 0903204010 0903204030 0903204040 0903204095 0903205040 0903205085 \n         1          1          1          1          1          1          1 \n0903206070 0903206120 0903206160 0903206170 0903225040 0903225050 0903225090 \n         1          1          1          1          1          1          1 \n0903225140 0903225150 0903225160 0903226060 0903226130 0903226150 0903226160 \n         1          1          1          1          1          1          1 \n0903226170 0903227020 0903227070 0903227080 0903227090 0903227140 0903227150 \n         1          1          1          1          1          1          1 \n0903228040 0903228060 0903228070 0903228080 0903228090 0903228120 0903228150 \n         1          1          1          1          1          1          1 \n0903229040 0903230100 0903230110 0903230120 0903231010 0903231030 0903231050 \n         1          1          1          1          1          1          1 \n0903231060 0903231070 0903231080 0903231090 0903231130 0903231180 0903231190 \n         1          1          1          1          1          1          1 \n0903231220 0903231240 0903232030 0903232060 0903232090 0903232100 0903232170 \n         1          1          1          1          1          1          1 \n0903232190 0903233030 0903233050 0903233080 0903233100 0903233120 0903233140 \n         1          1          1          1          1          1          1 \n0903233180 0903233220 0903234030 0903234050 0903234090 0903234100 0903234160 \n         1          1          1          1          1          1          1 \n0903234220 0903235010 0903235020 0903235100 0903236010 0903236040 0903236130 \n         1          1          1          1          1          1          1 \n0903236200 0903400030 0903400040 0903400180 0903400220 0903401020 0903401030 \n         1          1          1          1          1          1          1 \n0903401050 0903401070 0903425110 0903425190 0903425245 0903425270 0903425280 \n         1          1          1          1          1          1          1 \n0903425340 0903425420 0903426010 0903426160 0903426180 0903426200 0903427040 \n         1          1          1          1          1          1          1 \n0903427090 0903427120 0903429010 0903429080 0903429110 0903430040 0903430050 \n         1          1          1          1          1          1          1 \n0903430060 0903430070 0903430080 0903430090 0903430130 0903450060 0903450110 \n         1          1          1          1          1          1          1 \n0903451050 0903451090 0903451110 0903452025 0903452090 0903453050 0903453080 \n         1          1          1          1          1          1          1 \n0903454010 0903454020 0903454060 0903454080 0903454090 0903454100 0903455030 \n         1          1          1          1          1          1          1 \n0903455090 0903455140 0903456060 0903456090 0903456110 0903456130 0903457040 \n         1          1          1          1          1          1          1 \n0903457130 0903458010 0903458020 0903458060 0903458110 0903458170 0903475020 \n         1          1          1          1          1          1          1 \n0903475040 0903475060 0903475100 0903476030 0903476090 0903476100 0903476110 \n         1          1          1          1          1          1          1 \n0903481010 0903481100 0903481120 0903481130 0903484020 0903484080 0903484110 \n         1          1          1          1          1          1          1 \n0904100020 0904100030 0904100040 0904100090 0904100100 0904100140 0904100160 \n         1          1          1          1          1          1          1 \n0904100170 0904100190 0904101040 0904101070 0904101110 0904101170 0904300150 \n         1          1          1          1          1          1          1 \n0904301060 0904301070 0904301100 0904301110 0904301375 0904301410 0904302010 \n         1          1          1          1          1          1          1 \n0904302020 0904302030 0904302260 0904350045 0904351040 0904351200 0904351240 \n         1          1          1          1          1          1          1 \n0904351280 0904352190 0905100020 0905100030 0905100060 0905100080 0905101070 \n         1          1          1          1          1          1          1 \n0905101100 0905101200 0905101260 0905101300 0905101310 0905101330 0905101450 \n         1          1          1          1          1          1          1 \n0905101490 0905102010 0905102060 0905102170 0905103030 0905103040 0905103060 \n         1          1          1          1          1          1          1 \n0905103070 0905103110 0905103130 0905103140 0905103180 0905104030 0905104080 \n         1          1          1          1          1          1          1 \n0905104090 0905104110 0905104170 0905104180 0905104210 0905104240 0905105010 \n         1          1          1          1          1          1          1 \n0905105040 0905105070 0905105170 0905105190 0905105200 0905105260 0905106140 \n         1          1          1          1          1          1          1 \n0905106150 0905106170 0905106210 0905107070 0905107110 0905107140 0905107220 \n         1          1          1          1          1          1          1 \n0905107240 0905107250 0905107280 0905107300 0905107310 0905107320 0905107380 \n         1          1          1          1          1          1          1 \n0905108090 0905108120 0905108170 0905108190 0905108220 0905109130 0905109170 \n         1          1          1          1          1          1          1 \n0905200010 0905200090 0905200100 0905200160 0905200220 0905200260 0905200280 \n         1          1          1          1          1          1          1 \n0905200290 0905200340 0905200350 0905200380 0905200490 0905200510 0905201020 \n         1          1          1          1          1          1          1 \n0905201030 0905201080 0905201090 0905201110 0905201120 0905202090 0905202190 \n         1          1          1          1          1          1          1 \n0905202210 0905202230 0905225020 0905225040 0905225080 0905225090 0905226030 \n         1          1          1          1          1          1          1 \n0905226050 0905226080 0905226110 0905226130 0905226170 0905227050 0905228020 \n         1          1          1          1          1          1          1 \n0905228050 0905229040 0905300010 0905300020 0905300080 0905301050 0905325020 \n         1          1          1          1          1          1          1 \n0905325030 0905351045 0905351089 0905351150 0905352010 0905352030 0905352070 \n         1          1          1          1          1          1          1 \n0905352140 0905352180 0905376090 0905377010 0905377020 0905377050 0905377130 \n         1          1          1          1          1          1          1 \n0905378040 0905401020 0905401045 0905401060 0905401100 0905402060 0905402070 \n         1          1          1          1          1          1          1 \n0905402090 0905402110 0905403050 0905403060 0905403150 0905425125 0905426010 \n         1          1          1          1          1          1          1 \n0905426030 0905426060 0905426100 0905426150 0905426200 0905427010 0905427030 \n         1          1          1          1          1          1          1 \n0905427050 0905427070 0905427140 0905450020 0905451050 0905451300 0905451320 \n         1          1          1          1          1          1          1 \n0905451330 0905451340 0905451390 0905451410 0905452040 0905452050 0905452070 \n         1          1          1          1          1          1          1 \n0905452110 0905452120 0905452140 0905452150 0905452160 0905453040 0905453080 \n         1          1          1          1          1          1          1 \n0905475150 0905475160 0905475270 0905475500 0905475510 0905475520 0905476120 \n         1          1          1          1          1          1          1 \n0905476160 0905476170 0905476225 0905476230 0905477010 0905477050 0905478030 \n         1          1          1          1          1          1          1 \n0905478140 0905478190 0905478220 0905479110 0905480030 0905480090 0905480150 \n         1          1          1          1          1          1          1 \n0905480160 0905480180 0905480210 0905480240 0906200230 0906201021 0906201022 \n         1          1          1          1          1          1          1 \n0906201023 0906201030 0906201050 0906201120 0906201130 0906201200 0906202040 \n         1          1          1          1          1          1          1 \n0906203120 0906203140 0906204140 0906204150 0906204180 0906204230 0906204280 \n         1          1          1          1          1          1          1 \n0906223040 0906223060 0906223080 0906223090 0906223110 0906223140 0906223180 \n         1          1          1          1          1          1          1 \n0906223210 0906223220 0906225180 0906225210 0906226060 0906226070 0906226080 \n         1          1          1          1          1          1          1 \n0906226090 0906226100 0906226110 0906226120 0906226130 0906226140 0906230010 \n         1          1          1          1          1          1          1 \n0906230020 0906230030 0906340080 0906340090 0906340100 0906340110 0906340120 \n         1          1          1          1          1          1          1 \n0906340130 0906378100 0906378110 0906378120 0906378150 0906378170 0906378190 \n         1          1          1          1          1          1          1 \n0906380010 0906380030 0906380040 0906380050 0906380060 0906380070 0906380100 \n         1          1          1          1          1          1          1 \n0906380110 0906380120 0906380130 0906380140 0906380150 0906380160 0906380170 \n         1          1          1          1          1          1          1 \n0906380180 0906380190 0906380200 0906380210 0906380220 0906382020 0906382030 \n         1          1          1          1          1          1          1 \n0906382040 0906382050 0906382060 0906385010 0906385020 0906392070 0906392080 \n         1          1          1          1          1          1          1 \n0906392090 0906392100 0906392110 0906392120 0906392130 0906394010 0906394020 \n         1          1          1          1          1          1          1 \n0906394040 0906394050 0906394060 0906402060 0906402070 0906402200 0906403060 \n         1          1          1          1          1          1          1 \n0906412010 0906412050 0906420020 0906420130 0906424010 0906425045 0906426060 \n         1          1          1          1          1          1          1 \n0906426090 0906426195 0906426210 0906475050 0906475070 0906475100 0906475110 \n         1          1          1          1          1          1          1 \n0906475170 0906475200 0906476030 0906476140 0906477110 0907125020 0907125040 \n         1          1          1          1          1          1          1 \n0907125080 0907125090 0907125120 0907126010 0907126030 0907126050 0907130060 \n         1          1          1          1          1          1          1 \n0907130100 0907130110 0907131060 0907131070 0907131090 0907131120 0907131190 \n         1          1          1          1          1          1          1 \n0907135020 0907135040 0907135050 0907135080 0907135170 0907135180 0907135230 \n         1          1          1          1          1          1          1 \n0907135260 0907175030 0907175060 0907175080 0907175100 0907176010 0907180040 \n         1          1          1          1          1          1          1 \n0907180050 0907180120 0907180130 0907181090 0907181100 0907181120 0907181140 \n         1          1          1          1          1          1          1 \n0907185050 0907185060 0907187010 0907187030 0907187040 0907187060 0907187080 \n         1          1          1          1          1          1          1 \n0907192010 0907192020 0907192030 0907192040 0907192090 0907192120 0907192130 \n         1          1          1          1          1          1          1 \n0907192150 0907194010 0907194020 0907194110 0907194130 0907194160 0907196020 \n         1          1          1          1          1          1          1 \n0907196040 0907196050 0907196060 0907200110 0907200130 0907200170 0907200190 \n         1          1          1          1          1          1          1 \n0907200230 0907200250 0907200270 0907200280 0907200290 0907200340 0907201060 \n         1          1          1          1          1          1          1 \n0907201160 0907201220 0907201230 0907201280 0907202010 0907202050 0907202080 \n         1          1          1          1          1          1          1 \n0907202100 0907202130 0907202140 0907202160 0907202190 0907202220 0907202240 \n         1          1          1          1          1          1          1 \n0907203010 0907203040 0907203060 0907203070 0907203090 0907203110 0907227030 \n         1          1          1          1          1          1          1 \n0907227050 0907227060 0907227080 0907227090 0907227100 0907227110 0907227290 \n         1          1          1          1          1          1          1 \n0907227300 0907230240 0907250020 0907250030 0907250050 0907250070 0907251090 \n         1          1          1          1          1          1          1 \n0907251100 0907252020 0907252050 0907252060 0907252120 0907252190 0907252210 \n         1          1          1          1          1          1          1 \n0907252220 0907253060 0907253110 0907253130 0907254020 0907254050 0907254090 \n         1          1          1          1          1          1          1 \n0907255010 0907255020 0907255030 0907255050 0907255060 0907260010 0907260030 \n         1          1          1          1          1          1          1 \n0907260040 0907260050 0907262020 0907262030 0907262050 0907262060 0907262070 \n         1          1          1          1          1          1          1 \n0907262080 0907265010 0907265030 0907265100 0907270040 0907270050 0907275010 \n         1          1          1          1          1          1          1 \n0907275030 0907275080 0907275090 0907275140 0907275150 0907280040 0907280090 \n         1          1          1          1          1          1          1 \n0907280100 0907280170 0907285010 0907285020 0907285040 0907285050 0907285080 \n         1          1          1          1          1          1          1 \n0907285100 0907290020 0907290040 0907290070 0907290080 0907290090 0907290140 \n         1          1          1          1          1          1          1 \n0907290150 0907290160 0907290170 0907290180 0907290210 0907290230 0907290240 \n         1          1          1          1          1          1          1 \n0907290250 0907290260 0907295020 0907295040 0907295100 0907401050 0907401060 \n         1          1          1          1          1          1          1 \n0907401080 0907401090 0907405020 0907405030 0907405130 0907405140 0907410020 \n         1          1          1          1          1          1          1 \n0907410040 0907410050 0907410080 0907410100 0907410110 0907410130 0907412010 \n         1          1          1          1          1          1          1 \n0907412020 0907412040 0907412070 0907412080 0907412090 0907414020 0907414030 \n         1          1          1          1          1          1          1 \n0907414040 0907414060 0907414070 0907414080 0907418010 0907418020 0907418040 \n         1          1          1          1          1          1          1 \n0907420040 0907420050 0907420060 0907420070 0907420080 0907420090 0907420110 \n         1          1          1          1          1          1          1 \n0907425010 0907425015 0907425030 0907425035 0908102020 0908102030 0908102040 \n         1          1          1          1          1          1          1 \n0908102050 0908102060 0908102100 0908102110 0908102130 0908102140 0908102160 \n         1          1          1          1          1          1          1 \n0908102170 0908102260 0908102270 0908102290 0908102300 0908102320 0908102330 \n         1          1          1          1          1          1          1 \n0908103090 0908103260 0908103280 0908103310 0908103320 0908103350 0908127040 \n         1          1          1          1          1          1          1 \n0908127050 0908127060 0908127070 0908127100 0908127120 0908128050 0908128060 \n         1          1          1          1          1          1          1 \n0908128100 0908128110 0908130020 0908151040 0908151050 0908152035 0908152060 \n         1          1          1          1          1          1          1 \n0908152070 0908152080 0908152100 0908152110 0908152180 0908152260 0908152270 \n         1          1          1          1          1          1          1 \n0908152280 0908152290 0908154040 0908154080 0908178050 0908186040 0908186050 \n         1          1          1          1          1          1          1 \n0908186060 0908186070 0908186080 0908186090 0908188110 0908188120 0908188130 \n         1          1          1          1          1          1          1 \n0908188140 0908188150 0908201040 0908201100 0908201110 0908201120 0908203090 \n         1          1          1          1          1          1          1 \n0908203100 0908204090 0908204160 0908204180 0908208020 0908225170 0908225180 \n         1          1          1          1          1          1          1 \n0908225240 0908225290 0908225310 0908225320 0908225370 0908226120 0908226130 \n         1          1          1          1          1          1          1 \n0908226180 0908227010 0908227030 0908227040 0908228010 0908228040 0908229020 \n         1          1          1          1          1          1          1 \n0908250030 0908250040 0908250090 0908250100 0908275040 0908275090 0908275110 \n         1          1          1          1          1          1          1 \n0908275130 0908275180 0908275200 0908275220 0908275240 0908275280 0908275290 \n         1          1          1          1          1          1          1 \n0908275300 0908276140 0908276150 0908276230 0909100080 0909100110 0909100150 \n         1          1          1          1          1          1          1 \n0909100170 0909101010 0909101050 0909101060 0909101070 0909101080 0909101100 \n         1          1          1          1          1          1          1 \n0909101140 0909101180 0909101190 0909101330 0909103020 0909129090 0909129100 \n         1          1          1          1          1          1          1 \n0909131125 0909131170 0909175030 0909175050 0909175080 0909175100 0909176080 \n         1          1          1          1          1          1          1 \n0909176140 0909176150 0909176170 0909176180 0909177100 0909177120 0909178160 \n         1          1          1          1          1          1          1 \n0909179020 0909201110 0909201180 0909250030 0909250040 0909250060 0909250070 \n         1          1          1          1          1          1          1 \n0909250080 0909250120 0909250150 0909250160 0909250200 0909250210 0909250220 \n         1          1          1          1          1          1          1 \n0909250230 0909251030 0909251040 0909251050 0909251080 0909251090 0909251170 \n         1          1          1          1          1          1          1 \n0909252010 0909252020 0909252110 0909252150 0909252170 0909252220 0909253010 \n         1          1          1          1          1          1          1 \n0909253080 0909253180 0909253190 0909254010 0909254050 0909254070 0909254090 \n         1          1          1          1          1          1          1 \n0909254100 0909254130 0909254140 0909254150 0909256010 0909256020 0909256060 \n         1          1          1          1          1          1          1 \n0909256120 0909257050 0909275020 0909275030 0909275040 0909275050 0909275080 \n         1          1          1          1          1          1          1 \n0909275110 0909275160 0909275230 0909275250 0909275310 0909276010 0909276070 \n         1          1          1          1          1          1          1 \n0909276110 0909276160 0909276170 0909276200 0909276210 0909277040 0909277070 \n         1          1          1          1          1          1          1 \n0909277090 0909277100 0909278020 0909279010 0909279040 0909279080 0909280030 \n         1          1          1          1          1          1          1 \n0909280070 0909281010 0909281020 0909281030 0909281080 0909281110 0909281130 \n         1          1          1          1          1          1          1 \n0909282020 0909282030 0909282060 0909282110 0909282130 0909425010 0909425060 \n         1          1          1          1          1          1          1 \n0909425085 0909425120 0909425130 0909425140 0909425180 0909425270 0909426060 \n         1          1          1          1          1          1          1 \n0909426080 0909426090 0909427200 0909427230 0909428110 0909428120 0909428170 \n         1          1          1          1          1          1          1 \n0909428180 0909428190 0909428240 0909428280 0909428340 0909451020 0909451040 \n         1          1          1          1          1          1          1 \n0909451100 0909451130 0909451140 0909451150 0909451180 0909452050 0909452102 \n         1          1          1          1          1          1          1 \n0909452114 0909455030 0909455040 0909455060 0909475020 0909475040 0909475050 \n         1          1          1          1          1          1          1 \n0909475070 0909475140 0909475230 0909475300 0910200020 0910200040 0910200060 \n         1          1          1          1          1          1          1 \n0910200080 0910200110 0910201020 0910201050 0910201110 0910201130 0910201180 \n         1          1          1          1          1          1          1 \n0910202030 0910202050 0910202060 0910202070 0910202100 0910203020 0910203090 \n         1          1          1          1          1          1          1 \n0910203100 0910203230 0910203250 0910203290 0910204050 0910204090 0910205010 \n         1          1          1          1          1          1          1 \n0910205020 0910205120 0910205130 0910206010 0910206110 0910207110 0910226040 \n         1          1          1          1          1          1          1 \n0910226060 0910251050 0911102090 0911102170 0911102180 0911103050 0911103060 \n         1          1          1          1          1          1          1 \n0911104060 0911128010 0911128020 0911128180 0911175360 0911175410 0911175430 \n         1          1          1          1          1          1          1 \n0911175440 0911202100 0911204090 0911204100 0911225110 0911226010 0911226030 \n         1          1          1          1          1          1          1 \n0911370410 0911370430 0911370450 0911370460 0911370490 0911370500 0911370510 \n         1          1          1          1          1          1          1 \n0911370520 0911370530 0911370540 0912251110 0913350030 0914452060 0914452090 \n         1          1          1          1          1          1          1 \n0914452120 0914452190 0914453045 0914460020 0914460110 0914465020 0914465040 \n         1          1          1          1          1          1          1 \n0914465055 0914465060 0914467040 0914467050 0914474020 0914474070 0914475010 \n         1          1          1          1          1          1          1 \n0914475020 0914475030 0914475090 0914476010 0914476020 0914476050 0914476080 \n         1          1          1          1          1          1          1 \n0914476130 0914476330 0914476380 0914476430 0914476450 0914476500 0914476520 \n         1          1          1          1          1          1          1 \n0914478020 0914478045 0914478100 0914478110 0916125360 0916125425 0916176030 \n         1          1          1          1          1          1          1 \n0916176125 0916225130 0916226030 0916226090 0916252170 0916253320 0916325040 \n         1          1          1          1          1          1          1 \n0916325080 0916326010 0916326040 0916326090 0916380060 0916380070 0916382010 \n         1          1          1          1          1          1          1 \n0916382100 0916382110 0916382120 0916384050 0916384070 0916384080 0916384100 \n         1          1          1          1          1          1          1 \n0916384150 0916386010 0916386020 0916386040 0916386060 0916386080 0916386090 \n         1          1          1          1          1          1          1 \n0916386140 0916386170 0916386180 0916402015 0916402115 0916402125 0916402215 \n         1          1          1          1          1          1          1 \n0916403010 0916403020 0916403040 0916403130 0916403200 0916403230 0916403250 \n         1          1          1          1          1          1          1 \n0916403290 0916455010 0916455050 0916455070 0916455110 0916455120 0916455150 \n         1          1          1          1          1          1          1 \n0916455170 0916460020 0916460060 0916460070 0916460110 0916475020 0916475040 \n         1          1          1          1          1          1          1 \n0916475100 0916475110 0916477010 0916477020 0916477060 0917425190 0921126010 \n         1          1          1          1          1          1          1 \n0921126030 0921128020 0921128030 0921128050 0921201060 0921205030 0921205050 \n         1          1          1          1          1          1          1 \n0921205070 0923125030 0923201020 0923201100 0923202005 0923202015 0923202025 \n         1          1          1          1          1          1          1 \n0923202060 0923202105 0923202134 0923202137 0923202220 0923203010 0923203090 \n         1          1          1          1          1          1          1 \n0923203100 0923203110 0923203140 0923203190 0923204040 0923204050 0923204140 \n         1          1          1          1          1          1          1 \n0923204150 0923205015 0923205025 0923205120 0923225040 0923225050 0923225080 \n         1          1          1          1          1          1          1 \n0923225150 0923225190 0923225200 0923225240 0923225260 0923225300 0923225310 \n         1          1          1          1          1          1          1 \n0923225360 0923225370 0923225390 0923225440 0923225490 0923225510 0923226150 \n         1          1          1          1          1          1          1 \n0923226180 0923226250 0923226270 0923226290 0923226300 0923226320 0923227030 \n         1          1          1          1          1          1          1 \n0923227080 0923227100 0923228080 0923228110 0923228130 0923228150 0923228180 \n         1          1          1          1          1          1          1 \n0923228200 0923228210 0923228220 0923228230 0923228250 0923228260 0923228270 \n         1          1          1          1          1          1          1 \n0923228290 0923228310 0923228370 0923228390 0923228420 0923229010 0923229040 \n         1          1          1          1          1          1          1 \n0923229100 0923229110 0923229140 0923230010 0923230040 0923230120 0923230180 \n         1          1          1          1          1          1          1 \n0923230190 0923230200 0923250060 0923250210 0923251080 0923251160 0923251180 \n         1          1          1          1          1          1          1 \n0923252075 0923252080 0923252100 0923275010 0923275040 0923275080 0923275090 \n         1          1          1          1          1          1          1 \n0923275140 0923275200 0923276030 0923276100 0923276180 0923276250 0923276260 \n         1          1          1          1          1          1          1 \n0923277030 0923277040 0923277080 0923400025 0923400040 0923400110 0923400125 \n         1          1          1          1          1          1          1 \n0923400150 0923425030 0923426010 0923426070 0924100020 0924100040 0924100050 \n         1          1          1          1          1          1          1 \n0924100060 0924100070 0924151040 0924151050 0924152030 1007100110 \n         1          1          1          1          1          1 \n\n$MSSubClass\n\n 020  030  040  045  050  060  070  075  080  085  090  120  150  160  180  190 \n1078  139    6   18  287  571  128   23  118   48  109  192    1  129   17   61 \n\n$MSZoning\n\nA (agr) C (all)      FV I (all)      RH      RL      RM \n      2      25     139       2      27    2268     462 \n\n$Street\n\nGrvl Pave \n  12 2913 \n\n$Alley\n\nGrvl Pave \n 120   78 \n\n$LotShape\n\n IR1  IR2  IR3  Reg \n 975   76   15 1859 \n\n$LandContour\n\n Bnk  HLS  Low  Lvl \n 114  120   60 2631 \n\n$Utilities\n\nAllPub NoSeWa NoSewr \n  2922      1      2 \n\n$LotConfig\n\n Corner CulDSac     FR2     FR3  Inside \n    508     180      85      14    2138 \n\n$LandSlope\n\n Gtl  Mod  Sev \n2784  125   16 \n\n$Neighborhood\n\nBlmngtn Blueste  BrDale BrkSide ClearCr CollgCr Crawfor Edwards Gilbert  Greens \n     28      10      30     108      44     267     103     191     165       8 \nGrnHill  IDOTRR Landmrk MeadowV Mitchel   NAmes NoRidge NPkVill NridgHt  NWAmes \n      2      93       1      37     114     443      69      23     166     131 \nOldTown  Sawyer SawyerW Somerst StoneBr   SWISU  Timber Veenker \n    239     151     125     182      51      48      72      24 \n\n$Condition1\n\nArtery  Feedr   Norm   PosA   PosN   RRAe   RRAn   RRNe   RRNn \n    92    163   2519     20     38     28     50      6      9 \n\n$Condition2\n\nArtery  Feedr   Norm   PosA   PosN   RRAe   RRAn   RRNn \n     5     13   2896      4      3      1      1      2 \n\n$BldgType\n\n  1Fam 2fmCon Duplex  Twnhs TwnhsE \n  2420     62    109    101    233 \n\n$HouseStyle\n\n1.5Fin 1.5Unf 1Story 2.5Fin 2.5Unf 2Story SFoyer   SLvl \n   314     19   1480      8     24    869     83    128 \n\n$RoofStyle\n\n   Flat   Gable Gambrel     Hip Mansard    Shed \n     20    2320      22     547      11       5 \n\n$RoofMatl\n\nCompShg Membran   Metal    Roll Tar&Grv WdShake WdShngl \n   2884       1       1       1      23       9       6 \n\n$Exterior1st\n\nAsbShng AsphShn BrkComm BrkFace  CBlock CemntBd HdBoard ImStucc MetalSd Plywood \n     44       2       6      88       2     124     441       1     450     221 \nPreCast   Stone  Stucco VinylSd Wd Sdng WdShing \n      1       2      42    1026     419      56 \n\n$Exterior2nd\n\nAsbShng AsphShn Brk Cmn BrkFace  CBlock CmentBd HdBoard ImStucc MetalSd   Other \n     38       4      22      47       3     124     405      14     447       1 \nPlywood PreCast   Stone  Stucco VinylSd Wd Sdng Wd Shng \n    274       1       6      46    1015     397      81 \n\n$MasVnrType\n\n BrkCmn BrkFace  CBlock    None   Stone \n     25     879       1    1751     246 \n\n$ExterQual\n\n  Ex   Fa   Gd   TA \n 103   35  988 1799 \n\n$ExterCond\n\n  Ex   Fa   Gd   Po   TA \n  12   67  299    3 2544 \n\n$Foundation\n\nBrkTil CBlock  PConc   Slab  Stone   Wood \n   311   1244   1305     49     11      5 \n\n$BsmtQual\n\n  Ex   Fa   Gd   Po   TA \n 253   88 1219    2 1283 \n\n$BsmtCond\n\n  Ex   Fa   Gd   Po   TA \n   3  104  122    5 2611 \n\n$BsmtExposure\n\n  Av   Gd   Mn   No \n 417  280  239 1906 \n\n$BsmtFinType1\n\nALQ BLQ GLQ LwQ Rec Unf \n429 269 854 154 288 851 \n\n$BsmtFinType2\n\n ALQ  BLQ  GLQ  LwQ  Rec  Unf \n  53   68   34   89  106 2494 \n\n$Heating\n\nFloor  GasA  GasW  Grav  OthW  Wall \n    1  2880    27     9     2     6 \n\n$HeatingQC\n\n  Ex   Fa   Gd   Po   TA \n1490   92  476    3  864 \n\n$CentralAir\n\n   N    Y \n 196 2729 \n\n$Electrical\n\nFuseA FuseF FuseP   Mix SBrkr \n  188    50     8     1  2677 \n\n$KitchenQual\n\n  Ex   Fa   Gd   Po   TA \n 200   70 1160    1 1494 \n\n$Functional\n\nMaj1 Maj2 Min1 Min2  Mod  Sal  Sev  Typ \n  19    9   65   70   35    2    2 2723 \n\n$FireplaceQu\n\n Ex  Fa  Gd  Po  TA \n 42  75 741  46 599 \n\n$GarageType\n\n 2Types  Attchd Basment BuiltIn CarPort  Detchd \n     23    1727      36     185      15     782 \n\n$GarageFinish\n\n Fin  RFn  Unf \n 723  812 1231 \n\n$GarageQual\n\n  Ex   Fa   Gd   Po   TA \n   3  124   24    5 2610 \n\n$GarageCond\n\n  Ex   Fa   Gd   Po   TA \n   3   74   15   14 2660 \n\n$PavedDrive\n\n   N    P    Y \n 216   62 2647 \n\n$PoolQC\n\nEx Fa Gd TA \n 3  2  3  3 \n\n$Fence\n\nGdPrv  GdWo MnPrv  MnWw \n  118   112   329    12 \n\n$MiscFeature\n\nGar2 Othr Shed TenC \n   5    4   95    1 \n\n$SaleType\n\n  COD   Con ConLD ConLI ConLw   CWD   New   Oth   VWD    WD \n   87     5    26     9     8    12   236     7     1  2534 \n\n$SaleCondition\n\nAbnorml AdjLand  Alloca  Family  Normal Partial \n    189      12      24      46    2412     242 \n\ndf |&gt; select(where(is.numeric)) |&gt; map ( summary )\n\n$Order\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1     732    1463    1465    2199    2930 \n\n$LotFrontage\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n     21      58      68      69      80     313     490 \n\n$LotArea\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1300    7438    9428   10104   11515  215245 \n\n$OverallQual\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1       5       6       6       7      10 \n\n$OverallCond\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1       5       5       6       6       9 \n\n$YearBuilt\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1872    1954    1973    1971    2001    2010 \n\n$`YearRemod/Add`\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1950    1965    1993    1984    2004    2010 \n\n$MasVnrArea\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      0       0       0     101     164    1600      23 \n\n$BsmtFinSF1\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      0       0     370     438     733    2288       1 \n\n$BsmtFinSF2\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      0       0       0      50       0    1526       1 \n\n$BsmtUnfSF\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      0     219     464     559     801    2336       1 \n\n$TotalBsmtSF\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      0     793     990    1047    1299    3206       1 \n\n$`1stFlrSF`\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    334     876    1082    1155    1383    3820 \n\n$`2ndFlrSF`\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0     334     702    1862 \n\n$LowQualFinSF\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       5       0    1064 \n\n$GrLivArea\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    334    1126    1441    1494    1740    3820 \n\n$BsmtFullBath\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n    0.0     0.0     0.0     0.4     1.0     3.0       2 \n\n$BsmtHalfBath\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n    0.0     0.0     0.0     0.1     0.0     2.0       2 \n\n$FullBath\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       1       2       2       2       4 \n\n$HalfBath\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0     0.0     0.0     0.4     1.0     2.0 \n\n$BedroomAbvGr\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       2       3       3       3       8 \n\n$KitchenAbvGr\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       1       1       1       1       3 \n\n$TotRmsAbvGrd\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      2       5       6       6       7      14 \n\n$Fireplaces\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       1       1       1       4 \n\n$GarageYrBlt\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1895    1960    1979    1978    2002    2207     159 \n\n$GarageCars\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      0       1       2       2       2       5       1 \n\n$GarageArea\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      0     320     480     472     576    1488       1 \n\n$WoodDeckSF\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0      93     168    1424 \n\n$OpenPorchSF\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0      27      47      70     742 \n\n$EnclosedPorch\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0      23       0    1012 \n\n$`3SsnPorch`\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       3       0     508 \n\n$ScreenPorch\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0      16       0     576 \n\n$PoolArea\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       2       0     800 \n\n$MiscVal\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0      45       0   15500 \n\n$MoSold\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1       4       6       6       8      12 \n\n$YrSold\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2006    2007    2008    2008    2009    2010 \n\n$SalePrice\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12789  129500  160000  180412  213500  625000 \n\n\nUsing the above functions would let you quickly and easily to determine that you should delete some columns and convert others to factor or integer. You can delete columns by saying, for instance,\n\ndf &lt;- df |&gt; select(-c(\"Order\",\"PID\"))\n\nYou may add as many names to the list in the -c() vector as desired.\nYou may wish to delete columns with a lot of NAs rather than deleting rows with lots of NAs. (Why is this?)\nThis is easy to discover in the case of numerical columns. For instance, LotFrontage has 490 NAs. You can quickly see that in the output of the above summary. But it doesn’t work for character data. One quick way to see the number of NAs in all columns is to say\n\ncolSums(is.na(df)) |&gt; sort()\n\n   MSSubClass      MSZoning       LotArea        Street      LotShape \n            0             0             0             0             0 \n  LandContour     Utilities     LotConfig     LandSlope  Neighborhood \n            0             0             0             0             0 \n   Condition1    Condition2      BldgType    HouseStyle   OverallQual \n            0             0             0             0             0 \n  OverallCond     YearBuilt YearRemod/Add     RoofStyle      RoofMatl \n            0             0             0             0             0 \n  Exterior1st   Exterior2nd     ExterQual     ExterCond    Foundation \n            0             0             0             0             0 \n      Heating     HeatingQC    CentralAir      1stFlrSF      2ndFlrSF \n            0             0             0             0             0 \n LowQualFinSF     GrLivArea      FullBath      HalfBath  BedroomAbvGr \n            0             0             0             0             0 \n KitchenAbvGr   KitchenQual  TotRmsAbvGrd    Functional    Fireplaces \n            0             0             0             0             0 \n   PavedDrive    WoodDeckSF   OpenPorchSF EnclosedPorch     3SsnPorch \n            0             0             0             0             0 \n  ScreenPorch      PoolArea       MiscVal        MoSold        YrSold \n            0             0             0             0             0 \n     SaleType SaleCondition     SalePrice    BsmtFinSF1    BsmtFinSF2 \n            0             0             0             1             1 \n    BsmtUnfSF   TotalBsmtSF    Electrical    GarageCars    GarageArea \n            1             1             1             1             1 \n BsmtFullBath  BsmtHalfBath    MasVnrType    MasVnrArea      BsmtQual \n            2             2            23            23            80 \n     BsmtCond  BsmtFinType1  BsmtFinType2  BsmtExposure    GarageType \n           80            80            81            83           157 \n  GarageYrBlt  GarageFinish    GarageQual    GarageCond   LotFrontage \n          159           159           159           159           490 \n  FireplaceQu         Fence         Alley   MiscFeature        PoolQC \n         1422          2354          2727          2820          2914 \n\n\nThis makes it clear that you should not use FireplaceQu, Fence, Alley, or MiscFeature in addition to LotFrontage.\nA better-looking display may be obtained by saying\n\nv &lt;- colSums(is.na(df))\nv[v&gt;0] |&gt; sort(decreasing=TRUE) |&gt; as.data.frame()\n\n             sort(v[v &gt; 0], decreasing = TRUE)\nPoolQC                                    2914\nMiscFeature                               2820\nAlley                                     2727\nFence                                     2354\nFireplaceQu                               1422\nLotFrontage                                490\nGarageYrBlt                                159\nGarageFinish                               159\nGarageQual                                 159\nGarageCond                                 159\nGarageType                                 157\nBsmtExposure                                83\nBsmtFinType2                                81\nBsmtQual                                    80\nBsmtCond                                    80\nBsmtFinType1                                80\nMasVnrType                                  23\nMasVnrArea                                  23\nBsmtFullBath                                 2\nBsmtHalfBath                                 2\nBsmtFinSF1                                   1\nBsmtFinSF2                                   1\nBsmtUnfSF                                    1\nTotalBsmtSF                                  1\nElectrical                                   1\nGarageCars                                   1\nGarageArea                                   1\n\n\n\n\n2.9.3 Exercises\nCreate a Quarto document called week02exercises.qmd. Use your name as the author name and the date as the current date. Make the title within the document “Week 2 Exercises”.\nAnswer the following questions in the document, using a combination of narration and R chunks.\n\nUse the loan50 data set. Find the mean and median of annual_income using R. Tell why they differ in words.\nUse the loan50 data set. Make a contingency table of loan_purpose and grade. Tell the most frequently occurring grade and most frequently occurring loan purpose in words.\nUse the loan50 data set. Provide a statistical summary of total_credit_limit.\nUse the loan50 data set. Show the column means for all numeric columns.\nUse the loan50 data set. Make a contingency table of state and homeownership. Tell which state has the most mortgages in words.\n\nNow render the document and submit both the .qmd file and the .html file to Canvas under “week02exercises”.\n\n\n2.9.4 Solutions to exercises\n\nUse the loan50 data set. Find the mean and median of annual_income using R. Tell why they differ in words.\n\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/loan50.rda\"))\nwith(loan50,mean(annual_income))\n\n[1] 86170\n\nwith(loan50,median(annual_income))\n\n[1] 74000\n\n\nThey differ because the mean is susceptible to outliers. There are about four outliers in this data set (high annual incomes) and they drag the mean upward but not the median. The median is a more reliable measure of centrality when there are influential outliers.\n\nUse the loan50 data set. Make a contingency table of loan_purpose and grade. Tell the most frequently occurring grade and most frequently occurring loan purpose in words.\n\n\nwith(loan50,addmargins(table(loan_purpose,grade)))\n\n                    grade\nloan_purpose             A  B  C  D  E  F  G Sum\n                      0  0  0  0  0  0  0  0   0\n  car                 0  0  1  1  0  0  0  0   2\n  credit_card         0  6  4  1  1  1  0  0  13\n  debt_consolidation  0  2  9  4  7  1  0  0  23\n  home_improvement    0  1  4  0  0  0  0  0   5\n  house               0  0  1  0  0  0  0  0   1\n  major_purchase      0  0  0  0  0  0  0  0   0\n  medical             0  0  0  0  0  0  0  0   0\n  moving              0  0  0  0  0  0  0  0   0\n  other               0  4  0  0  0  0  0  0   4\n  renewable_energy    0  1  0  0  0  0  0  0   1\n  small_business      0  1  0  0  0  0  0  0   1\n  vacation            0  0  0  0  0  0  0  0   0\n  wedding             0  0  0  0  0  0  0  0   0\n  Sum                 0 15 19  6  8  2  0  0  50\n\n\nThe most frequently occurring purpose is debt consolidation, while the most frequently occurring grade is B.\n\nUse the loan50 data set. Provide a statistical summary of total_credit_limit.\n\n\nwith(loan50,summary(total_credit_limit))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15980   70526  147364  208547  299766  793009 \n\nwith(loan50,scales::comma_format(summary(total_credit_limit)))\n\nfunction (x) \n{\n    number(x, accuracy = accuracy, scale = scale, prefix = prefix, \n        suffix = suffix, big.mark = big.mark, decimal.mark = decimal.mark, \n        style_positive = style_positive, style_negative = style_negative, \n        scale_cut = scale_cut, trim = trim, ...)\n}\n&lt;bytecode: 0x7fd3573b8680&gt;\n&lt;environment: 0x7fd35b0940e0&gt;\n\n#. same info with commas in numbers\nloan50 |&gt;\n    summarise(Min=comma(min(total_credit_limit)),\n              firstq=comma(quantile(total_credit_limit,0.25)),\n              Median=comma(median(total_credit_limit)),\n              Mean=comma(mean(total_credit_limit)),\n              thirdq=comma(quantile(total_credit_limit,0.75)),\n              Max=comma(max(total_credit_limit)))\n\n     Min firstq  Median    Mean  thirdq     Max\n1 15,980 70,526 147,364 208,547 299,766 793,009\n\n\n\nUse the loan50 data set. Show the column means for all numeric columns.\n\n\noptions(digits=1)\nformat(colMeans(loan50[sapply(loan50, is.numeric)]),scientific=FALSE,big.mark=\",\")\n\n             emp_length                    term           annual_income \n           \"        NA\"            \"     42.72\"            \" 86,170.00\" \n         debt_to_income      total_credit_limit   total_credit_utilized \n           \"      0.72\"            \"208,546.64\"            \" 61,546.54\" \nnum_cc_carrying_balance             loan_amount           interest_rate \n           \"      5.06\"            \" 17,083.00\"            \"     11.57\" \n public_record_bankrupt            total_income \n           \"      0.08\"            \"105,220.56\" \n\n\n\nUse the loan50 data set. Make a contingency table of state and homeownership. Tell which state has the most mortgages in words.\n\n\nwith(loan50,table(state,homeownership))\n\n     homeownership\nstate rent mortgage own\n         0        0   0\n   AK    0        0   0\n   AL    0        0   0\n   AR    0        0   0\n   AZ    0        0   1\n   CA    7        2   0\n   CO    0        0   0\n   CT    1        0   0\n   DC    0        0   0\n   DE    0        0   0\n   FL    1        2   0\n   GA    0        0   0\n   HI    1        1   0\n   ID    0        0   0\n   IL    3        0   1\n   IN    0        1   1\n   KS    0        0   0\n   KY    0        0   0\n   LA    0        0   0\n   MA    1        1   0\n   MD    2        1   0\n   ME    0        0   0\n   MI    0        1   0\n   MN    0        1   0\n   MO    0        1   0\n   MS    0        1   0\n   MT    0        0   0\n   NC    0        0   0\n   ND    0        0   0\n   NE    0        1   0\n   NH    1        0   0\n   NJ    2        1   0\n   NM    0        0   0\n   NV    0        2   0\n   NY    1        0   0\n   OH    0        1   0\n   OK    0        0   0\n   OR    0        0   0\n   PA    0        0   0\n   RI    0        1   0\n   SC    0        1   0\n   SD    0        0   0\n   TN    0        0   0\n   TX    0        5   0\n   UT    0        0   0\n   VA    1        0   0\n   VT    0        0   0\n   WA    0        0   0\n   WI    0        1   0\n   WV    0        1   0\n   WY    0        0   0\n\n\nTexas has five mortgages, more than any other state.\n\n\n2.9.5 Exercise Notes\n\nMany students did not follow instructions on file naming. I will take off a lot of points if this happens when you turn in a graded assignment. I expect all files to be uniformly named.\nSeveral students left the boilerplate verbiage in their .qmd file. I will take off a lot of points if this happens when you turn in a graded assignment.\nOne student put their narrative inside the code chunks as R comments. Don’t do this. It undercuts the purpose of mixing narrative and code in a Quarto document.\nSome students didn’t try to answer the second part of question 1. One way to understand this is to draw a boxplot of the data, showing that there are four outliers at the top end, dragging the mean upward but leaving the median pretty much alone.\n\n\nloan50 |&gt; ggplot(aes(annual_income)) + geom_boxplot()\n\n\n\n\n\nSome students included graphics, which don’t show up in the copy on Canvas. One way to make these graphics show up is to add the following code to the front matter (the front matter is the stuff between two sets of three dashes at the beginning of the file):\n\nformat:\n  html:\n    embed-resources: true\nThe indentation shown above is essential for it to work.\n\nSome students highlighted relevant rows and columns as shown below. This was a really great addition.\n\n\ntbl &lt;- with(loan50,table(loan_purpose,grade))\ntbl &lt;- addmargins(tbl)\nlibrary(kableExtra)\ntbl |&gt;\n  kbl() |&gt;\n  kable_classic(full_width=F) |&gt;\n  row_spec(4, color = \"white\", background = \"#AAAAAA\") |&gt;\n  column_spec(4, color = \"white\", background = \"#AAAAAA\") |&gt;\n  column_spec(1, color = \"black\", background = \"white\")\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nSum\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\ncar\n0\n0\n1\n1\n0\n0\n0\n0\n2\n\n\ncredit_card\n0\n6\n4\n1\n1\n1\n0\n0\n13\n\n\ndebt_consolidation\n0\n2\n9\n4\n7\n1\n0\n0\n23\n\n\nhome_improvement\n0\n1\n4\n0\n0\n0\n0\n0\n5\n\n\nhouse\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\nmajor_purchase\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nmedical\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nmoving\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nother\n0\n4\n0\n0\n0\n0\n0\n0\n4\n\n\nrenewable_energy\n0\n1\n0\n0\n0\n0\n0\n0\n1\n\n\nsmall_business\n0\n1\n0\n0\n0\n0\n0\n0\n1\n\n\nvacation\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nwedding\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nSum\n0\n15\n19\n6\n8\n2\n0\n0\n50\n\n\n\n\n\n\n\nAnother way to do this is to say\n\nmaxrow&lt;-max(tbl[1:length(levels(loan50$loan_purpose))-1,\"Sum\"])\nmaxcol&lt;-max(tbl[\"Sum\",1:length(levels(loan50$grade))-1])\n\nand\n\nmaxrownum &lt;- which.max(tbl[1:length(levels(loan50$loan_purpose))-1,\"Sum\"])\nmaxcolnum &lt;- which.max(tbl[\"Sum\",1:length(levels(loan50$grade))-1])+1\n\nNow you can plug maxrownum and maxcolnum into the formula without having to know which row and column you’re talking about.\n\ntbl |&gt;\n  kbl() |&gt;\n  kable_classic(full_width=F) |&gt;\n  row_spec(maxrownum, color = \"white\", background = \"#AAAAAA\") |&gt;\n  column_spec(maxcolnum, color = \"white\", background = \"#AAAAAA\") |&gt;\n  column_spec(1, color = \"black\", background = \"white\")\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nSum\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\ncar\n0\n0\n1\n1\n0\n0\n0\n0\n2\n\n\ncredit_card\n0\n6\n4\n1\n1\n1\n0\n0\n13\n\n\ndebt_consolidation\n0\n2\n9\n4\n7\n1\n0\n0\n23\n\n\nhome_improvement\n0\n1\n4\n0\n0\n0\n0\n0\n5\n\n\nhouse\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\nmajor_purchase\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nmedical\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nmoving\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nother\n0\n4\n0\n0\n0\n0\n0\n0\n4\n\n\nrenewable_energy\n0\n1\n0\n0\n0\n0\n0\n0\n1\n\n\nsmall_business\n0\n1\n0\n0\n0\n0\n0\n0\n1\n\n\nvacation\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nwedding\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nSum\n0\n15\n19\n6\n8\n2\n0\n0\n50\n\n\n\n\n\n\n\nAnd, in the narrative you can say that the maximum frequency of loan_purpose is 23. In the narrative you can alo say that the maximum frequency of grade is 19.\n\nOne student stipulated that the mean and median could not ever be the same except in two unusual circumstances. Actually it is quite easy for the mean to equal the mean as you can see from this simple example.\n\n\nx &lt;- c(1,2,3,4,5,6,7,8,9)\nmean(x)\n\n[1] 5\n\nmedian(x)\n\n[1] 5"
  },
  {
    "objectID": "week03.html#recap-week-02",
    "href": "week03.html#recap-week-02",
    "title": "3  More about R and Quarto",
    "section": "3.1 Recap Week 02",
    "text": "3.1 Recap Week 02\nWe did some exercises, for which there are now solutions in the file week02exercises-soln.qmd and week02exercises-soln.html. You should examine and compare these two files, especially the exercise parts."
  },
  {
    "objectID": "week03.html#week-03-more-on-r-and-quarto",
    "href": "week03.html#week-03-more-on-r-and-quarto",
    "title": "3  More about R and Quarto",
    "section": "3.2 Week 03: More on R and Quarto",
    "text": "3.2 Week 03: More on R and Quarto\nWe will establish groups for the milestones. I’m open to moving you around if needed, subject to the constraint that we have no more than five members in a group. I expect to have four groups of four and two groups of five.\n\n3.2.1 The template files\nThe template files show how you can begin the milestones. Notice that I have named the data frame as df in the template.qmd file. As a result, I can write functions like the following.\n\nlibrary(tidyverse)\ndf &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.csv\"))\ndf |&gt; count(title_status,sort=TRUE)\n\n# A tibble: 7 × 2\n  title_status      n\n  &lt;chr&gt;         &lt;int&gt;\n1 clean        405117\n2 &lt;NA&gt;           8242\n3 rebuilt        7219\n4 salvage        3868\n5 lien           1422\n6 missing         814\n7 parts only      198\n\n\nThe above is a good way to investigate categorical data. Notice that I’ve used the pipe character, so that the data frame df is sent to the count() function. Then a particular categorical column of df is counted and sorted in descending numerical order. The count() function is part of the dplyr package, which is one of the tidyverse packages. You only need to load the tidyverse set of packages once in a document, preferably near the beginning.\nAnother point about the above result is that the count() function behaves different for large numbers of values. For example, region has 404 values. The count() function will just display the first and last 5 by default. I can make it display all the values by adding the print fuction:\n\ndf |&gt; count(region,sort=TRUE) |&gt; print(n=404)\n\n# A tibble: 404 × 2\n    region                         n\n    &lt;chr&gt;                      &lt;int&gt;\n  1 columbus                    3608\n  2 jacksonville                3562\n  3 spokane / coeur d'alene     2988\n  4 eugene                      2985\n  5 fresno / madera             2983\n  6 orlando                     2983\n  7 bend                        2982\n  8 omaha / council bluffs      2982\n  9 kennewick-pasco-richland    2981\n 10 new hampshire               2981\n 11 nashville                   2980\n 12 salem                       2980\n 13 oklahoma city               2979\n 14 reno / tahoe                2979\n 15 boston                      2978\n 16 rochester                   2978\n 17 sarasota-bradenton          2977\n 18 stockton                    2977\n 19 boise                       2976\n 20 portland                    2976\n 21 houston                     2975\n 22 south jersey                2974\n 23 minneapolis / st paul       2973\n 24 modesto                     2973\n 25 philadelphia                2973\n 26 seattle-tacoma              2973\n 27 grand rapids                2972\n 28 baltimore                   2971\n 29 las vegas                   2971\n 30 milwaukee                   2971\n 31 pittsburgh                  2971\n 32 cincinnati                  2970\n 33 sacramento                  2970\n 34 tulsa                       2970\n 35 washington, DC              2970\n 36 austin                      2969\n 37 north jersey                2969\n 38 tucson                      2969\n 39 charlotte                   2968\n 40 maine                       2966\n 41 atlanta                     2965\n 42 hawaii                      2964\n 43 long island                 2964\n 44 central NJ                  2961\n 45 phoenix                     2960\n 46 detroit metro               2959\n 47 dallas / fort worth         2957\n 48 kansas city, MO             2957\n 49 ft myers / SW florida       2955\n 50 cleveland                   2953\n 51 san diego                   2953\n 52 albuquerque                 2952\n 53 denver                      2952\n 54 orange county               2952\n 55 inland empire               2950\n 56 new york city               2950\n 57 norfolk / hampton roads     2946\n 58 tampa bay area              2945\n 59 st louis, MO                2940\n 60 los angeles                 2937\n 61 SF bay area                 2936\n 62 chicago                     2929\n 63 des moines                  2923\n 64 south florida               2920\n 65 raleigh / durham / CH       2918\n 66 colorado springs            2914\n 67 san antonio                 2891\n 68 knoxville                   2763\n 69 anchorage / mat-su          2742\n 70 hartford                    2564\n 71 albany                      2537\n 72 bakersfield                 2528\n 73 redding                     2526\n 74 springfield                 2520\n 75 ventura county              2518\n 76 vermont                     2513\n 77 fayetteville                2415\n 78 madison                     2387\n 79 fort collins / north CO     2385\n 80 richmond                    2346\n 81 greenville / upstate        2327\n 82 rhode island                2320\n 83 bellingham                  2313\n 84 indianapolis                2303\n 85 akron / canton              2211\n 86 greensboro                  2078\n 87 western massachusetts       2039\n 88 buffalo                     2006\n 89 palm springs                1978\n 90 el paso                     1977\n 91 medford-ashland             1962\n 92 louisville                  1938\n 93 worcester / central MA      1914\n 94 little rock                 1841\n 95 ocala                       1828\n 96 dayton / springfield        1787\n 97 yuba-sutter                 1747\n 98 memphis                     1724\n 99 yakima                      1704\n100 billings                    1701\n101 wichita                     1694\n102 hudson valley               1692\n103 birmingham                  1647\n104 new haven                   1644\n105 daytona beach               1633\n106 charleston                  1509\n107 chico                       1486\n108 san luis obispo             1455\n109 monterey bay                1451\n110 asheville                   1423\n111 toledo                      1406\n112 columbia                    1388\n113 missoula                    1378\n114 lansing                     1367\n115 jackson                     1351\n116 space coast                 1346\n117 syracuse                    1338\n118 fredericksburg              1328\n119 wenatchee                   1324\n120 new orleans                 1321\n121 huntsville / decatur        1273\n122 appleton-oshkosh-FDL        1261\n123 lehigh valley               1261\n124 flint                       1258\n125 columbia / jeff city        1257\n126 bozeman                     1249\n127 kalamazoo                   1236\n128 western slope               1188\n129 corpus christi              1176\n130 treasure coast              1173\n131 corvallis/albany            1168\n132 roanoke                     1121\n133 lexington                   1115\n134 tri-cities                  1110\n135 ann arbor                   1085\n136 rockford                    1059\n137 east idaho                  1052\n138 santa barbara               1052\n139 lakeland                    1048\n140 myrtle beach                1043\n141 chattanooga                 1038\n142 green bay                   1033\n143 winston-salem               1027\n144 mcallen / edinburg          1019\n145 scranton / wilkes-barre     1008\n146 moses lake                   994\n147 tyler / east TX              992\n148 lancaster                    988\n149 kalispell                    978\n150 harrisburg                   976\n151 wilmington                   975\n152 fargo / moorhead             969\n153 south bend / michiana        969\n154 st cloud                     954\n155 delaware                     949\n156 gainesville                  926\n157 visalia-tulare               922\n158 eastern NC                   895\n159 flagstaff / sedona           881\n160 saginaw-midland-baycity      876\n161 hickory / lenoir             853\n162 eau claire                   846\n163 jersey shore                 838\n164 erie                         805\n165 santa fe / taos              801\n166 twin falls                   794\n167 fort wayne                   788\n168 clarksville                  785\n169 york                         777\n170 tallahassee                  771\n171 gold country                 765\n172 duluth / superior            763\n173 kenosha-racine               757\n174 prescott                     746\n175 pueblo                       746\n176 santa maria                  740\n177 baton rouge                  733\n178 wausau                       726\n179 east oregon                  720\n180 olympic peninsula            717\n181 lewiston / clarkston         710\n182 skagit / island / SJI        701\n183 boulder                      694\n184 oregon coast                 694\n185 macon / warner robins        687\n186 quad cities, IA/IL           687\n187 waco                         681\n188 winchester                   672\n189 youngstown                   664\n190 killeen / temple / ft hood   662\n191 merced                       654\n192 cedar rapids                 648\n193 south coast                  645\n194 charlottesville              642\n195 battle creek                 639\n196 mobile                       626\n197 pensacola                    622\n198 sioux falls / SE SD          621\n199 northern michigan            612\n200 wyoming                      610\n201 athens                       607\n202 utica-rome-oneida            607\n203 lincoln                      604\n204 cape cod / islands           598\n205 pullman / moscow             595\n206 wichita falls                589\n207 eastern CT                   583\n208 topeka                       582\n209 amarillo                     564\n210 southern illinois            555\n211 waterloo / cedar falls       555\n212 holland                      534\n213 brainerd                     533\n214 monroe                       530\n215 great falls                  524\n216 la crosse                    521\n217 savannah / hinesville        516\n218 rapid city / west SD         502\n219 lynchburg                    496\n220 lubbock                      492\n221 lima / findlay               485\n222 salt lake city               485\n223 augusta                      480\n224 southwest michigan           476\n225 binghamton                   474\n226 muskegon                     473\n227 poconos                      464\n228 north mississippi            462\n229 central michigan             461\n230 mankato                      445\n231 finger lakes                 439\n232 mohave county                435\n233 odessa / midland             433\n234 peoria                       432\n235 danville                     427\n236 fairbanks                    427\n237 shreveport                   422\n238 reading                      416\n239 bowling green                415\n240 northwest GA                 415\n241 watertown                    414\n242 evansville                   413\n243 montgomery                   408\n244 southern maryland            405\n245 northwest CT                 397\n246 sioux city                   393\n247 humboldt county              385\n248 elmira-corning               379\n249 harrisonburg                 375\n250 eastern shore                368\n251 bemidji                      365\n252 altoona-johnstown            364\n253 st george                    362\n254 williamsport                 362\n255 brownsville                  361\n256 mansfield                    359\n257 las cruces                   358\n258 port huron                   358\n259 upper peninsula              355\n260 janesville                   343\n261 klamath falls                343\n262 boone                        338\n263 jonesboro                    337\n264 laredo                       337\n265 yuma                         335\n266 joplin                       328\n267 st augustine                 328\n268 fort smith                   327\n269 dothan                       325\n270 florence                     324\n271 st joseph                    323\n272 college station              313\n273 zanesville / cambridge       313\n274 morgantown                   307\n275 ithaca                       303\n276 panama city                  298\n277 roseburg                     294\n278 frederick                    288\n279 beaumont / port arthur       287\n280 the thumb                    286\n281 annapolis                    285\n282 western maryland             285\n283 texoma                       284\n284 imperial county              282\n285 champaign urbana             281\n286 sheboygan                    281\n287 new river valley             275\n288 glens falls                  274\n289 ashtabula                    270\n290 northern panhandle           268\n291 parkersburg-marietta         264\n292 northern WI                  262\n293 valdosta                     262\n294 lafayette / west lafayette   260\n295 chillicothe                  257\n296 plattsburgh-adirondacks      256\n297 helena                       255\n298 southwest VA                 254\n299 iowa city                    253\n300 eastern panhandle            251\n301 manhattan                    251\n302 muncie / anderson            248\n303 southeast missouri           245\n304 sandusky                     244\n305 victoria                     243\n306 sierra vista                 241\n307 mendocino county             238\n308 lake of the ozarks           236\n309 abilene                      235\n310 north central FL             230\n311 dubuque                      228\n312 galveston                    225\n313 chautauqua                   221\n314 kenai peninsula              221\n315 bloomington                  217\n316 bloomington-normal           217\n317 mason city                   217\n318 eastern kentucky             213\n319 grand island                 212\n320 western KY                   212\n321 texarkana                    211\n322 state college                210\n323 gadsden-anniston             207\n324 lawton                       207\n325 florida keys                 203\n326 lawrence                     199\n327 stillwater                   198\n328 terre haute                  198\n329 ames                         194\n330 okaloosa / walton            194\n331 grand forks                  192\n332 cookeville                   189\n333 brunswick                    187\n334 hilton head                  186\n335 cumberland valley            179\n336 lafayette                    179\n337 san marcos                   179\n338 hanford-corcoran             178\n339 meadville                    177\n340 southeast IA                 175\n341 decatur                      171\n342 heartland florida            170\n343 high rockies                 169\n344 salina                       168\n345 florence / muscle shoals     165\n346 tuscaloosa                   162\n347 san angelo                   161\n348 kokomo                       156\n349 lake charles                 156\n350 mattoon-charleston           156\n351 tuscarawas co                154\n352 logan                        152\n353 catskills                    147\n354 northwest OK                 145\n355 northwest KS                 143\n356 auburn                       142\n357 owensboro                    140\n358 statesboro                   140\n359 farmington                   136\n360 twin tiers NY/PA             136\n361 butte                        134\n362 central louisiana            132\n363 gulfport / biloxi            127\n364 north dakota                 126\n365 elko                         120\n366 kirksville                   119\n367 southeast KS                 119\n368 susanville                   119\n369 huntington-ashland           118\n370 scottsbluff / panhandle      117\n371 la salle co                  116\n372 show low                     112\n373 outer banks                  109\n374 del rio / eagle pass         108\n375 potsdam-canton-massena       102\n376 roswell / carlsbad           100\n377 hattiesburg                   99\n378 southwest KS                  96\n379 bismarck                      92\n380 south dakota                  90\n381 deep east texas               88\n382 houma                         88\n383 fort dodge                    87\n384 southeast alaska              84\n385 siskiyou county               83\n386 north platte                  80\n387 clovis / portales             78\n388 ogden-clearfield              76\n389 eastern montana               75\n390 provo / orem                  75\n391 western IL                    63\n392 oneonta                       62\n393 southwest MN                  56\n394 pierre / central SD           55\n395 eastern CO                    40\n396 southern WV                   36\n397 st louis                      35\n398 northeast SD                  34\n399 southwest TX                  30\n400 meridian                      28\n401 southwest MS                  14\n402 kansas city                   11\n403 fort smith, AR                 9\n404 west virginia (old)            8\n\n\nThe number 404 is because there are 404 unique elements in the list, which you can find out by looking at the output of the count() function. Of course, in a report for a manager or recruiter I would be unlikely to print out a column of 404 numbers. A manager or recruiter would be more interested in some sort of summary. This is primarily for your preparation.\nAnother solution (more cumbersome) follows.\n\ndf %&gt;%\n  group_by(region) %&gt;%\n  do(data.frame(nrow=nrow(.))) %&gt;%\n  arrange(desc(nrow)) %&gt;%\n  print(n=40)\n\n# A tibble: 404 × 2\n# Groups:   region [404]\n   region                    nrow\n   &lt;chr&gt;                    &lt;int&gt;\n 1 columbus                  3608\n 2 jacksonville              3562\n 3 spokane / coeur d'alene   2988\n 4 eugene                    2985\n 5 fresno / madera           2983\n 6 orlando                   2983\n 7 bend                      2982\n 8 omaha / council bluffs    2982\n 9 kennewick-pasco-richland  2981\n10 new hampshire             2981\n11 nashville                 2980\n12 salem                     2980\n13 oklahoma city             2979\n14 reno / tahoe              2979\n15 boston                    2978\n16 rochester                 2978\n17 sarasota-bradenton        2977\n18 stockton                  2977\n19 boise                     2976\n20 portland                  2976\n21 houston                   2975\n22 south jersey              2974\n23 minneapolis / st paul     2973\n24 modesto                   2973\n25 philadelphia              2973\n26 seattle-tacoma            2973\n27 grand rapids              2972\n28 baltimore                 2971\n29 las vegas                 2971\n30 milwaukee                 2971\n31 pittsburgh                2971\n32 cincinnati                2970\n33 sacramento                2970\n34 tulsa                     2970\n35 washington, DC            2970\n36 austin                    2969\n37 north jersey              2969\n38 tucson                    2969\n39 charlotte                 2968\n40 maine                     2966\n# ℹ 364 more rows\n\n\nAnother way (and a better one for high level understanding) to investigate categorical data is through contingency tables. You have already made some of these using the table() function and some associated functions that are mentioned in the week02exercises-soln.qmd file. It will take some intuition to figure out which pairs of categorical columns should be tabulated for Milestone 1.\n\n\n3.2.2 Numerical Data\nFor numerical data, you can do what we did last week to investigate a single column.\n\nsummary(df$price)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.000e+00 5.900e+03 1.395e+04 7.520e+04 2.649e+04 3.737e+09 \n\n\nor\n\nlibrary(scales)\ndf$price &lt;- as.double(df$price)\ndf %&gt;%\n    summarize(Min=comma(min(price)),\n              firstq=comma(quantile(price,0.25)),\n              Median=comma(median(price)),\n              Mean=comma(mean(price)),\n              thirdq=comma(quantile(price,0.75)),\n              Max=comma(max(price)))\n\n# A tibble: 1 × 6\n  Min   firstq Median Mean   thirdq Max          \n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;        \n1 0     5,900  13,950 75,199 26,486 3,736,928,711\n\ndf$price &lt;- as.integer(df$price)\n\nWarning: NAs introduced by coercion to integer range\n\n\nYou’ll notice that one of the prices is 3.7 billion dollars for a Toyota truck! This is obviously a misprint! You should probably remove this row from the data frame and save the data frame without it. A more sophisticated alternative would be to impute a value for this truck. There are many advanced statistical ways to do this, but they are beyond the scope of this course. van Buuren (2018) describes several excellent ways to do so, particularly in that book’s Section 5.1.1, Recommended Workflows. It is usually a mistake to use an average for missing (NA) values because to do so compresses the variance unnaturally. You may remove that particular row by saying df &lt;- df[-(which.max(df$price)),]. Unfortunately, you will find that not to be very useful because there are several prices of over a billion dollars for generic cars! In fact, there are several with prices listed as 1234567890 and some listed at 987654321. Many other ridiculous patterns can be found for price. So what can you do? Personally, I might add the following line right after reading in the file:\n\ndf &lt;- df[df$price&lt;100000&df$price&gt;0,]\n\nor, better yet,\n\ndf &lt;- df[df$price &lt; quantile(df$price,.9,na.rm=TRUE) & df$price &gt; quantile(df$price,.1,na.rm=TRUE),]\n\nThe first one will rid the data frame of cars priced at greater than 100,000 dollars and still leave you with over 300,000 automobiles to analyze. Of course, there are will still be spurious entries, but at least it’s a start. The first code will also get rid of cars priced at exactly zero dollars. Examination of the data frame will show that many of the zero dollar entries are just ads for used car dealers. The expression price&lt;100000&price&gt;0 is called a compound Boolean expression. It is compound because of the ampersand, which stands for the word and. It means that the row has to contain a price less than 100,000 AND it also has to be a price greater than zero.\nThe second code gets rid of the 90th percentile and above and the 10th percentile and below, which will still leave plenty.\nBy the way, this is a good reminder that a data frame has a row and a column index. You can refer to rows by saying df[expression,] and to columns by saying df[,expression]. The expression can be any mathematical expression that resolves to TRUE or FALSE. The first above expression resolves to TRUE for cars priced at greater than zero but less than 100,000 dollars, and FALSE for cars priced at any integer greater than or equal to 100,000. You can tell that price is a 64 bit integer by saying str(df) which will tell the structure of the df data frame. You should be able to see that most of the columns are classified as chr or character. This is not desirable. Most of the columns clasified as chr should more properly be classified as factors. Factors take up less space on your computer, are faster to process, and allow more types of processing than chr. Unfortunately, if you store your intermediate work as a .csv file, you will lose the factor designation. Therefore, I recommend that you do the following.\nStep 1. Get rid of rows you don’t want, such as those with prices over or under some threshold value you choose.\nStep 2. Get rid of columns you don’t want to analyze, such as url or VIN.\nStep 3. Convert some of the chr columns to factor. For instance, you can say df$state &lt;- as.factor(df$state).\nStep 4. Save your file by saying something like save(df,file=\"df.RData\")\nStep 5. Quit using this file and open a file called intermediate.qmd\nStep 6. At the beginning of that file, say load(\"df.RData\").\nStep 7. Do all your work in that file, then paste the work back into your template.qmd file so you can run it as required. (Remember, you are not turning in a .RData file. Your m1.qmd file must start with reading in the vehicles.csv file and do processing on the resulting data frame.)\nStep 8. Merge your template.qmd file with those of your group members into one m1.qmd file. For example, you could name all your individual template files with your names and one group member could merge them together. This should be easy for Milestone 1 since an obvious way to divide up your work is to assign different columns to different group members.\n\n\n3.2.3 Combining numerical and categorical selection\n\ndfX &lt;- subset(df,state %in% c(\"ca\",\"ny\") & type %in% c(\"sedan\",\"SUV\") & price&lt;99999 & price&gt;0)\ntbl &lt;- table(dfX$state,dfX$type)\naddmargins(tbl)\n\n     \n      sedan   SUV   Sum\n  ca  10313  6537 16850\n  ny   3487  2953  6440\n  Sum 13800  9490 23290\n\n\nAbove is an example of getting a small contingency table with only the data you want. The first line selects only cars offered in ca or ny, only sedans or SUVs, and only with prices below 99,999 dollars and more than zero dollars. Then we can make a compact contingency table of that new data frame and add the margins to it.\n\n\n3.2.4 Getting the data displayed as you wish\nSomeone asked me how to display price ranges by manufacturer. Here’s one way to do that:\n\ndf |&gt;\n  group_by(manufacturer) |&gt;\n  reframe(min = min(price),max=max(price)) |&gt;\n  print(n=43)\n\n# A tibble: 43 × 3\n   manufacturer      min   max\n   &lt;chr&gt;           &lt;int&gt; &lt;int&gt;\n 1 acura             574 37500\n 2 alfa-romeo       1000 37500\n 3 aston-martin     1947 37000\n 4 audi              504 37509\n 5 bmw               501 37500\n 6 buick             521 37500\n 7 cadillac          507 37500\n 8 chevrolet         502 37587\n 9 chrysler          539 37500\n10 datsun           2200 30000\n11 dodge             513 37500\n12 ferrari          2034 35000\n13 fiat              800 32500\n14 ford              502 37513\n15 gmc               501 37550\n16 harley-davidson  2500 27995\n17 honda             502 37500\n18 hyundai           543 36900\n19 infiniti          517 37500\n20 jaguar            514 37388\n21 jeep              501 37587\n22 kia               529 37500\n23 land rover       3199 10999\n24 lexus             521 37500\n25 lincoln           541 37495\n26 mazda             515 36995\n27 mercedes-benz     502 37509\n28 mercury           600 37495\n29 mini              600 35990\n30 mitsubishi        538 37500\n31 morgan           1000 36500\n32 nissan            522 37575\n33 pontiac           600 37500\n34 porsche           560 37500\n35 ram               502 37556\n36 rover             507 37500\n37 saturn            600 22000\n38 subaru            501 37200\n39 tesla             563 36999\n40 toyota            505 37583\n41 volkswagen        507 37000\n42 volvo             502 37000\n43 &lt;NA&gt;               NA    NA\n\n\nThe number 43 is because there are 43 manufacturers in the data frame. Note that, using the reframe() function, I could add a few more comma-separated statistics to the output.\n\n\n3.2.5 Investigating words\nTo make a word cloud, I first exported the model column from the data frame to a file called bla, using the write_csv() function. Next I used Vim to convert all spaces to newlines, so that the file would have one word on each line. To do so I said :%s/ /\\r/g in Vim.\nNext I said sort bla | uniq -c | sort -n &gt;blabla to get the following output. This is just the last few lines of the file. Note that the most frequently occuring word in the file is 1500, which occurs 24,014 times.\n3383    fusion\n3475    tundra\n3479    xl\n3528    corolla\n3585    unlimited\n3985    altima\n3985    explorer\n4072    3500\n4105    f-250\n4256    mustang\n4277    escape\n5162    camry\n5208    series\n5244    pickup\n5277    NA\n5479    accord\n5585    xlt\n5659    awd\n5660    cherokee\n5667    f150\n5680    tacoma\n5734    civic\n5744    s\n5865    2d\n6185    limited\n6213    lt\n6295    coupe\n6519    crew\n6869    premium\n7190    duty\n7319    se\n7531    2500\n7564    utility\n8093    wrangler\n8327    super\n8642    sierra\n8733    grand\n9578    4x4\n10283   f-150\n14877   sedan\n15152   cab\n17181   silverado\n17488   4d\n23130   sport\n24014   1500\nNext I opened the blabla file in Vim and converted all sequences of spaces to a tab character, saying :%s/^  *// to get rid of leading spaces, then :%s/  */\\t/ to convert the intercolumn spaces to tabs. I saved this file as wordfreq.tsv and opened it in R, using the following code to convert it to a word cloud.\n\nlibrary(tidyverse)\ndf&lt;-read_tsv(\"wordfreq.tsv\")\ndf&lt;-df|&gt;relocate(freq,.after=word)\nhead(df)\n\n# A tibble: 6 × 2\n  word         freq\n  &lt;chr&gt;       &lt;dbl&gt;\n1 \"!\"           102\n2 \"!!\"            4\n3 \"!!!\"           2\n4 \"!!!\\\"\"         1\n5 \"!!clean!!\"     2\n6 \"!!leveled\"     2\n\nlibrary(wordcloud2)\nwordcloud2(df)\n\n\n\n\n\n\nI was not able to reproduce the error message I kept getting in class. This simply worked the first time through after class. I also can not explain why several of the most frequently occurring words do not appear in the word cloud. I suspect it is because the word cloud is truncated in this display. When I have constructed this word cloud previously, it was ellipse-shaped and zoomed out. I have forgotten whatever I did to make that happen!\n\n\n3.2.6 Filtering two specific trucks\nTwo of the most common words I found above were f-150 and silverado. Since these are two popular truck models, I thought to compare them by making a data frame for each. To do so I used the filter() function of the dplyr package. This is described in great detail in Chapter 4 of Wickham, Çetinkaya-Rundel, and Grolemund (2023). The special construction (?i) makes whatever follows case-insensitive. Thus, in this case, it picks up Silverado and SILVERADO, as well as silverado. It would also pick up sIlVeRaDo if such a strange mixed case version presented itself. This is called a regular expression or regex and is commonly used in finding and replacing text patterns.\nThe regexes for f-150 are much more complicated. First, I used the alternating selector [Ff]. This stands for either a capital F or small f but not both. I can put any number of characters in the brackets and this will select any of them occurring one time. Next, I used the zero-or one character selector, which consists of a dot followed by a question mark. That selector attaches itself to whatever precedes it, in this case [Ff]. So the whole construct [Ff].? can be read as “exactly one F or f followed by exactly one character.” Next comes a literal 150, so the only time that an F or f plus at most one character will be matched is if it is immediately followed by 150. The last construct in this regular expression is the negating alternator, [^] with a zero in it. The negating alternator matches anything except the characters following the ^ in brackets. In this case it means that any character except a zero is okay. It’s actually a vestige of an earlier attempt. I had previously written the expression as [Ff].*150 and run it and it erroneously picked up a model string that said “pacifica $1500”. That was because I used a * instead of a ?. The * symbol means zero or more characters. As a result, the words “pacifica $1500” matched because there is an f followed by some characters, followed by 150! So I stuck the negating alternator [^0] in to get rid of the 15000 before I realized that the real problem was the *. I report this so you can see that it is a potentially long iterative process to find the right regex. This regex picks up f150, F 150, f-150, and so on. I could have refined it further. Can you see how?\n\nlibrary(tidyverse)\n#df &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.csv\"))\n#load(\"/home/mm223266/data/vehicles.Rdata\")\n#df &lt;- read_csv(\"/home/mm223266/data/vehicles.csv\")\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.Rdata\"))\n#names(df)\ndf2 &lt;- df |&gt; filter(str_detect(df$model,regex(\"(?i)silverado\")))\nhead(table(df2$model),12)\n\n\n      1500 sierra silverado              1500 silverado \n                          1                          23 \n         1500 silverado 4x4      1500 silverado ltz z71 \n                          5                           2 \n      1500 silverado sierra        1500 silverado titan \n                          1                           2 \n         1500 silverado z71          1980 Silverado K10 \n                          1                           1 \n             2000 Silverado              2006 Silverado \n                          1                           1 \n     2006 SILVERADO 2500 HD 2007 Silverado crew cab 4x4 \n                          1                           1 \n\nprint(\"::::::::::::::::::::::::::::::::::\")\n\n[1] \"::::::::::::::::::::::::::::::::::\"\n\ndf3 &lt;- df |&gt; filter(str_detect(df$model,regex(\"[Ff].?150[^0]\")))\nhead(table(df3$model),12)\n\n\n              1999 F150 4X4               1999 f150 xlt \n                          1                           2 \n      2000 F150 4x4 5 speed      2000 F150 XLT SUPERCAB \n                          1                           1 \n             2002 F150 4 WD               2004 f150 4x4 \n                          1                           1 \n              2004 F150 XLT               2005 f150 fx4 \n                          1                           1 \n           2006 F150 Lariat     2006 f150 Supercrew Cab \n                          1                           1 \n2012 F-150 Lariat SuperCrew        2013 F150 KING RANCH \n                          1                           1 \n\n\n\n\n3.2.7 A function between numeric and visual\nMilestone 1 is supposed to be entirely about numeric descriptions of the data, not visual descriptions, which will be covered in Milestone 2. Yet there is one function that exists in a gray area between numeric and graphical. That is the stem and leaf plot. Consider the following output.\n\nstem(df$odometer)\n\n\n  The decimal point is 6 digit(s) to the right of the |\n\n  0 | 00000000000000000000000000000000000000000000000000000000000000000000+421502\n  1 | 00000000000000000000000000000000000000000000000000000000000000000000+562\n  2 | 000000000000000000111122223333333335555566666677789\n  3 | 02333357\n  4 | 0577777777\n  5 | 005556666666669\n  6 | 15\n  7 | 555678888888888888\n  8 | 04789999\n  9 | 000189\n  10 | 00000000000000000000000000000000000000000000000000000000000000000000+58\n\n\nThe output does not need any graphical processor. It is only characters that can be included in text. Yet it is a kind of graphic because you can see, for instance, that of the cars have either very little mileage on the odometer or very much. Read it like this:\n\nThe stem is the vertical line.\nThe numbers to the left of the stem are, in this case, numbers in the sixth place to the left of the decimal point. In other words the first row represents zero to 999999.\nEach character to the right of the stem represents one car. There are probably 80 zeros in the first row. The +390151 indicates that there are 390,151 cars in that category that are not represented. The numbers in these cases represent the next significant digit after the one on the stem.\nIt is probably easier to read a stem and leaf plot for a smaller data frame, in the following case for the first 100 cars in the above data frame.\n\n\nstem(df$odometer[1:100])\n\n\n  The decimal point is 4 digit(s) to the right of the |\n\n   0 | 0222788900012223447789999\n   2 | 11267799000004555678\n   4 | 01123335568\n   6 | 39117\n   8 | 0804567\n  10 | 0\n  12 | 8\n  14 | 5\n  16 | 6\n  18 | 2\n\n\nThese entries come from a reduced data frame where I first ran the above code, getting rid of the high-priced and free cars. It may make it easier to understand to look at the entries themselves.\n\nhead(df$odometer,n=100L) |&gt; sort(decreasing=TRUE)\n\n [1] 192000 176144 144700 128000  99615  97000  96003  95000  94020  90000\n[11]  88000  80318  77087  71229  70760  68696  63129  57923  55783  55251\n[21]  55068  43182  43000  42755  41568  41124  40784  39508  37725  37332\n[31]  35835  35320  35290  34940  34152  30237  30176  30047  30041  29652\n[41]  29499  28942  26978  26685  26129  22120  20856  20581  19179  19160\n[51]  18650  18531  17805  17302  16594  14230  14169  13035  12302  12231\n[61]  12102  10688   9954   9859   9704   8663   8141   7885   6897   2195\n[71]   1834   1722     21\n\n\nThe very first row in the stem and leaf plot above counts cars priced at less than 20,000 dollars. There are 28 of them. They all have 0 or 1 in the fifth position to the left of the decimal. Only one of those, which is offered at 21 dollars, has zeros in both of the first two positions. It is the very first entry after the stem, represented as a zero. The next three entries are the cars that sell for the next lowest prices, between zero and 2 in the next decimal position. They are represented as 2s. You can see at a glance that, in this group of 100 cars, the lower odometer readings predominate. By the way, the stem() function discards NA values before processing the remainder. So there are only a total of 78 characters to the right of the stem on all the rows put together.\nThere is some difference of opinion as to whether stem() is graphical or numerical. What do you think?\n\n\n3.2.8 More on Quarto\nIt may surprise you to know that Quarto, the tool you’re using to write your reports for this course, has other uses as well. All the syllabuses for my courses are written in Quarto, as are all my slideshows. Why not just use Microsoft Word and Powerpoint for these things? There are a great many reasons but the ones that concern us for this course are chiefly reproducible research and literate programming. If I required you to use GitHub for your group projects, there would be a third: version control. Consider this last one first. The following cartoon from the archive of phdcomics highlights the problem of version control using Microsoft Word to write your papers.\n\n\n\nversion control cartoon\n\n\nThe illustrated problem concerns only a single grad student and advisor, but the problem gets worse in a group project. The solution is to break the work into parts and use a shared version control system to keep track of the changes made by different people so you can revert some text to an earlier version if needed and integrate all the parts at the end. Quarto enables this by being a plain text system. Everything you write is a plain text document without hidden binary data. It can be opened in your choice of text editor. This is valuable because most data scientists (like programmers) have a preferred text editor and don’t like to be told which one to use. It is also valuable because a tracking system like Git can track all changes in the file. We may even require the use of Git in a future iteration of this course because of the many issues that groups experience in trying to integrate their files. (If you only use the RStudio interface to your file, it may be a surprise that it contains only plain text since you see color coding and panels with graphical results. These are addons of RStudio and the underlying file is the same with or without these addons. I usually use a plain text editor called Vim to edit Quarto files, with a terminal window open on R for pasting in code from the document to check it. I do this because it’s quicker than RStudio and takes up less screen real estate.)\nAs for literate programming, that is defined as writing programs for people to read as much as for machines to read. By integrating R code and narrative in one file, you make it possible for people to understand your code better than they could if documentation were written separately.\nReproducible research is the most important of the three. There is a widely discussed phenomenon in the scientific community called the reproducibility crisis. That crisis is that much of academic research results can not be reproduced by other researchers. There are many reasons proposed for the reproducibility crisis and at least as many solutions proposed. One of the best is to leave an audit trail and this is enabled by Quatro’s blend of narrative and code. This is why I insist that you start with raw data and document all your transformations in your qmd file.\n\n\n\n\nvan Buuren, Stef. 2018. Flexible Imputation of Missing Data. Boca Raton, FL: CRC Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd ed. O’Reilly Media, Inc. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "week04.html#recap-week-03",
    "href": "week04.html#recap-week-03",
    "title": "4  Introduction to Probability",
    "section": "4.1 Recap Week 03",
    "text": "4.1 Recap Week 03\nWe did some exercises, for which there are now solutions in the file week03exercises-soln.qmd and week03exercises-soln.html. You should examine and compare these two files.\nLast semester, for each student, we calculated the number of exercises successfully completed. Following is a stem and leaf diagram, as well as summary statistics.\n\nx&lt;-scan(\"week03exercisesList.txt\")\nstem(x)\n\n\n  The decimal point is at the |\n\n   0 | 000\n   2 | 0\n   4 | 0000000\n   6 | 00000000\n   8 | 0000000\n  10 | 0\n\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    4.50    6.00    5.63    8.00   10.00 \n\nsd(x)\n\n[1] 2.691127\n\n\nYou can make a number of inferences from this information. First, we should generally expect the average student to be able to complete six exercises during class time. If you completed fewer, you may need extra work outside class. If you completed more, you probably need less time outside of class. We can also see that the mean has been dragged downward by the students who didn’t turn anything in, so the median is a better measure of centrality.\nWe can compare the stem and leaf plot to the following histogram and see that the stem and leaf plot looks kind of like a histogram turned on its side, and with a bit more information, especially if we alter the scale, so that it’s not compressed to two exercises per row.\n\nhist(x)\n\n\n\nstem(x,scale=2)\n\n\n  The decimal point is at the |\n\n   0 | 000\n   1 | \n   2 | 0\n   3 | \n   4 | 000\n   5 | 0000\n   6 | 00000\n   7 | 000\n   8 | 000000\n   9 | 0\n  10 | 0\n\n\nWe also talked about milestone 1, for which there are some hints in the previous chapter.\n\n4.1.1 Name value pairs\nIt came to my attention that not everyone knows what a name value pair is or how to read it. The name is always on the left and the value is always on the right. Usually there is an equal sign in between, but it doesn’t mean equals. Instead it means gets. So, for example, in ggplot, there are aesthetics x and y. You can say something like x=bill_depth_mm and y=bill_length_mm. You would read the first one as “x gets bill depth in millimeters”. It would make no sense to say the reverse. Don’t ever say bill_depth_mm=x. Instead, let x and y be the names that ggplot knows about, the x-axis and the y-axis, and bill_depth_mm and bill_length_mm be the values that we plug into x and y.\nSimilarly, I found out that not everyone knows what we mean by local and remote. The local machine is your laptop. It is the machine close to you. The remote machine is the machine that hosts RStudio Server, the machine you connect to when you access the URL for RStudio. When you work in RStudio on the RStudio server, your work gets saved to the remote machine. Then you have to download it to your local machine and upload it to the machine that hosts Canvas. There is no direct path between the two remote machines that host RStudio Server and Canvas."
  },
  {
    "objectID": "week04.html#bayesian-approach",
    "href": "week04.html#bayesian-approach",
    "title": "4  Introduction to Probability",
    "section": "4.2 Bayesian approach",
    "text": "4.2 Bayesian approach\nThere are two competing schools of thought about what probability is. The bayesian approach is that probability is quantified belief or reasonable expectation of the outcomes of events based on a state of knowledge. This approach is recently taught in graduate schools. We will not extensively study this approach, but I want you to know that it exists and is rising in academic popularity."
  },
  {
    "objectID": "week04.html#frequentist-approach",
    "href": "week04.html#frequentist-approach",
    "title": "4  Introduction to Probability",
    "section": "4.3 Frequentist approach",
    "text": "4.3 Frequentist approach\nOur textbook takes a frequentist approach to probability, one of the two main approaches to probability and the one usually taught in undergraduate courses in the USA. This approach models probability of an outcome as the number of times the outcome would occur if we observed the random process that produced it an infinite number of times. For example, if we flip a fair coin an infinite number of times, it comes up heads half the time, so the probability of heads is 0.5."
  },
  {
    "objectID": "week04.html#the-law-of-large-numbers",
    "href": "week04.html#the-law-of-large-numbers",
    "title": "4  Introduction to Probability",
    "section": "4.4 The law of large numbers",
    "text": "4.4 The law of large numbers\nThis law claims that, as more outcomes are observed, the proportion of outcomes converges to the probability of the outcome. For example, if we flip a fair coin a hundred times, the probability of heads coming up half the time is greater than if we only flip it ten times.\nThe textbook uses the examples of rolling fair dice and flipping fair coins. Gaston Sanchez gives the example of flipping a fair coin modeled in R.\n\n#. number of flips\nnum_flips &lt;- 1000\n\n#. flips simulation\ncoin &lt;- c('heads', 'tails')\nflips &lt;- sample(coin, size = num_flips, replace = TRUE)\n\n#. number of heads and tails\nfreqs &lt;- table(flips)\nfreqs\n\nflips\nheads tails \n  481   519 \n\nheads_freq &lt;- cumsum(flips == 'heads') / 1:num_flips\nplot(heads_freq,      # vector\n     type = 'l',      # line type\n     lwd = 2,         # width of line\n     col = 'tomato',  # color of line\n     las = 1,         # orientation of tick-mark labels\n     ylim = c(0, 1),  # range of y-axis\n     xlab = \"number of tosses\",    # x-axis label\n     ylab = \"relative frequency\")  # y-axis label\nabline(h = 0.5, col = 'gray50')"
  },
  {
    "objectID": "week04.html#disjoint-outcomes",
    "href": "week04.html#disjoint-outcomes",
    "title": "4  Introduction to Probability",
    "section": "4.5 Disjoint outcomes",
    "text": "4.5 Disjoint outcomes\nThese are outcomes that can not both happen. For example, in the fair coin flipping case, the outcome cannot be both heads and tails. But the sum of all the disjoint probabilities is always 1."
  },
  {
    "objectID": "week04.html#probabilities-when-outcomes-are-not-disjoint",
    "href": "week04.html#probabilities-when-outcomes-are-not-disjoint",
    "title": "4  Introduction to Probability",
    "section": "4.6 Probabilities when outcomes are not disjoint",
    "text": "4.6 Probabilities when outcomes are not disjoint\nThe textbook uses playing cards to illustrate concepts like cards that are neither diamonds nor face cards. You have to familiarize yourself with playing cards to understand these examples. The textbook uses the following Venn diagram to illustrate the above example."
  },
  {
    "objectID": "week04.html#general-addition-rule",
    "href": "week04.html#general-addition-rule",
    "title": "4  Introduction to Probability",
    "section": "4.7 General addition rule",
    "text": "4.7 General addition rule\nThe textbook gives a general rule for multiple outcomes, whether they are disjoint or not.\nIf A and B are any two events, disjoint or not, then the probability that at least one of them will occur is\n\\[\nP (A\\text{ or }B) = P (A) + P (B) − P (A\\text{ and }B)\n\\]\nwhere \\(P (A\\text{ and }B)\\) is the probability that both events occur."
  },
  {
    "objectID": "week04.html#counting-permutations-and-combinations",
    "href": "week04.html#counting-permutations-and-combinations",
    "title": "4  Introduction to Probability",
    "section": "4.8 Counting Permutations and Combinations",
    "text": "4.8 Counting Permutations and Combinations\nPermutations can be thought of as lineups. For instance, suppose you have five people to put in a line. There are five people to choose from to be first in line, then four people remain to be second in line, and so on. You can count this up as \\(5 \\times 4 \\times 3 \\times 2 \\times 1 = 5!\\) or five factorial. This holds true for as many objects as you wish to line up.\nCombinations can be thought of as committees. There is no order as in a lineup. You’re either a member or you’re not. Suppose you want to form a committee of five people from among twenty people. It doesn’t matter what order they come in so you can’t use the factorial method to count them. There is another method, which you can find described in detail in Ash (1993). The main result is that, to choose a committee of 5 from among 20 people, use\n\\[\n\\binom{20}{5} = \\frac{20!}{5!(20-5)!}\n\\]\nThis is read as twenty choose 5.\nKeep in mind that\n\\[\n\\binom{n}{r}=\\binom{n}{n-r}\n\\]\nand\n\\[\n\\binom{n}{1}=n\n\\]\nand\n\\[\n\\binom{n}{n}=\\binom{n}{0}=1\n\\]\nThis last result is because \\(0!=1\\) by definition.\nAsh (1993) gives the examples of finding and not finding the Queen of Spades (\\(Q_s\\)) in a poker hand. You can think of a poker hand as a committee of 5 cards drawn from 52, so the total number of poker hands is given by \\(\\binom{52}{5}\\). Finding hands containing the \\(Q_s\\) amounts to choosing a committee of size four (the remainder of the hand, from among 51 cards (the remainder of the deck. So there are \\(\\binom{51}{4}\\) such hands.\n\\[\nP(Q_s)=\\dfrac{\\binom{51}{4}}{\\binom{52}{5}}=\\frac{5}{52}\n\\]\nKeep in mind when canceling in stacked fractions that\n\\[\n\\dfrac{\\frac{a}{b}}{\\frac{c}{d}}=\\dfrac{a \\cdot d}{b \\cdot c}\n\\]"
  },
  {
    "objectID": "week04.html#probability-distributions",
    "href": "week04.html#probability-distributions",
    "title": "4  Introduction to Probability",
    "section": "4.9 Probability distributions",
    "text": "4.9 Probability distributions\nA probability distribution is a list of the possible outcomes with corresponding probabilities that satisfies three rules:\n\nThe outcomes listed must be disjoint.\nEach probability must be between 0 and 1.\nThe probabilities must total 1."
  },
  {
    "objectID": "week04.html#probability-distribution-for-two-fair-dice",
    "href": "week04.html#probability-distribution-for-two-fair-dice",
    "title": "4  Introduction to Probability",
    "section": "4.10 Probability distribution for two fair dice",
    "text": "4.10 Probability distribution for two fair dice\nFrancis DiTraglia shows the following example of plotting the probability distribution for rolling two fair dice on his website.\n\ntwo.dice &lt;- function() {\n  dice &lt;- sample(1:6, size = 2, replace = TRUE)\n  return(sum(dice))\n}\nsims &lt;- replicate(1000, two.dice())\nplot(table(sims), xlab = 'Sum', ylab = 'Frequency', main = '100 Rolls of 2 Fair Dice')\n\n\n\n\nWhy is 7 the most likely outcome?"
  },
  {
    "objectID": "week04.html#terms-from-set-theory",
    "href": "week04.html#terms-from-set-theory",
    "title": "4  Introduction to Probability",
    "section": "4.11 Terms from set theory",
    "text": "4.11 Terms from set theory\nSet theory is a mathematical discipline that uses tools like Venn diagrams to describes sets of objects. We can think of outcomes from random processes as objects, too, with the following terms.\n\nsample space: the set of all possible outcomes\nevent: a particular outcome\ncomplement of an event: outcomes in the sample space outside a given event or events\n\nThe complement of event \\(A\\) is denoted \\(A^c\\), and \\(A^c\\) represents all outcomes not in \\(A\\). \\(A\\) and \\(A^c\\) are mathematically related:\n\\[\nP (A) + P (A^c) = 1, \\text{ i.e. } P (A) = 1 − P (A^c)\n\\]"
  },
  {
    "objectID": "week04.html#independence",
    "href": "week04.html#independence",
    "title": "4  Introduction to Probability",
    "section": "4.12 Independence",
    "text": "4.12 Independence\nJust as variables and observations can be independent, random processes can be independent, too. Two processes are independent if knowing the outcome of one provides no useful information about the outcome of the other. For instance, flipping a coin and rolling a die are two independent processes—knowing the coin was heads does not help determine the outcome of a die roll. On the other hand, stock prices usually move up or down together, so they are not independent."
  },
  {
    "objectID": "week04.html#multiplication-rule-for-independent-processes",
    "href": "week04.html#multiplication-rule-for-independent-processes",
    "title": "4  Introduction to Probability",
    "section": "4.13 Multiplication rule for independent processes",
    "text": "4.13 Multiplication rule for independent processes\nIf \\(A\\) and \\(B\\) represent events from two different and independent processes, then the probability that both \\(A\\) and \\(B\\) occur can be calculated as the product of their separate probabilities:\n\\[\nP (A\\text{ and }B) = P (A) × P (B)\n\\]\nSimilarly, if there are \\(k\\) events \\(A_1, \\ldots, A_k\\) from \\(k\\) independent processes, then the probability they all occur is\n\\[\nP (A_1) × P (A_2) × \\cdots × P (A_k)\n\\]"
  },
  {
    "objectID": "week04.html#conditional-probability",
    "href": "week04.html#conditional-probability",
    "title": "4  Introduction to Probability",
    "section": "4.14 Conditional probability",
    "text": "4.14 Conditional probability\nThis is where probability gets interesting. Some things depend on other things! The textbook uses a contingency table of the photos_classify data frame, which you can download from OpenIntro Stats, to explore this concept.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/photo_classify.rda\"))\nstr(photo_classify)\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   1822 obs. of  2 variables:\n $ mach_learn: Factor w/ 2 levels \"pred_fashion\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ truth     : Factor w/ 2 levels \"fashion\",\"not\": 1 1 1 1 1 1 1 1 1 1 ...\n\naddmargins(table(photo_classify))\n\n              truth\nmach_learn     fashion  not  Sum\n  pred_fashion     197   22  219\n  pred_not         112 1491 1603\n  Sum              309 1513 1822\n\n\nWe can use the entries in the contingency table to make statements about probability.\n\nprobability that a fashion photo is correctly classified by ML: 197/309\nprobability that a given photo is about fashion when predicted by ML to be not: 112/1603\n\nMarginal probability is the probability in the margins of table (right column and bottom row), e.g., ML predicts fashion photo at all: 219/1822.\nJoint probability is the probabillity of two (or more) things being true, e.g., ML predicts fashion and truth is fashion: 197/1822. A joint probability would be any of the four interior cells divided by the lower right cell.\nConditional probability is the probability of some outcome given the condition of another outcome, such as the probability that ML predicts fashion when the photo is truly about fashion: 197/219.\nConditional probability is very important. It’s useful to know the general formula for conditional probability. The conditional probability of outcome \\(A\\) given condition \\(B\\) is computed as the following:\n\\[\nP (A|B) = \\frac{P (A\\text{ and }B)}{P (B)}\n\\]"
  },
  {
    "objectID": "week04.html#general-multiplication-rule",
    "href": "week04.html#general-multiplication-rule",
    "title": "4  Introduction to Probability",
    "section": "4.15 General multiplication rule",
    "text": "4.15 General multiplication rule\nWe already saw a specific multiplication rule for independent events. But there is a more general rule, applicable whether independence is true or not. If \\(A\\) and \\(B\\) represent two outcomes or events, then\n\\[\nP (A\\text{ and }B) = P (A|B) × P (B)\n\\]\nIt is useful to think of \\(A\\) as the outcome of interest and \\(B\\) as the condition."
  },
  {
    "objectID": "week04.html#tree-diagrams-of-probability",
    "href": "week04.html#tree-diagrams-of-probability",
    "title": "4  Introduction to Probability",
    "section": "4.16 Tree diagrams of probability",
    "text": "4.16 Tree diagrams of probability\nThe textbook claims that tree diagrams aid our thinking about conditional probabilities. Luckily, law professor Harry Surden provides an example of a tree diagram of probabilities on his website.\n\n#.. R Conditional Probability Tree Diagram\n\n#.. The Rgraphviz graphing package must be installed to do this\nlibrary(Rgraphviz)\n\n#.. Change the three variables below to match your actual values\n#.. These are the values that you can change for your own probability tree\n#.. From these three values, other probabilities (e.g. prob(b)) will be calculated\n\n#.. Probability of a\na&lt;-.01\n\n#.. Probability (b | a)\nbGivena&lt;-.99\n\n#. Probability (b | ¬a)\nbGivenNota&lt;-.10\n\n###################### Everything below here will be calculated\n\n#. Calculate the rest of the values based upon the 3 variables above\nnotbGivena&lt;-1-bGivena\nnotA&lt;-1-a\nnotbGivenNota&lt;-1-bGivenNota\n\n#. Joint Probabilities of a and B, a and notb, nota and b, nota and notb\naANDb&lt;-a*bGivena\naANDnotb&lt;-a*notbGivena\nnotaANDb &lt;- notA*bGivenNota\nnotaANDnotb &lt;- notA*notbGivenNota\n\n#. Probability of B\nb&lt;- aANDb + notaANDb\nnotB &lt;- 1-b\n\n#. Bayes theorum - probabiliyt of A | B\n#. (a | b) = Prob (a AND b) / prob (b)\naGivenb &lt;- aANDb / b\n\n#. These are the labels of the nodes on the graph\n#. To signify \"Not A\" - we use A' or A prime\n\nnode1&lt;-\"P\"\nnode2&lt;-\"A\"\nnode3&lt;-\"A'\"\nnode4&lt;-\"A&B\"\nnode5&lt;-\"A&B'\"\nnode6&lt;-\"A'&B\"\nnode7&lt;-\"A'&B'\"\nnodeNames&lt;-c(node1,node2,node3,node4, node5,node6, node7)\n\nrEG &lt;- new(\"graphNEL\", nodes=nodeNames, edgemode=\"directed\")\n\n#. Draw the \"lines\" or \"branches\" of the probability Tree\nrEG &lt;- addEdge(nodeNames[1], nodeNames[2], rEG, 1)\nrEG &lt;- addEdge(nodeNames[1], nodeNames[3], rEG, 1)\nrEG &lt;- addEdge(nodeNames[2], nodeNames[4], rEG, 1)\nrEG &lt;- addEdge(nodeNames[2], nodeNames[5], rEG, 1)\nrEG &lt;- addEdge(nodeNames[3], nodeNames[6], rEG, 1)\nrEG &lt;- addEdge(nodeNames[3], nodeNames[7], rEG, 10)\n\neAttrs &lt;- list()\n\nq&lt;-edgeNames(rEG)\n\n#. Add the probability values to the the branch lines\n\neAttrs$label &lt;- c(toString(a),toString(notA),\n toString(bGivena), toString(notbGivena),\n toString(bGivenNota), toString(notbGivenNota))\nnames(eAttrs$label) &lt;- c(q[1],q[2], q[3], q[4], q[5], q[6])\nedgeAttrs&lt;-eAttrs\n\n#. Set the color, etc, of the tree\nattributes&lt;-list(node=list(label=\"foo\", fillcolor=\"lightgreen\", fontsize=\"15\"),\n edge=list(color=\"red\"),graph=list(rankdir=\"LR\"))\n\n#. Plot the probability tree using Rgraphvis\nplot(rEG, edgeAttrs=eAttrs, attrs=attributes)\nnodes(rEG)\n\n[1] \"P\"     \"A\"     \"A'\"    \"A&B\"   \"A&B'\"  \"A'&B\"  \"A'&B'\"\n\nedges(rEG)\n\n$P\n[1] \"A\"  \"A'\"\n\n$A\n[1] \"A&B\"  \"A&B'\"\n\n$`A'`\n[1] \"A'&B\"  \"A'&B'\"\n\n$`A&B`\ncharacter(0)\n\n$`A&B'`\ncharacter(0)\n\n$`A'&B`\ncharacter(0)\n\n$`A'&B'`\ncharacter(0)\n\n#. Add the probability values to the leaves of A&B, A&B', A'&B, A'&B'\ntext(500,420,aANDb, cex=.8)\n\ntext(500,280,aANDnotb,cex=.8)\n\ntext(500,160,notaANDb,cex=.8)\n\ntext(500,30,notaANDnotb,cex=.8)\n\ntext(340,440,\"(B | A)\",cex=.8)\n\ntext(340,230,\"(B | A')\",cex=.8)\n\n#. Write a table in the lower left of the probablites of A and B\ntext(80,50,paste(\"P(A):\",a),cex=.9, col=\"darkgreen\")\ntext(80,20,paste(\"P(A'):\",notA),cex=.9, col=\"darkgreen\")\n\ntext(160,50,paste(\"P(B):\",round(b,digits=2)),cex=.9)\ntext(160,20,paste(\"P(B'):\",round(notB, 2)),cex=.9)\n\ntext(80,420,paste(\"P(A|B): \",round(aGivenb,digits=2)),cex=.9,col=\"blue\")\n\n\n\n\nIt’s pretty ugly because it was designed for a different width and height than I have allocated, but it gives the general idea."
  },
  {
    "objectID": "week04.html#bayes-theorem",
    "href": "week04.html#bayes-theorem",
    "title": "4  Introduction to Probability",
    "section": "4.17 Bayes’ Theorem",
    "text": "4.17 Bayes’ Theorem\nIt’s the centerpiece of Bayesian statistics so it’s a bit like a fish out of water in a frequentist course. It states, in words, that the posterior probability of an outcome \\(A\\) given an outcome \\(B\\) is the likelihood of the outcome \\(B\\) times the prior probability of the outcome \\(A\\) divided by the evidence of outcome \\(B\\). More succinctly,\n\\[\nP(A|B)=\\frac{P(B|A)P(A)}{P(B)}\n\\]\nThe intuition is that we usually know something and shouldn’t go into problem solving with no assumptions. An example of drug user testing given by Wikipedia is shown below graphically, where the test is ninety percent sensitive to a recipient being a drug user. The test is also eighty percent specific, meaning that it can detect that a non-user is a non-user eighty percent of the time.\n\nWhat does this tell us about drug testing? Even if someone tests positive, the probability that they are a drug user is only 19%! This assumes prior knowledge that five percent of the general population are users of the drug.\nIn Bayesian terms:\n\\[\n\\frac{0.9 \\times 0.05}{0.9\\times 0.05 + 0.2\\times0.95}\n\\] By the way, this formulation uses the law of total probability in the denominator, expanding \\(P(B)\\) into\n\\[\nP(B|A)P(A)+P(B|\\neg A)P(\\neg A)\n\\]\nwhere \\(\\neg A\\) is the complement of \\(A\\).\nThis is very important to know about because, for example, someone might only tell you that a test is ninety percent sensitive and eighty percent specific and leave out the Bayes rule result that says that, given that only five percent of the general population are drug users, there’s a nineteen percent chance that testing positive indicates that you are a drug user. Again, Bayes’ rule is very important when you have some prior knowledge. In this case, the prior knowledge is a study that says that five percent of the general population are users of this drug."
  },
  {
    "objectID": "week04.html#sampling-from-a-small-population",
    "href": "week04.html#sampling-from-a-small-population",
    "title": "4  Introduction to Probability",
    "section": "4.18 Sampling from a small population",
    "text": "4.18 Sampling from a small population\nRecall that the population includes every object and is usually not practical to measure. For examples, every fish, every person, every thunderstorm are too many to represent. So we take a sample. A typical rule of thumb is that, if we can sample more than ten percent of the population, we regard it as a small population.\nThe textbook gives an example of being called on by the professor. The chance that you are called on in a class of 15 is 1/15. If the professor calls on three different people in succession, the chance that you are called on increases to 1/5:\n\\[\\begin{align}\nP(\\neg\\text{3 in a row}) &=\\\\\n&=P(\\text{not picked first, second, third})\\\\\n&=\\frac{14}{15}\\times\\frac{13}{14}\\times\\frac{12}{13}\\\\\n&=\\frac{12}{15}\\\\\n\\end{align}\\]\nand the complement of 12/15 is 1/5."
  },
  {
    "objectID": "week04.html#random-variables",
    "href": "week04.html#random-variables",
    "title": "4  Introduction to Probability",
    "section": "4.19 Random variables",
    "text": "4.19 Random variables\nA process with a random numerical outcome is called a random variable. It’s kind of like a stochastic function in that there is an input (the process) and output (the outcome number).\nA random variable is usually represented as a capital, italicized Latin letter, e.g., \\(X, Y, Z\\). Specific outcomes are usually represented as a lowercase, italicized Latin letter with a subscript to denote which outcome, such as \\(x_1, x_2, x_3\\). The probability that a random variable \\(X\\) has a specific outcome \\(x_1\\) is represented as \\(P(X=x_1)\\).\nThe expectation of \\(X\\) is the expected value of \\(X\\), represented as \\(E(X)\\). The expected value is typically the average, but not always. If there are \\(k\\) possible outcomes, then\n\\[\nE(X)=\\sum_{i=1}^kx_iP(X=x_i)\n\\]\nThe Greek letter \\(\\sum\\) (Sigma) denotes a sum of a series of numbers, indexed in this case by \\(i\\). You can read it in English as the sum, going from \\(1\\) to \\(k\\), of the expressions to the right of the Sigma sign. In other words,\n\\[\nx_1P(X=x_1)+x_2P(X=x_2)+\\cdots+x_kP(X=x_k)\n\\]\nIt’s the average as we usually understand it if each outcome is equally probable, in which case it’s the sum of the outcomes divided by the number of outcomes.\nWriters often substitute \\(E(X)=\\mu\\) which can be confusing because Greek letters are usually used as parameters, while Latin letters are usually used as specific realizations of those parameters.\nThe above assumes there are \\(k\\) specific outcomes. We call this case a discrete random variable. It’s also possible to do math with a continuous random variable. In other words, \\(k=\\infty\\). However, that requires calculus so the textbook skips it for now.\n\n4.19.1 Variability in random variables\nRecall that random variables are a kind of mapping between a process and specific numerical outcomes. Those specific outcomes differ. For example, the revenue of a store varies day by day, and we can say something about that variability.\nWe usually express it by two related concepts: variance (denoted by a lowercase sigma squared or \\(\\sigma^2\\)), and its square root, standard deviation (denoted by a lowercase sigma or \\(\\sigma\\)). You might wonder why we don’t just choose one of these symbols. Most statisticians just use standard deviation, but to prove that it is an unbiased estimator of variance, we need to square it for mathematical reasons that are beyond the scope of this course.\nStandard deviation is expressed in the same units as the subject under consideration, whereas variance is expressed in squared units. So when I said at the beginning of this file that the standard deviation of your completed exercises was about 2.69, I meant that in terms of number of exercises, meaning that most of you were within 2.69 completed exercises either way of each other. In other words, if you had three or nine completed exercises, you were a kind of outlier.\n\n\n4.19.2 Linear combinations of random variables\nWe can put random variables together. For example, your GPA is calculated from a set of grades that may differ. If you play fantasy sports, your score comes from many different players. A recommender system for music listening may make calculations based on many different songs you listened to.\nA linear combination of two random variables can be expressed as \\(aX+bY\\), where \\(a\\) and \\(b\\) are fixed constants, for example, the number of credits applied to each class in your GPA portfolio. If you take four four credit classes, your GPA might be expressed as the linear combination\n\\[\n4E(X_1)+4E(X_2)+4E(X_3)+4E(X_4)\n\\]\nwhere \\(E(X_i)\\) is a function of your grade, such as your grade mapped to a number and divided by the number of credits you’re taking. (Of course, you are used to thinking of it as the average of the grades since most classes carry the same number of credits and your grades are mapped to numbers. But this formulation holds for all kinds of sets.)"
  },
  {
    "objectID": "week04.html#continuous-distributions",
    "href": "week04.html#continuous-distributions",
    "title": "4  Introduction to Probability",
    "section": "4.20 Continuous distributions",
    "text": "4.20 Continuous distributions\nSo far, we’ve considered discrete distributions, where \\(k\\) takes on one of a finite set of values. What about the continuous case? For example, temperature can theoretically take on an infinite number of values, even though we only have the tools for discrete measurements.\nThe most famous continuous distribution is the normal distribution. This picture illustrates the normal distribution. The mound-shaped curve represents the probability density function and the area between the curve and the horizontal line represents the value of the cumulative distribution function.\n\n\n\n\n\nConsider a normally distributed nationwide test.\nThe total shaded area between the curve and the straight horizontal line can be thought of as one hundred percent of that area. In the world of probability, we measure that area as 1. The curve is symmetrical, so measure all the area to the left of the highest point on the curve as 0.5. That is half, or fifty percent, of the total area between the curve and the horizontal line at the bottom. Instead of saying area between the curve and the horizontal line at the bottom, people usually say the area under the curve.\nFor any value along the \\(x\\)-axis, the \\(y\\)-value on the curve represents the value of the probability density function.\nThe area bounded by the vertical line between the \\(x\\)-axis and the corresponding \\(y\\)-value on the curve, though, is what we are usually interested in because that area represents probability.\nHere is a graph of the size of that area. It’s called the cumulative distribution function (cdf).\n\n\n\n\n\nThe above graph can be read as having an input and output that correspond to the previous graph of the probability density function. As we move from right to left on the \\(x\\)-axis, the area that would be to the left of a given point on the probability density function is the \\(y\\)-value on this graph. For example, if we go half way across the \\(x\\)-axis of the probability density function, the area to its left is one half of the total area, so the \\(y\\)-value on the cumulative distribution function graph is one half.\nThe shape of the cumulative distribution function is called a sigmoid curve. You can see how it gets this shape by looking again at the probability density function graph above. As you move from left to right on that graph, the area under the curve increases very slowly, then more rapidly, then slowly again. The places where the area grows more rapidly and then more slowly on the probability density function curve correspond to the s-shaped bends on the cumulative distribution curve.\nAt the left side of the cumulative distribution curve, the \\(y\\)-value is zero meaning zero probability. When we reach the right side of the cumulative distribution curve, the \\(y\\)-value is 1 or 100 percent of the probability.\nLet’s get back to the example of a nationwide test. If we say that students nationwide took an test that had a mean score of 75 and that the score was normally distributed, we’re saying that the value on the \\(x\\)-axis in the center of the curve is 75. Moreover, we’re saying that the area to the left of 75 is one half of the total area. We’re saying that the probability of a score less than 75 is 0.5 or fifty percent. We’re saying that half the students got a score below 75 and half got a score above 75.\nThat is called the frequentist interpretation of probability. In general, that interpretation says that a probability of 0.5 is properly measured by saying that, if we could repeat the event enough times, we would find the event happening half of those times.\nFurthermore, the frequentist interpretation of the normal distribution is that, if we could collect enough data, such as administering the above test to thousands of students, we would see that the graph of the frequency of their scores would look more and more like the bell curve in the picture, where \\(x\\) is a test score and \\(y\\) is the number of students receiving that score.\nSuppose we have the same test and the same distribution but that the mean score is 60. Then 60 is in the middle and half the students are on each side. That is easy to measure. But what if, in either case, we would like to know the probability associated with scores that are not at that convenient midpoint?\nIt’s hard to measure any other area under the normal curve except for \\(x\\)-values in the middle of the curve, corresponding to one half of the area. Why is this?\nTo see why it’s hard to measure the area corresponding to any value except the middle value, let’s first consider a different probability distribution, the uniform distribution. Suppose I have a machine that can generate any number between 0 and 1 at random. Further, suppose that any such number is just as likely as any other such number.\nHere’s a graph of the the uniform distribution of numbers generated by the machine. The horizontal line is the probability density function and the shaded area is the cumulative distribution function from 0 to 1/2. In other words, the probability of the machine generating numbers from 0 to 1/2 is 1/2. The probability of generating numbers from 0 to 1 is 1, the area of the entire rectangle.\n\n\n\n\n\nIt’s very easy to calculate any probability for this distribution, in contrast to the normal distribution. The reason it is easy is that you can just use the formula for the area of a rectangle, where area is base times side. The probability of being in the entire rectangle is \\(1\\times1=1\\), and the probability of being in the part from \\(x=0\\) to \\(x=1/4\\) is just \\(1\\times(1/4)=1/4\\). The cumulative distribution function of the uniform distribution is simpler than that of the normal distribution because area is being added at the same rate as we move from left to right on the above graph. Therefore it is just a straight diagonal line from (0,1) on the left to (1,1) on the right.\n\n\n\n\n\nReading it is the same as reading the cumulative distribution function for the normal distribution. For any value on the \\(x\\)-axis, say, 1/2, go up to the diagonal line and over to the value on the \\(y\\)-axis. In this case, that value is 1/2. That is the area under the horizontal line in the probability density function graph from 0 to 1/2 (the shaded area). For a rectangle, calculating area is trivial.\nCalculating the area of a curved region like the normal distribution can be more difficult. If you’ve studied any calculus, you know that there are techniques for calculating the area under a curve. These techniques are called integration techniques. In the case of the normal distribution the formula for the height of the curve at any point on the \\(x\\)-axis is\n\\[\\begin{equation*}\n\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}\n\\end{equation*}\\]\nand the area is the integral of that quantity from \\(-\\infty\\) to \\(x\\), which can be rewritten as\n\\[\\begin{equation*}\n\\frac{1}{\\sqrt{2\\pi}}\\int^x_{-\\infty}e^{-t^2/2}dt\n=(1/2)\\left(1+\\text{erf}\\left(\\frac{x-\\mu}{\\sigma\\sqrt{2}}\\right)\\right)\n\\end{equation*}\\]\nThe integral on the left is difficult to evaluate so people use numerical approximation techniques to find the expression on the right in the above equation. Those techniques are so time-consuming that, rather than recompute them every time they are needed, a very few people used to write the results into a table and publish it and most people working with probability would just consult the tables. Only in the past few decades have calculators become available that can do the tedious approximations. Hence, most statistics books, written by people who were educated decades ago, still teach you how to use such tables. There is some debate as to whether there is educational value in using the tables vs using calculators or smartphone apps or web-based tables or apps."
  },
  {
    "objectID": "week04.html#the-f-distribution",
    "href": "week04.html#the-f-distribution",
    "title": "4  Introduction to Probability",
    "section": "4.21 The F distribution",
    "text": "4.21 The F distribution\nAnother distribution that we’ll enounter later is called the F distribution. We’ll calculate an F-statistic when we build linear regression models, but I just want you to know the general shape of it for now. The region marked \\(\\alpha\\) corresponds inversely to the magnitude of the F statistic. In other words, a larger F statistic means a smaller \\(\\alpha\\).\n\n\n\n\n\n\n\n\n\nAsh, Carol. 1993. The Probability Tutoring Book. New York, NY: IEEE Press."
  },
  {
    "objectID": "week05.html#recap-week-04",
    "href": "week05.html#recap-week-04",
    "title": "5  Distributions of Random Variables",
    "section": "5.1 Recap Week 04",
    "text": "5.1 Recap Week 04\nWe did week 04 in one day, covering numerous probability basics and doing a few exercises in probability, mostly paper and pencil exercises. We saw three examples of continuous probability distributions: normal, uniform, and F. This week, we’ll examine continuous distributions in some more detail."
  },
  {
    "objectID": "week05.html#five-distributions",
    "href": "week05.html#five-distributions",
    "title": "5  Distributions of Random Variables",
    "section": "5.2 Five distributions",
    "text": "5.2 Five distributions\nWe previously looked at the normal distribution. In this Chapter of the textbook, we visit five distributions:\n\nNormal distribution\nGeometric distribution\nBinomial distribution\nNegative binomial distribution\nPoisson distribution\n\nTo save time, we will follow the textbook instead of lecture notes.\nThe following picture illustrates the normal distribution. The mound-shaped curve represents the probability density function and the area between the curve and the horizontal line represents the value of the cumulative distribution function.\n\n\n\n\n\nConsider a normally distributed nationwide test.\nThe total shaded area between the curve and the straight horizontal line can be thought of as one hundred percent of that area. In the world of probability, we measure that area as 1. The curve is symmetrical, so measure all the area to the left of the highest point on the curve as 0.5. That is half, or fifty percent, of the total area between the curve and the horizontal line at the bottom. Instead of saying area between the curve and the horizontal line at the bottom, people usually say the area under the curve.\nFor any value along the \\(x\\)-axis, the \\(y\\)-value on the curve represents the value of the probability density function.\nThe area bounded by the vertical line between the \\(x\\)-axis and the corresponding \\(y\\)-value on the curve, though, is what we are usually interested in because that area represents probability."
  },
  {
    "objectID": "week05.html#notation",
    "href": "week05.html#notation",
    "title": "5  Distributions of Random Variables",
    "section": "5.3 Notation",
    "text": "5.3 Notation\n\\(N(\\mu,\\sigma)\\) is said to characterize a normal distribution. If we know the parameters \\(\\mu\\), the mean, and \\(\\sigma\\), the standard deviation, we have a complete picture of the normal distribution, namely the height of the hump and its width."
  },
  {
    "objectID": "week05.html#r-function-to-find-probability",
    "href": "week05.html#r-function-to-find-probability",
    "title": "5  Distributions of Random Variables",
    "section": "5.4 R function to find probability",
    "text": "5.4 R function to find probability\nThe pnorm() function takes a value of a normally distributed random variable and returns the probability of that value or less. The textbook gives an example of the SAT and ACT scores. For the SAT, the textbook claims that \\(\\mu = 1100\\) and \\(\\sigma=200\\) so we can say the SAT score \\(x\\sim N(1100,200)\\) which is read as “x has the normal distribution with mean 1100 and standard deviation 200.” Suppose you score 1100. What is the probability that someone will score lower than you?\n\npnorm(1100,mean=1100,sd=200)\n\n[1] 0.5\n\n\nThis returns a value of 0.5 or fifty percent. That’s something we can tell without a computer, but for the normal distribution, other values are difficult to compute by hand or guess correctly. For instance,\n\npnorm(1400,mean=1100,sd=200)\n\n[1] 0.9331928\n\n\nreturns a value of 0.9331928, or that 93 percent of students get below this score.\nBecause all probabilities sum to 1, we can use this information to tell the probability of someone getting a higher score, or the probability of getting a score between two scores by subtracting the smaller from the larger.\nThe textbook gives another example as finding the probability that a particular student scores at least 1190:\n\n1 - pnorm(1190,mean=1100,sd=200)\n\n[1] 0.3263552\n\n\nwhich returns 32.64 percent. Note that we did not explicitly calculate the \\(Z\\)-score that leads to this probability. The \\(Z\\)-score is a way of standardizing so that instead of (in this case) \\(x\\sim N(1100,200)\\), we calculate the probability of \\(Z\\sim N(0,1)\\). Here, we standardize by recognizing that \\(Z=(x-\\mu)/\\sigma\\) or \\((1190-1100)/200=0.45\\) and then we can say\n\n1 - pnorm(0.45,mean=0,sd=1)\n\n[1] 0.3263552\n\n\nand we get the same answer as above, 32.64 percent."
  },
  {
    "objectID": "week05.html#geometric-distribution",
    "href": "week05.html#geometric-distribution",
    "title": "5  Distributions of Random Variables",
    "section": "5.5 Geometric distribution",
    "text": "5.5 Geometric distribution\nFirst we need Bernoulli random variables to understand the Geometric distribution.\nIf \\(X\\) is a random variable that takes value 1 with probability of success \\(p\\) and \\(0\\) with probability \\(1 − p\\), then \\(X\\) is a Bernoulli random variable with mean and standard deviation \\(\\mu=p\\) and \\(\\sigma=\\sqrt{p(1-p)}\\). A Bernoulli random variable is a process with only two outcomes: success or failure. Flipping a coin and calling it is an example of Bernoulli random variable.\nNow the question is “What happens if you flip a coin or roll the dice or some other win/lose process many times in a row?”\nThe geometric distribution describes how many times it takes to obtain success in a series of Bernoulli trials.\nThe textbook (p. 145) gives an example where an insurance company employee is looking for the first person to meet a criteria where the probability of meeting that criteria is 0.7 or seventy percent. We can calculate the probability of 0 failures (the first person, one failure (the second person), and so on, using R:\n\na &lt;- dgeom(x=0,prob=0.7) # the first person\nb &lt;- dgeom(x=1,prob=0.7) # the second person\nc &lt;- dgeom(x=2,prob=0.7) # the third person\nd &lt;- dgeom(x=3,prob=0.7) # the fourth person\ne &lt;- dgeom(x=4,prob=0.7) # the fifth person\n\nand if we graph these numbers, we’ll find they have the property of exponential decay.\n\nx&lt;-c(a,b,c,d,e)\nplot(x)\n\n\n\n\nThe textbook gives the following definition of the geometric distribution.\nIf the probability of a success in one trial is \\(p\\) and the probability of a failure is \\(1 − p\\), then the probability of finding the first success in the \\(n\\)th trial is given by \\((1 − p)^{n−1}p\\). The mean (i.e. expected value) and standard deviation of this wait time are given by \\(μ = 1/p\\), \\(σ = \\sqrt{(1 − p)/p^2}\\)."
  },
  {
    "objectID": "week05.html#binomial-distribution",
    "href": "week05.html#binomial-distribution",
    "title": "5  Distributions of Random Variables",
    "section": "5.6 Binomial distribution",
    "text": "5.6 Binomial distribution\nThe textbook says that the binomial distribution is used to describe the number of successes in a fixed number of trials. This is different from the geometric distribution, which described the number of trials we must wait before we observe a success.\nSuppose the probability of a single trial being a success is \\(p\\). Then the probability of observing exactly \\(k\\) successes in \\(n\\) independent trials is given by\n\\[\n\\binom{n}{k}p^k(1-p)^{n-k}\n\\]\nand \\(\\mu=np\\), \\(\\sigma=\\sqrt{np(1-p)}\\)\n\n5.6.1 Example using R\nThe website r-tutor.com gives the following problem as an example:\n\n5.6.1.1 Problem\nSuppose there are twelve multiple choice questions in an English class quiz. Each question has five possible answers, and only one of them is correct. Find the probability of having four or fewer correct answers if a student attempts to answer every question at random.\n\n\n5.6.1.2 Solution\nSince only one out of five possible answers is correct, the probability of answering a question correctly by random is 1/5=0.2. We can find the probability of having exactly 4 correct answers by random attempts as follows.\n\ndbinom(4, size=12, prob=0.2)\n\n[1] 0.1328756\n\n\nwhich returns 0.1329.\nTo find the probability of having four or less correct answers by random attempts, we apply the function dbinom with \\(x = 0,\\ldots,4\\).\n\ndbinom(0, size=12, prob=0.2) +\ndbinom(1, size=12, prob=0.2) +\ndbinom(2, size=12, prob=0.2) +\ndbinom(3, size=12, prob=0.2) +\ndbinom(4, size=12, prob=0.2)\n\n[1] 0.9274445\n\n\nwhich returns 0.9274.\nAlternatively, we can use the cumulative probability function for binomial distribution pbinom.\n\npbinom(4, size=12, prob=0.2)\n\n[1] 0.9274445\n\n\nwhich returns the same answer.\n\n\n5.6.1.3 Answer\nThe probability of four or fewer questions answered correctly by random in a twelve question multiple choice quiz is 92.7%."
  },
  {
    "objectID": "week05.html#negative-binomial-distribution",
    "href": "week05.html#negative-binomial-distribution",
    "title": "5  Distributions of Random Variables",
    "section": "5.7 Negative binomial distribution",
    "text": "5.7 Negative binomial distribution\nThis is a generalization of the geometric distribution, defined in the textbook as follows.\nThe negative binomial distribution describes the probability of observing the \\(k\\)th success on the \\(n\\)th trial, where all trials are independent:\n\\[\nP (\\text{the kth success on the nth trial}) =\n\\binom{n − 1}{k − 1} p^k(1 − p)^{n−k}\n\\]\nThe value \\(p\\) represents the probability that an individual trial is a success."
  },
  {
    "objectID": "week05.html#poisson-distribution",
    "href": "week05.html#poisson-distribution",
    "title": "5  Distributions of Random Variables",
    "section": "5.8 Poisson distribution",
    "text": "5.8 Poisson distribution\nThe textbook defines the Poisson distribution as follows.\nSuppose we are watching for events and the number of observed events follows a Poisson distribution with rate \\(λ\\). Then\n\\[\nP (\\text{observe k events}) = \\frac{λ^ke^{−λ}}{k!}\n\\]\nwhere \\(k\\) may take a value 0, 1, 2, and so on, and \\(k!\\) represents \\(k\\)-factorial, as described on page 150. The letter e ≈ 2.718 is the base of the natural logarithm. The mean and standard deviation of this distribution are λ and √λ, respectively.\nThe r-tutor website mentioned above offers the following example of a Poisson distribution problem solved using R.\n\n5.8.0.1 Problem\nIf there are twelve cars crossing a bridge per minute on average, find the probability of having seventeen or more cars crossing the bridge in a particular minute.\n\n\n5.8.0.2 Solution\nThe probability of having sixteen or fewer cars crossing the bridge in a particular minute is given by the function ppois.\n\nppois(16, lambda=12)   # lower tail\n\n[1] 0.898709\n\n\nwhich returns 0.89871\nHence the probability of having seventeen or more cars crossing the bridge in a minute is in the upper tail of the probability density function.\n\nppois(16, lambda=12, lower=FALSE)   # upper tail\n\n[1] 0.101291\n\n\nwhich returns 0.10129\n\n\n5.8.0.3 Answer\nIf there are twelve cars crossing a bridge per minute on average, the probability of having seventeen or more cars crossing the bridge in a particular minute is 10.1%."
  },
  {
    "objectID": "week06.html#recap-week-05",
    "href": "week06.html#recap-week-05",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.1 Recap Week 05",
    "text": "6.1 Recap Week 05\n\nNormal distribution\nGeometric distribution\nBinomial distribution\nNegative binomial distribution\nPoisson distribution"
  },
  {
    "objectID": "week06.html#ggplot2",
    "href": "week06.html#ggplot2",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.2 ggplot2",
    "text": "6.2 ggplot2\nWe’ll follow the ggplot2 cheatsheet, available at https://posit.co/wp-content/uploads/2022/10/data-visualization-1.pdf.\nBy the way, there are many more cheatsheets for different aspects of R and RStudio available at https://posit.co/resources/cheatsheets/."
  },
  {
    "objectID": "week06.html#base-layer",
    "href": "week06.html#base-layer",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.3 Base layer",
    "text": "6.3 Base layer\n\nggplot(data = mpg, aes(x = cty, y = hwy))\n\n\n\n\nThe above code doesn’t produce a plot by itself. We would have to add a layer, such as a geom or stat. (Every geom has a default stat and every stat has a default geom.) For example,\n\nggplot(data = mpg, aes(x = cty, y = hwy)) +\n  geom_point()\n\n\n\n\nadds a scatterplot, whereas\n\nggplot(data = mpg, aes(x = cty, y = hwy)) +\n  geom_point(aes(color=displ,size=displ))\n\n\n\n\nadds the aesthetics from the second example on the cheatsheet.\n\n6.3.1 Graphical primitives\n\na &lt;- ggplot(economics,aes(date,unemploy))\nb &lt;- ggplot(seals,aes(x=long,y=lat))\na + geom_blank()\n\n\n\na + expand_limits()\n\nWarning in max(lengths(data)): no non-missing arguments to max; returning -Inf\n\n\n\n\nb + geom_curve(aes(yend = lat + 1, xend = long + 1), curvature = 1)\n\n\n\na + geom_path(lineend = \"butt\", linejoin = \"round\", linemitre = 1)\n\n\n\na + geom_polygon(aes(alpha = 50))\n\n\n\nb + geom_rect(aes(xmin = long, ymin = lat, xmax = long + 1, ymax = lat + 1))\n\n\n\na + geom_ribbon(aes(ymin = unemploy - 900, ymax = unemploy + 900))\n\n\n\n\n\n6.3.1.1 Line segments\n\nb + geom_abline(aes(intercept = 0, slope = 1))\n\n\n\nb + geom_hline(aes(yintercept = lat))\n\n\n\nb + geom_vline(aes(xintercept = long))\n\n\n\n\n\n\n\n6.3.2 One variable—continuous\n\nc &lt;- ggplot(mpg, aes(hwy)); c2 &lt;- ggplot(mpg)\nc + geom_area(stat = \"bin\")\n\n\n\nc + geom_density(kernel = \"gaussian\")\n\n\n\nc + geom_dotplot()\n\n\n\nc + geom_freqpoly()\n\n\n\nc + geom_histogram(binwidth = 5)\n\n\n\nc2 + geom_qq(aes(sample = hwy))\n\n\n\n\n\n\n6.3.3 One variable—discrete\n\nd &lt;- ggplot(mpg, aes(fl))\nd + geom_bar()\n\n\n\n\n\n\n6.3.4 Two variables—both continuous\n\ne &lt;- ggplot(mpg,aes(cty,hwy))\ne + geom_label(aes(label = cty), nudge_x = 1, nudge_y = 1)\n\n\n\ne + geom_point()\n\n\n\ne + geom_quantile()\n\n\n\ne + geom_rug(sides = \"bl\")\n\n\n\ne + geom_smooth(method = lm)\n\n\n\ne + geom_text(aes(label = cty), nudge_x = 1, nudge_y = 1)\n\n\n\n\n\n\n6.3.5 Two variables—one discrete, one continuous\n\nf &lt;- ggplot(mpg,aes(class,hwy))\nf + geom_col()\n\n\n\nf + geom_boxplot()\n\n\n\nf + geom_dotplot(binaxis = \"y\", stackdir = \"center\")\n\n\n\nf + geom_violin(scale = \"area\")\n\n\n\n\n\n\n6.3.6 Two variables—both discrete\n\ng &lt;- ggplot(diamonds, aes(cut, color))\ng + geom_count()\n\n\n\ng + geom_jitter(height = 2, width = 2)\n\n\n\n\n\n\n6.3.7 Continuous bivariate distribution\n\nh &lt;- ggplot(diamonds, aes(carat, price))\nh + geom_bin2d(binwidth = c(0.25, 500))\n\n\n\nh + geom_density_2d()\n\n\n\nh + geom_hex()\n\nWarning: Computation failed in `stat_binhex()`\nCaused by error in `compute_group()`:\n! The package \"hexbin\" is required for `stat_binhex()`\n\n\n\n\n\n\n\n6.3.8 Continuous function\n\ni &lt;- ggplot(economics, aes(date, unemploy))\ni + geom_area()\n\n\n\ni + geom_line()\n\n\n\ni + geom_step(direction = \"hv\")\n\n\n\n\n\n\n6.3.9 Visualizing error\n\ndf &lt;- data.frame(grp = c(\"A\", \"B\"), fit = 4:5, se = 1:2)\nj &lt;- ggplot(df, aes(grp, fit, ymin = fit - se, ymax = fit + se))\nj + geom_crossbar(fatten = 2)\n\n\n\nj + geom_errorbar()\n\n\n\nj + geom_linerange()\n\n\n\nj + geom_pointrange()\n\n\n\n\n\n\n6.3.10 Maps\nPreviously, I showed how to use the map_data() function to make maps. But I found out that, according to Chapter 6 of the third edition of the ggplot2 book (available free at https://ggplot2-book.org/), the map_data() function isn’t currently maintained. That chapter recommends the use of a method of saving geographic data called simple features and packaged as the sf package. Here is a map made using that package and the USAboundaries package, which contains a wealth of geographic information about the USA.\n\nlibrary(USAboundaries)\nlibrary(sf)\nlibrary(tidyverse)\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehiclesTiny.Rdata\"))\ndfMeanPrice &lt;- dfTiny |&gt; group_by(state) |&gt;\n  dplyr::summarize(meanprice=mean(price,na.rm=TRUE))\ndfMeanPrice &lt;- dfMeanPrice |&gt; mutate(stusps=toupper(state))\nstatesContemporary &lt;- us_states(states=c(\"Alabama\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"))\ndfStateMeanPrice &lt;- merge(statesContemporary,dfMeanPrice,by=\"stusps\")\nggplot(dfStateMeanPrice) + geom_sf(data=dfStateMeanPrice,mapping=aes(fill=meanprice)) + coord_sf() + geom_sf_text(aes(label=stusps),color=\"white\")\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\nIt probably seems inconvenient to have to name all the states. The only reason I did that was that, if I didn’t, the resulting map included some US territories in the Pacific Ocean, greatly reducing the size of the continental USA in the map. I couldn’t find a way to omit them except by just manually listing all the states in the continental US."
  },
  {
    "objectID": "week06.html#additional-plots",
    "href": "week06.html#additional-plots",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.4 Additional plots",
    "text": "6.4 Additional plots\n\nlibrary(tidyverse)\ndf &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.csv\"))\ndf$cylinders &lt;- as_factor(df$cylinders)\ndf &lt;- df[df$price&lt;500000&df$price&gt;500,]\ndf&lt;-df[df$cylinders==\"10 cylinders\"|df$cylinders==\"12 cylinders\",]\ndf$price&lt;-as.numeric(df$price)\noptions(scipen=999)\ndf |&gt; ggplot(aes(price,cylinders))+geom_boxplot()\n\nWarning: Removed 156391 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\ndfa &lt;- df |&gt; count(condition,cylinders)\nlibrary(treemap)\npalette.HCL.options &lt;- list(hue_start=270, hue_end=360+150)\ntreemap(dfa,\n  index=c(\"condition\",\"cylinders\"),\n  vSize=\"n\",\n  type=\"index\",\n  palette=\"Reds\"\n)\n\n\n\n\nThe palette in this case is selected from RColorbrewer. You can find the full set of RColorbrewer palettes at the R Graph Gallery."
  },
  {
    "objectID": "week06.html#a-complete-scatterplot",
    "href": "week06.html#a-complete-scatterplot",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.5 A complete scatterplot",
    "text": "6.5 A complete scatterplot\nThe following scatterplot is completed in stages in section 2.2 of R4DS, which is available on Canvas as Wickham2023.pdf or online by googling r4ds 2e.\n\nlibrary(palmerpenguins)\nlibrary(ggthemes)\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "week06.html#foundations-for-inference",
    "href": "week06.html#foundations-for-inference",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.6 Foundations for Inference",
    "text": "6.6 Foundations for Inference\nThe following material comes from Mick McQuaid’s study guide for a previous course. In that course, we used Mendenhall and Sincich (2012) as a textbook, so there are innumerable references to that textbook in the following material. You don’t actually need that textbook and I will eventually delete references to it from this file. I just don’t have time right now."
  },
  {
    "objectID": "week06.html#normal-distribution",
    "href": "week06.html#normal-distribution",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.7 normal distribution",
    "text": "6.7 normal distribution\nThis picture illustrates the normal distribution. The mound-shaped curve represents the probability density function and the area between the curve and the horizontal line represents the value of the cumulative distribution function.\n\n\n\n\n\nConsider a normally distributed nationwide test.\nThe total shaded area between the curve and the straight horizontal line can be thought of as one hundred percent of that area. In the world of probability, we measure that area as 1. The curve is symmetrical, so measure all the area to the left of the highest point on the curve as 0.5. That is half, or fifty percent, of the total area between the curve and the horizontal line at the bottom. Instead of saying area between the curve and the horizontal line at the bottom, people usually say the area under the curve.\nFor any value along the \\(x\\)-axis, the \\(y\\)-value on the curve represents the value of the probability density function.\nThe area bounded by the vertical line between the \\(x\\)-axis and the corresponding \\(y\\)-value on the curve, though, is what we are usually interested in because that area represents probability.\nHere is a graph of the size of that area. It’s called the cumulative distribution function.\n\n\n\n\n\nThe above graph can be read as having an input and output that correspond to the previous graph of the probability density function. As we move from right to left on the \\(x\\)-axis, the area that would be to the left of a given point on the probability density function is the \\(y\\)-value on this graph. For example, if we go half way across the \\(x\\)-axis of the probability density function, the area to its left is one half of the total area, so the \\(y\\)-value on the cumulative distribution function graph is one half.\nThe shape of the cumulative distribution function is called a sigmoid curve. You can see how it gets this shape by looking again at the probability density function graph above. As you move from left to right on that graph, the area under the curve increases very slowly, then more rapidly, then slowly again. The places where the area grows more rapidly and then more slowly on the probability density function curve correspond to the s-shaped bends on the cumulative distribution curve.\nAt the left side of the cumulative distribution curve, the \\(y\\)-value is zero meaning zero probability. When we reach the right side of the cumulative distribution curve, the \\(y\\)-value is 1 or 100 percent of the probability.\nLet’s get back to the example of a nationwide test. If we say that students nationwide took an test that had a mean score of 75 and that the score was normally distributed, we’re saying that the value on the \\(x\\)-axis in the center of the curve is 75. Moreover, we’re saying that the area to the left of 75 is one half of the total area. We’re saying that the probability of a score less than 75 is 0.5 or fifty percent. We’re saying that half the students got a score below 75 and half got a score above 75.\nThat is called the frequentist interpretation of probability. In general, that interpretation says that a probability of 0.5 is properly measured by saying that, if we could repeat the event enough times, we would find the event happening half of those times.\nFurthermore, the frequentist interpretation of the normal distribution is that, if we could collect enough data, such as administering the above test to thousands of students, we would see that the graph of the frequency of their scores would look more and more like the bell curve in the picture, where \\(x\\) is a test score and \\(y\\) is the number of students receiving that score.\nSuppose we have the same test and the same distribution but that the mean score is 60. Then 60 is in the middle and half the students are on each side. That is easy to measure. But what if, in either case, we would like to know the probability associated with scores that are not at that convenient midpoint?\nIt’s hard to measure any other area under the normal curve except for \\(x\\)-values in the middle of the curve, corresponding to one half of the area. Why is this?\nTo see why it’s hard to measure the area corresponding to any value except the middle value, let’s first consider a different probability distribution, the uniform distribution. Suppose I have a machine that can generate any number between 0 and 1 at random. Further, suppose that any such number is just as likely as any other such number.\nHere’s a graph of the the uniform distribution of numbers generated by the machine. The horizontal line is the probability density function and the shaded area is the cumulative distribution function from 0 to 1/2. In other words, the probability of the machine generating numbers from 0 to 1/2 is 1/2. The probability of generating numbers from 0 to 1 is 1, the area of the entire rectangle.\n\n\n\n\n\nIt’s very easy to calculate any probability for this distribution, in contrast to the normal distribution. The reason it is easy is that you can just use the formula for the area of a rectangle, where area is base times side. The probability of being in the entire rectangle is \\(1\\times1=1\\), and the probability of being in the part from \\(x=0\\) to \\(x=1/4\\) is just \\(1\\times(1/4)=1/4\\).\nThe cumulative distribution function of the uniform distribution is simpler than that of the normal distribution because area is being added at the same rate as we move from left to right on the above graph. Therefore it is just a straight diagonal line from (0,1) on the left to (1,1) on the right.\n\n\n\n\n\nReading it is the same as reading the cumulative distribution function for the normal distribution. For any value on the \\(x\\)-axis, say, 1/2, go up to the diagonal line and over to the value on the \\(y\\)-axis. In this case, that value is 1/2. That is the area under the horizontal line in the probability density function graph from 0 to 1/2 (the shaded area). For a rectangle, calculating area is trivial.\nCalculating the area of a curved region, like the normal distribution, though, can be more difficult. If you’ve studied any calculus, you know that there are techniques for calculating the area under a curve. These techniques are called integration techniques. In the case of the normal distribution the formula for the height of the curve at any point on the \\(x\\)-axis is \\[\\begin{equation*}\n\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}\n\\end{equation*}\\] and the area is the integral of that quantity from \\(-\\infty\\) to \\(x\\), which can be rewritten as \\[\\begin{equation*}\n\\frac{1}{\\sqrt{2\\pi}}\\int^x_{-\\infty}e^{-t^2/2}dt\n=(1/2)\\left(1+\\text{erf}\\left(\\frac{x-\\mu}{\\sigma\\sqrt{2}}\\right)\\right)\n\\end{equation*}\\] The integral on the left is difficult to evaluate so people use numerical approximation techniques to find the expression on the right in the above equation. Those techniques are so time-consuming that, rather than recompute them every time they are needed, a very few people used to write the results into a table and publish it and most people working with probability would just consult the tables. Only in the past few decades have calculators become available that can do the tedious approximations. Hence, most statistics books, written by people who were educated decades ago, still teach you how to use such tables. There is some debate as to whether there is educational value in using the tables vs using calculators or smartphone apps or web-based tables or apps. We’ll demonstrate them and assume that you use a calculator or smartphone app on exams.\n\n6.7.1 Using the normal distribution\nSince there is no convenient integration formula, people used tables until recently. Currently you can google tables or apps that do the work of tables. We’re going to do two exercises with tables that help give you an idea of what’s going on. You can use your calculator afterward. The main reason for what follows is so you understand the results produced by your calculator to avoid ridiculous mistakes.\n\n\n6.7.2 calculating z scores\n\\[z=\\frac{y-\\mu}{\\sigma}\\]\nCalculations use the fact that the bell curve is symmetric and adds up to 1, so you can calculate one side and add it, subtract it, or double it\nFollowing are four examples.\n\n\\(P(-1 \\leqslant z \\leqslant 1)\\): Since the bell curve is symmetric, find the area from \\(z=0\\) to \\(z=1\\) and double that area. The table entry for \\(z=1\\) gives the relevant area under the curve, .3413. Doubling this gives the area from -1 to 1: .6826. Using a calculator may give you .6827 since a more accurate value than that provided by the table would be .6826895. This is an example where no points would be deducted from your score for using either answer.\n\\(P(-1.96 \\leqslant z \\leqslant 1.96)\\): This is one of the three most common areas of interest in this course, the other two being the one in part (c) below and the one I will add on after I show part (d) below. Here again, we can read the value from the table as .4750 and double it, giving .95. This is really common because because 95% is the most commonly used confidence interval.\n\\(P(-1.645 \\leqslant z \\leqslant 1.645)\\): The table does not have an entry for this extremely commonly desired value. A statistical calculator or software package will show that the result is .45, which can be doubled to give .90, another of the three most frequently used confidence intervals. If you use interpolation, you will get the correct answer in this case. Interpolation means to take the average of the two closest values, in this case \\((.4495+ .4505) / 2\\). You will rarely, if ever need to use interpolation in real life because software has made the tables obsolete and we only use them to try to drive home the concept of \\(z\\)-scores relating to area under the curve, rather than risking the possibility that you learn to punch numbers into an app without understanding them. Our hope is that, by first learning this method, you will be quick to recognize the results of mistakes, rather than naively reporting wacky results like the probability is 1.5 just because you typed a wrong number.\n\\(P(-3 \\leqslant z \\leqslant 3)\\): The table gives .4987 and doubling that gives .9974. A calculator would give the more correct (but equally acceptable in this course) result of .9973.\n\nThe other common confidence interval I mentioned above is the 99% confidence interval, used in cases where the calculation relates to something life-threatening, such as a question involving a potentially life-saving drug or surgery. A table or calculator will show that the \\(z\\)-score that would lead to this result is 2.576. So if you were asked to compute \\(P(-2.576 \\leqslant z \\leqslant 2.576)\\), the correct answer would be .99 or 99%. To use a calculator or statistical app to find the \\(z\\)-score given the desired probability, you would look in an app for something called a quantile function.\n\n\n6.7.3 sketch the graphs of probabilities\nSketch the normal curve six times, identifying a different region on it each time. For these graphs, let \\(y\\sim N(100,8)\\)."
  },
  {
    "objectID": "week06.html#central-limit-theorem",
    "href": "week06.html#central-limit-theorem",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.8 central limit theorem",
    "text": "6.8 central limit theorem\n\n6.8.1 Here’s the definition of the central limit theorem.\nFor large sample sizes, the sample mean \\(\\overline{y}\\) from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) has a sampling distribution that is approximately normal, regardless of the probability distribution of the sampled population.\n\n\n6.8.2 Why care about the central limit theorem?\nIn business, distributions of phenomena like waiting times and customer choices from a catalog are typically not normally distributed, but instead long-tailed. The central limit theorem means that resampling the mean of any of these distributions can be done on a large scale using the normal distribution assumptions without regard to the underlying distribution. This simplifies many real-life calculations. For instance, waiting times at each bus stop are exponentially distributed but if we take the mean waiting time at each of 100 bus stops, the mean of those 100 times is normally distributed, even though the individual waiting times are drawn from an exponential distribution.\n\n\n\n\n6.8.3 conditions for the central limit to hold\nLet \\(p\\) be the parameter of interest, in this case a proportion. (This not the same thing as a \\(p\\)-value, which we will explore later.) We don’t know \\(p\\) unless we examine every object in the population, which is usually impossible. For example, there are regulations in the USA requiring cardboard boxes used in interstate commerce to have a certain strength, which is measured by crushing the box. It would be economically unsound to crush all the boxes, so we crush a sample and obtain an estimate \\(\\hat{p}\\) of \\(p\\). The estimate is pronounced p-hat.\nThe central limit theorem can be framed in terms of its mean and standard error:\n\\[\n\\mu_{\\hat{p}}=p \\qquad \\text{SE}_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}}\n\\]\nThe conditions under which this holds true are\n\nThe sample observations are independent of each other\nThe sample is sufficiently large that the success-failure condition holds, namely \\(np\\geqslant10\\) and \\(n(1-p)\\geqslant10\\).\n\nThe sample observations are independent if they are drawn from a random sample. Sometimes you have to use your best judgment to determine whether the sample is truly random. For example, friends are known to influence each other’s opinions about pop music, so a sample of their opinions of a given pop star may not be random. On the other hand, their opinions of a previously unheard song from a previously unheard artist may have a better chance of being random.\nThese conditions matter when conducting activities like constructing a confidence interval or modeling a point estimate.\n\n\n6.8.4 normal distribution example problem\nConsider an example for calculating a \\(z\\)-score where \\(x\\sim N(50,15)\\), which is a statistical notation for saying \\(x\\) is a random normal variable with mean 50 and standard deviation 15. It is also read as if you said that \\(x\\) has the normal distribution with mean 50 and standard deviation 15.\n\n6.8.4.1 Picture the example.\n\n\n\n\n\n\n\n6.8.4.2 Here’s what we input into the \\(z\\) calculation.\nIn order to identify the size of the shaded area, we can use the table of \\(z\\)-scores by standardizing the parameters we believe to apply, as if they were the population parameters \\(\\mu\\) and \\(\\sigma\\). We only do this if we have such a large sample that we have reason to believe that the sample values approach the population parameters. For the much, much more typical case where we have a limited amount of data, we’ll learn a more advanced technique that you will use much more frequently in practice.\n\n\n6.8.4.3 We input a \\(z\\) to get an output probability.\nThe table in our textbook contains an input in the left and top margins and an output in the body. The input is a \\(z\\)-score, the result of the calculation \\[z=\\frac{y-\\mu}{\\sigma}\\] where \\(z\\geq 0\\). The output is a number in the body of the table, expressing the probability for the area between the normal curve and the axis, from the mean (0) to \\(z\\). Note that the values of \\(z\\) start at the mean and grow toward the right end of the graph. If \\(z\\) were \\(\\infty\\), the shaded area would be 0.5, also known as 50 percent.\n\n\n6.8.4.4 This is the \\(z\\)-score table concept.\n\n\n\n\n\n\n\n6.8.4.5 Calculate the \\(z\\)-score to input.\nFor now, let’s calculate the \\(z\\)-score as \\[z=\\frac{y-\\mu}{\\sigma}=\\frac{70-50}{15}=1.33\\] giving half of the answer we’re seeking:"
  },
  {
    "objectID": "week06.html#apply-the-z-score-to-the-table.",
    "href": "week06.html#apply-the-z-score-to-the-table.",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.9 Apply the \\(z\\)-score to the table.",
    "text": "6.9 Apply the \\(z\\)-score to the table.\n\n\n\n\n\n\n6.9.0.1 Obtain an intermediate result.\nNow use this to read the table. The input is 1.33 and you’ll use the left and top margins to find it. The output is the corresponding entry in the body of the table, .4082, also known as 40.82 percent of the area under the curve.\n\n\n6.9.0.2 Finish the example.\nRecall that our initial problem was to find \\(P(30&lt;y&lt;70)\\) and what we’ve just found, .4082, is \\(P(50&lt;y&lt;70)\\). We must multiply this result by 2 to obtain the correct answer, .8164 or 81.64 percent. That is to say that the probability that \\(y\\) is somewhere between 30 and 70 is .8164 or 81.64 percent. As a reality check, all probabilities for any single event must sum to 1, and the total area under the curve is 1, so it is a relief to note that the answer we’ve found is less than 1. It’s also comforting to note that the shaded area in the original picture of the example looks like it could plausibly represent about 80 percent of the total area. It is easy to get lost in the mechanics of calculations and come up with a wildly incorrect answer because of a simple arithmetic error.\n\n\n6.9.1 Some \\(z\\)-score tables differ from ours.\nBear in mind that anyone can publish a \\(z\\)-score table using their own customs. Students have found such tables that define \\(z\\) as starting at the extreme left of the curve. If we used such a table for the above example, the output would have been .9082 instead of .4082 and we would have had to subtract the left side, .5, from that result before multiplying by 2.\n\n\n6.9.2 typical exam questions for \\(z\\)-scores\nMany exam-style problems will ask questions such that you must do more or less arithmetic with the result from the table. Consider these questions, still using the above example where \\(y\\sim N(50,15)\\):\n\nWhat is the probability that \\(y\\) is greater than 50?\nWhat is the probability that \\(y\\) is greater than 70?\nWhat is the probability that \\(y\\) is less than 30?\nWhat is the probability that \\(y\\) is between 30 and 50?\n\n\n6.9.2.1 Answer the preceding questions.\nEach of these questions can be answered using \\(z=1.33\\) except the first. Since we know that \\(y\\) is normally distributed, we also know that the probability of \\(y\\) being greater than its mean is one half, so the answer to the first question is 0.5 or fifty percent. The second question simply requires us to subtract the result from the table, .4082, from .5 to find the area to the right of 1.33, which is .0918 or 9.18 percent. The third question is symmetrical with the second, so we can just use the method from the second question to find that it is also .0918. Similarly, the fourth question is symmetrical with the first step from the book example, so the answer is the answer to that first step, .4082.\n\n\n6.9.2.2 This one takes an extra step.\nWhat is the probability that \\(y\\) is between 30 and 40?\n\n\n\n\n\n\n\n6.9.2.3 Find the difference between areas.\nSubtract the probability that \\(y\\) is between 50 and 60 from the probability that \\(y\\) is between 50 and 70.\n\n\n\n\n\n\n\n\n6.9.3 How do you find areas in \\(z\\)-score tables?\n\ndraw picture to help you understand the question\nstandardize the picture so you can use a table or a\ndraw the standardized picture\npick one of three kinds of tables / apps\nwrite the standardized result (may do this multiple times)\nfit the standardized result(s) into the original problem\n\nLet’s look at these steps with an example. Suppose that \\(y\\sim N(50,8)\\). In words, this means that \\(y\\) has the normal distribution with true mean 50 and true standard deviation 8. Let’s answer the question What’s the probability that \\(y&gt;40\\)?\nStep 1 is to draw a picture to make sense of the question. The picture shows the area under the curve where the scale marking is to the right of 40. This picture tells you right away that the number that will answer the question is less than 1 (the entire curve would be shaded if it were 1) and more than 1/2 (the portion to the right of 50 would be 1/2 and we have certainly shaded more than that).\nStep 2 is to standardize the question so we can use a table or app to find the probability / area. We use the equation \\(z=(y-\\mu)/\\sigma\\) with values from the original question: \\((50-40)/8=-10/8=-1.25\\). Now we know the labels that would go on a standardized picture similar to the picture above. Now we can ask the standardized question What’s the probability that \\(z&gt;-1.25\\)?\nStep 3 is to draw that standardized picture. It’s the same as the picture above except standardized so that it fits the tables / apps for calculating probability / area. Now, instead of \\(y\\) we’re looking for \\(z\\) and the probability associated with \\(z\\) on a standardized table will be the same as for \\(y\\) on a table for the parameters given in the original question.\nStep 4 is to pick one of the three kinds of tables / apps to input the standardized \\(z\\) score to get a probability as output. In this example, we only have to do this step once because we only want to know the area greater than \\(y\\). If we wanted to know a range between two \\(y\\) values, we’d need the \\(z\\) scores for each of them so we’d have to do it twice.\nThe three kinds of output from tables / apps are as follows.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first figure shows how the table works in some books. It provides the value from 0 to \\(|z|\\). If \\(z\\) is negative, using this table requires you to input \\(|z|\\) instead. In this question, the value you get will be .3944. To get the ultimate answer to this question, Step 5 will be to add this value to .5, giving a final answer of .8944.\nThe second figure shows how most tables and apps work. It gives the value from \\(-\\infty\\) to \\(z\\). In this question, the value you get will be .1056. To get the ultimate answer to this question, Step 5 will be to subtract this value from 1, giving a final answer of .8944.\nThe bottom figure shows how some tables and apps work. It gives the value from z to \\(+\\infty\\). In this question, the value you get will be .8944. To get the ultimate answer to this question, Step 5 will be to simply report this value, giving a final answer of .8944.\nNotice that all three types of tables / apps lead to the same result by different paths. In this case, the right figure is the most convenient but, for other questions, one of the others may be more convenient.\nStep 5, the final step is to use the value you got from a table or app in conjunction with the original picture you drew in Step 1. Since the procedure for step 5 depends on the table / app you use, I gave the procedure for Step 5 above in the paragraphs for top, middle, and bottom figure."
  },
  {
    "objectID": "week06.html#estimate-a-population-parameter",
    "href": "week06.html#estimate-a-population-parameter",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.10 estimate a population parameter",
    "text": "6.10 estimate a population parameter\n\n6.10.1 Use a large-sample confidence interval.\nA large-sample \\(100(1-\\alpha)\\%\\) confidence interval for a population mean, \\(\\mu\\), is given by\n\\[\\mu \\pm z_{\\alpha/2} \\sigma_y \\approx \\overline{y}\\pm z_{\\alpha/2} \\frac{s}{\\sqrt{n}}\\]"
  },
  {
    "objectID": "week06.html#elements-of-a-hypothesis-test",
    "href": "week06.html#elements-of-a-hypothesis-test",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.11 elements of a hypothesis test",
    "text": "6.11 elements of a hypothesis test\n\nNull hypothesis\nAlternative hypothesis\nTest statistic\nLevel of significance\nRejection region marker\n\\(p\\)-value\nConclusion\n\n\n6.11.1 Work out a standard deviation example.\nExample exam question: ten samples of gas mileage have been calculated for a new model of car by driving ten examples of the car each for a week. The average of the ten samples is 17. The sum of the squares of the sample is 3281. What is their standard deviation?\n\n6.11.1.1 Recognize appropriate formulas.\nThe information in the problem statement hints that you should use \\[s=\\sqrt{\\frac{\\sum_{i=1}^n y_i^2 -\nn(\\overline{y})^2}{n-1}}\\] so you write \\[s=\\sqrt{\\frac{3281 - 10(17)^2}{10-1}}=\\sqrt{\\frac{3281-2890}{9}}=\\sqrt{43.4444}=6.5912\\]\n\n\n6.11.1.2 Why was the subtraction result positive?\nThe sample of ten gas mileage estimates is 17, 18, 16, 20, 14, 17, 21, 13, 22, 12, 17. The sum of their squares is inevitably larger than or equal to the mean squared times the number of values. The easiest way to see this is to use a series of identical values. Hence, finding the sum of the squares is the same as calculating the mean squared times the number of values. There is no variance at all in such a sample, so it makes sense to arrive at a standard deviation of zero. Is there any way to alter such a sample so that the sum of the squared values falls below the mean? No.\n\n\n6.11.1.3 What should you do when you don’t understand?\nThe previous example was developed as an answer to the question, what do I do if I need to do a negative square root? You can figure out that you will never need to do so by the preceding process of finding a way to make \\(\\sum y_i^2 = n(\\overline{y}^2)\\) and then trying to alter the values to decrease the left side or increase the right side."
  },
  {
    "objectID": "week06.html#type-i-and-type-ii-error",
    "href": "week06.html#type-i-and-type-ii-error",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.12 Type I and Type II error",
    "text": "6.12 Type I and Type II error\nTable 4.6 from James et al. (2021) shows the possible results of classification.\n\n\n\n\n\n\n\n\n\n\n\nTrue class\n\n\n\\(-\\) or Null\n\\(+\\) or Non-null\nTotal\n\n\n\n\nPredicted class\n\\(-\\) or Null\nTrue Negative (TN)\nFalse Negative (FN)\nN\\(*\\)\n\n\n\\(+\\) or Non-null\nFalse Positive (FP)\nTrue Positive (TP)\nP\\(*\\)\n\n\nTotal\nN\nP\n\n\n\n\nTable 4.7 from James et al. (2021) gives some synonyms for important measures of correctness and error in various disciplines.\n\n\n\nname\ndefinition\nsynonyms\n\n\n\n\nFalse Positive rate\nFP/N\nType I error, 1 \\(-\\) Specificity\n\n\nTrue Positive rate\nTP/P\n1 \\(-\\) Type II error, power, sensitivity, recall\n\n\nPositive Predictive value\nTP/P\\(*\\)\nPrecision, 1 \\(-\\) false discovery proportion\n\n\nNegative Predictive value\nTN/N\\(*\\)"
  },
  {
    "objectID": "week06.html#make-an-inference-about-a-population-parameter.",
    "href": "week06.html#make-an-inference-about-a-population-parameter.",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.13 Make an inference about a population parameter.",
    "text": "6.13 Make an inference about a population parameter.\nAt the beginning of the course, I said that statistics describes data and makes inferences about data. This course is partly about the latter, making inferences. You can make two kinds of inferences about a population parameter: estimate it or test a hypothesis about it.\n\n6.13.1 First distinguish between small and large.\nYou may have a small sample or a large sample. The difference in the textbook is typically given as a cutoff of 30. Less is small, more is large. Other cutoffs are given, but this is the most prevalent.\nLarge samples with a normal distribution can be used to estimate a population mean using a \\(z\\)-score. Small samples can be used to estimate a population mean using a \\(t\\)-statistic.\n\n\n6.13.2 Sampling leads to a new statistic.\nIf we take more and more samples from a given population, the variability of the samples will decrease. This relationship gives rise to the standard error of an estimate \\[\\sigma_{\\overline{y}}=\\frac{\\sigma}{\\sqrt{n}}\\]\nThe standard error of the estimate is not exactly the standard deviation. It is the standard deviation divided by a function of the sample size and it shrinks as the sample size grows."
  },
  {
    "objectID": "week06.html#estimate-a-population-mean",
    "href": "week06.html#estimate-a-population-mean",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.14 estimate a population mean",
    "text": "6.14 estimate a population mean\n\n6.14.1 Find a confidence interval.\nIf the sample is larger than or equal to 30, use the formula for finding a large-sample confidence interval to estimate the mean.\nA large-sample \\(100(1-\\alpha)\\%\\) confidence interval for a population mean, \\(\\mu\\), is given by\n\\[\\mu \\pm z_{\\alpha/2} \\sigma_{\\overline{y}} \\approx \\overline{y}\\pm z_{\\alpha/2} \\frac{s}{\\sqrt{n}}\\]\n\n\n6.14.2 For a small sample, use \\(t\\).\nIf the sample is smaller than 30, calculate a \\(t\\)-statistic and use the formula for finding a small-sample confidence interval to estimate the mean.\nThe \\(t\\)-statistic you calculate from the small sample is \\[\nt=\\frac{\\overline{y}-\\mu_0}{s/\\sqrt{n}}\n\\]\nDoes your calculated \\(t\\) fit within a \\(100(1-\\alpha)\\%\\) confidence interval? Find out by calculating that interval. A small-sample \\(100(1-\\alpha)\\%\\) confidence interval for a population mean, \\(\\mu\\), is given by\n\\[\\mu \\pm t_{\\alpha/2} s_{\\overline{y}} \\approx\n\\overline{y}\\pm t_{\\alpha/2,\\nu} \\frac{s}{\\sqrt{n}}\\] Here, you are using a \\(t\\) statistic based on a chosen \\(\\alpha\\) to compare to your calculated \\(t\\)-statistic. The Greek letter \\(\\nu\\), pronounced nyoo, represents the number of degrees of freedom.\n\n\n6.14.3 Find a probability to estimate a mean.\nEstimating a mean involves finding a region where the mean is likely to be. The first question to ask is how likely? and to do so, we use a concept called alpha, denoted by the Greek letter \\(\\alpha\\). Traditionally, people have used three values of \\(\\alpha\\) for the past century, .10, .05, and .01. These correspond to regions of \\(1-\\alpha\\) under the normal distribution curve, so these regions are 90 percent of the area, 95 percent of the area, and 99 percent of the area. What we are saying, for instance, if \\(\\alpha=0.01\\) is that we are 99 percent confident that the true population mean lies within the region we’ve calculated, \\(\\overline{y}\\pm 2.576\\sigma_{\\overline{y}}\\)\n\n\n6.14.4 \\(\\alpha\\) selection is driven by criticality.\nTraditionally, \\(\\alpha=0.01\\) is used in cases where life could be threatened by failure.\nTraditionally, \\(\\alpha=0.05\\) is used in cases where money is riding on the outcome.\nTraditionally, \\(\\alpha=0.10\\) is used in cases where the consequences of failing to capture the true mean are not severe.\n\nThe above picture shows these three cases. The top version, life-or-death, has the smallest rejection region. Suppose the test is whether a radical cancer treatment gives longer life than a traditional cancer treatment. Let’s say that the traditional treatment gives an average of 15 months longer life. The null hypothesis is that the new treatment also gives 15 months longer life. The alternative hypothesis is that the radical treatment gives 22 months more life on average based on on only five patients who received the new treatment. A patient can only do one treatment or the other. The status quo would be to take the traditional treatment unless there is strong evidence that the radical treatment provides an average longer life. In the following picture, the shaded area is where the test statistic would have to fall for us to say that there is strong evidence that the radical treatment provides longer life. We want to make that shaded region as small as possible so we minimize the chance our test statistic lands in it by mistake.\nWe can afford to let that shaded area be bigger (increasing the chance of mistakenly landing in it) if only money, not life, is riding on the outcome. And we can afford to let it be bigger still if the consequences of the mistake are small. To choose an \\(\\alpha\\) level, ask yourself how severe are the consequences of a Type I error.\n\n\n6.14.5 practice estimating a population mean\nConsider an outlet store that purchased used display panels to refurbish and resell. The relevant statistics for failure time of the sample are \\(n=50, \\overline{y}=1.9350, s=0.92865\\).\nBut these are the only necessary numbers to solve the problem. The standard error, .1313 can be calculated from \\(s/\\sqrt{n} = 0.92865/\\sqrt{50}=.1313.\\)\nFirst, find the 95% confidence interval, which can be calculated from the above information. Since the sample is greater than or equal to 30, we use the large sample formula: \\[\\begin{align*}\n\\overline{y}\\pm z_{\\alpha/2} \\sigma_{\\overline{y}} \\approx \\overline{y}\\pm z_{\\alpha/2} \\frac{s}{\\sqrt{n}}\n&= 1.9350 \\pm 1.96 (.1313) \\\\\n&= 1.9350 \\pm 0.2573 \\\\\n&= (1.6777,2.1932)\n\\end{align*}\\] The correct answer to a question asking for the confidence interval is simply that pair of numbers, 1.6777 and 2.1932.\nNow interpret that. This result says that we are 95% confident that the true average time to failure for these panels is somewhere between 1.6777 years and 2.1932 years. It is tempting to rephrase the result. Be careful that you don’t say something with a different meaning. Suppose the store wants to offer a warranty on these panels. Knowing that we are 95% confident that the true mean is in the given range helps the store evaluate the risk of different warranty lengths. The correct answer is to say that we are 95% confident that the true mean time to failure for these panels is somewhere between 1.6777 years and 2.1932 years.\nThe meaning of the 95 percent confidence interval is that, if we repeatedly resample the population, computing a 95% confidence interval for each sample, we expect 95% of the confidence intervals generated to capture the true mean.\nAny statistics software can also offer a graphical interpretation, such as a stem-and-leaf plot or histogram. The stem-and-leaf plot uses the metaphor of a stem bearing some number of leaves. In the following stem-and-leaf plot, the stem represents the first digit of a two-digit number. The top row of the plot has the stem 0 and two leaves, 0 and 2. Each leaf represents a data point as the second digit of a two-digit number. If you count the leaves (the digits to the right of the vertical bar), you will see that there are fifty of them, one for each recorded failure time. You can think of each stem as holding all the leaves in a certain range. The top stem holds all the leaves in the range .00 to .04 and there are two of them. The next stem holds the six leaves in the range .05 to .09. The third stem holds all six leaves in the range .10 to .15. The stem-and-leaf plot resembles a sideways bar chart and helps us see that the distribution of the failure times is somewhat mound-shaped. The main advantages of the stem-and-leaf plot are that it is compact for the amount of information it conveys and that it does not require a graphics program or even a computer to quickly construct it from the raw data. The programming website http://rosettacode.org uses the stem-and-leaf plot as a programming task, demonstrating how to create one in 37 different programming languages.\n0 | 02\n0 | 567788\n1 | 122222\n1 | 55666667888999\n2 | 0223344\n2 | 666788\n3 | 00233\n3 | 5555\nMost statistics programs offer many different histogram types. The simplest is equivalent to a barchart as follows."
  },
  {
    "objectID": "week06.html#test-a-hypothesis-about-a-population-mean",
    "href": "week06.html#test-a-hypothesis-about-a-population-mean",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.15 test a hypothesis about a population mean",
    "text": "6.15 test a hypothesis about a population mean\n\n6.15.1 Choose a null and alternative hypothesis.\nTesting a hypothesis must be done modestly. As an example of what I mean by modestly, consider criminal trials in the USA. The suspect on trial is considered innocent until proven guilty. The more modest hypothesis (the null hypothesis) is that the person is innocent. The more immodest hypothesis (the alternative hypothesis) carries the burden of proof.\n\n\n6.15.2 Type I error is worse than Type II error.\nThe traditional view of the legal system in the USA is that if you imprison an innocent person, it constitutes a more serious error than letting a guilty person go free.\nImprisoning an innocent person is like a Type I error: we rejected the null hypothesis when it was true. Letting a guilty person go free is like a Type II error: we failed to reject the null hypothesis when it was false.\n\n\n6.15.3 identify rejection region marker\nHow do you identify rejection region markers? A rejection region marker is the value a test statistic that leads to rejection of the null hypothesis. It marks the edge of a region under the curve corresponding to the level of significance, \\(\\alpha\\). The marker is not a measure of the size of the region. The rejection region marker can vary from \\(-\\infty\\) to \\(+\\infty\\) while the size of the region is somewhere between zero and one.\nThe following table shows the relevant values of \\(\\alpha\\) and related quantities.\n\n\n\n\n\n\n\n\n\n\\(1-\\alpha\\)\n\\(\\alpha\\)\n\\(\\alpha/2\\)\n\\(z_{\\alpha/2}\\)\n\n\n\n\n.90\n.10\n.05\n1.645\n\n\n.95\n.05\n.025\n1.96\n\n\n.99\n.01\n.005\n2.576\n\n\n\nThe above table refers to two-tailed tests. Only examples (e) and (f) below refer to two-tailed tests. In the other four cases, the \\(z\\)-scores refer to \\(z_{\\alpha}\\) rather than \\(z_{\\alpha/2}\\).\n(a) \\(\\alpha=0.025\\), a one-tailed rejection region\n\n\n\n\n\n(b) \\(\\alpha=0.05\\), a one-tailed rejection region\n\n\n\n\n\n(c) \\(\\alpha=0.005\\), a one-tailed rejection region\n\n\n\n\n\n(d) \\(\\alpha=0.0985\\), a one-tailed rejection region\n\n\n\n\n\n(e) \\(\\alpha=0.10\\), a two-tailed rejection region\n\n\n\n\n\n(f) \\(\\alpha=0.01\\), a two-tailed rejection region"
  },
  {
    "objectID": "week06.html#test-a-hypothesis-about-a-sample-mean",
    "href": "week06.html#test-a-hypothesis-about-a-sample-mean",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.16 test a hypothesis about a sample mean",
    "text": "6.16 test a hypothesis about a sample mean\nThe test statistic is \\(t_c\\), where the \\(c\\) subscript stands for calculated. Most people just call it \\(t\\). In my mind, that leads to occasional confusion about whether it is a value calculated from a sample. We’ll compare the test statistic to \\(t_{\\alpha,\\nu}\\), the value of the statistic at a given level of significance, identified using a table or calculator.\nThe test leads to two situations. The first, pictured below, is the situation where we fail to reject the null hypothesis and conclude that we have not seen evidence in the sample that the point estimate differs from what the modest hypothesis claims it is. Often, this is an estimate of a mean, where the population mean, \\(\\mu_0\\), is being estimated by a sample mean, \\(\\bar{y}\\). The following picture doesn’t show \\(\\bar{y}\\), just the tail. \\(\\bar{y}\\) would be at the top of the hump, not shown here.\n\n\n\n\n\nThe above possibility shows the situation where \\(t_{\\alpha,\\nu}&gt;t_c\\), which is equivalent to saying that \\(p&gt;\\alpha\\).\nThe \\(x\\)-axis scale begins to the left of the fragment shown here, and values of \\(t\\) increase from left to right. At the same time, the shaded regions decrease in size from left to right. Note that the entire shaded region above is \\(p\\), while only the darker region at the right is \\(\\alpha\\).\nThe situation pictured below is the reverse. In this situation, we reject the null hypothesis and conclude instead that the point estimate does not equal the value in the modest hypothesis. In this case \\(t_{\\alpha,\\nu}&lt;t_c\\), which is equivalent to saying that \\(p&lt;\\alpha\\).\n\n\n\n\n\nLet’s make this more concrete with an example.\nSuppose that a bulk vending machine dispenses bags expected to contain 15 candies on average. The attendant who refills the machine claims it’s out of whack, dispensing more than 15 candies on average, requiring more frequent replenishment and costing the company some extra money. The company representative asks him for a sample from the machine, so he produces five bags containing 25, 23, 21, 21, and 20 candies. Develop a hypothesis test to consider the possibility that the vending machine is defective. Use a level of significance that makes sense given the situation described above.\nThere are seven steps to solving this problem, as follows.\nStep 1. Choose a null hypothesis.\nAt a high level, we can say that we are trying to choose between two alternatives:\n\nthat the machine is defective, or\nthat the machine is operating normally.\n\nWe need to reduce this high level view to numbers. The problem states that the machine is expected to dispense 15 candies per bag, on average. This is equivalent to saying that the true mean is 15 or \\(\\mu=15\\).\nIf the machine is defective in the way the attendant claims, then \\(\\mu&gt;15\\). So we could say that one of the hypotheses would be that the sample came from a population with \\(\\mu=15\\) and the other hypothesis would be that the sample did not come from a population with \\(\\mu=15\\). Which should be the null hypothesis?\nThe null hypothesis represents the status quo, what we would believe if we had no evidence either for or against. Do you believe that the machine is defective if there is no evidence either that it is or isn’t? Let’s put it another way. Suppose you arrested a person for a crime and then realized that you have no evidence that they did commit the crime and no evidence that they did not commit the crime. Would you imprison them or let them go free? If you let them go free, it means that your null hypothesis is that they are innocent unless proven guilty.\nThis suggests that if you have no evidence one way or the other, assume the machine is operating normally. We can translate this into the null hypothesis \\(\\mu=15\\). The formal way of writing the null hypothesis is to say \\(H_0: \\mu=\\mu_0\\), where \\(\\mu_0=15\\). Later, when refer to this population mean, we call it \\(\\mu_0\\) because it is the population mean associated with the hypothesis \\(H_0\\). So later we will say \\(\\mu_0=15\\).\nAt the end of step 1, we have chosen the null hypothesis: \\(H_0: \\mu=\\mu_0\\) with \\(\\mu_0=15\\).\nStep 2. Choose the alternative hypothesis. The appropriate alternative hypothesis can be selected from among three choices: \\(\\mu&lt;\\mu_0\\), \\(\\mu&gt;\\mu_0\\), or \\(\\mu \\ne \\mu_0\\). The appropriate choice here seems obvious: all the sample values are much larger than \\(\\mu_0\\), so if the mean we calculate differs from \\(\\mu_0\\) it will have to be larger than \\(\\mu_0\\). If all the values in our sample are larger than \\(\\mu_0\\), there is just no way their average can be smaller than \\(\\mu_0\\).\nAt the end of step 2, we have determined the alternative hypothesis to be\n\\(H_a: \\mu&gt;\\mu_0\\) with \\(\\mu_0=15\\).\nStep 3. Choose the test statistic. Previously, we have learned two test statistics, \\(z\\) and \\(t\\). We have learned that the choice between them is predicated on sample size. If \\(n\\geqslant30\\), use \\(z\\), otherwise use \\(t\\). Here \\(n=5\\) so use \\(t\\). We can calculate the \\(t\\)-statistic, which I called \\(t_c\\) for calculated above, for the sample using the formula \\[\nt=\\frac{\\overline{y}-\\mu_0}{s/\\sqrt{n}}\n\\]\nWe can calculate the values to use from the following formulas or by using a machine.\n\\[\\overline{y}=\\sum{y_i}/n=22\\]\n\\[s=\\sqrt{\\frac{\\sum y_i^2 -\nn(\\overline{y})^2}{n-1}}=2\\]\n\\(\\mu_0\\) was given to us in the problem statement and \\(\\sqrt{n}\\) can be determined with the use of a calculator or spreadsheet program. The calculated \\(t\\)-statistic is \\(t_c=(22-15)/(2/\\sqrt{5})=7.8262\\).\nAt the end of step 3, you have determined and calculated the test statistic, \\(t_c=7.8262\\).\nStep 4. Determine the level of significance, \\(\\alpha\\). You choose the appropriate value of \\(\\alpha\\) from the circumstances given in the problem statement. Previously in class, I claimed that there are three common levels of significance in use as sumarized in the table on page 35: 0.01, 0.05, and 0.10. I gave rules of thumb for these three as 0.01 life-or-death, 0.05 money is riding on it, and 0.10 casual / low budget. In this case, the consequences seem to be a small amount of money lost by the company if they are basically giving away candies for free. I suggest that this is a case where some money is riding on it so choose \\(\\alpha=0.05\\).\nAt the end of step 4, you have determined \\(\\alpha=0.05\\).\nStep 5. Identify the rejection region marker. This is simply a matter of calculating (or reading from a table) an appropriate \\(t\\)-statistic for the \\(\\alpha\\) you chose in the previous step. This is \\(t_{\\alpha,\\nu}=t_{0.05,4}=2.131847\\). Note that \\(\\nu\\) is the symbol the book uses for df, or degrees of freedom. It is a Greek letter pronounced nyoo. For a single sample \\(t\\)-statistic, df\\(=\\nu=n-1\\).\nThis can be found using R by saying\n\nqt(0.95,4)\n\n[1] 2.131847\n\n\nAt the end of step 5, you have calculated the location of the rejection region (but not its size). It is located everywhere between the \\(t\\) curve and the horizontal line to the right of the point \\(t=2.131847\\).\nStep 6. Calculate the \\(p\\)-value. This is the size of the region whose location was specified in the previous step, written as \\(p=P(t_{\\alpha,\\nu}&gt;t_c)\\). It is the probability of observing a \\(t\\)-statistic greater than the calculated \\(t\\)-statistic if the null hypothesis is true. It is found by a calculator or app or software. It can only be calculated by hand if you know quite a bit more math than is required for this course. In this case \\(p=P(t_{\\alpha,\\nu}&gt;t_c)=0.0007195\\).\nWe can identify both \\(t_\\alpha\\) and \\(t_c\\) using R as follows:\n\n#. t sub alpha\nqt(0.95,4)\n\n[1] 2.131847\n\n#. find the alpha region associated with t sub alpha\n1-pt(2.131847,4)\n\n[1] 0.04999999\n\nx &lt;- c(25,23,21,21,20)\nt.test(x,alternative=\"greater\",mu=15)\n\n\n    One Sample t-test\n\ndata:  x\nt = 7.8262, df = 4, p-value = 0.0007195\nalternative hypothesis: true mean is greater than 15\n95 percent confidence interval:\n 20.09322      Inf\nsample estimates:\nmean of x \n       22 \n\n\nAlso note that the \\(t\\) test tells us that the true mean is far from 15. If we tested repeatedly, we would find that it is greater than 20.09322 in 95 percent of the tests.\nAt the end of step 6, we have calculated the \\(p\\)-value, \\(p=P(t_{\\alpha,\\nu} &gt; t_c)=0.0007195\\).\nStep 7. Form a conclusion from the hypothesis test. We reject the null hypothesis that \\(\\mu=\\mu_0\\), or in other words, we reject the hypothesis that these five bags came from a vending machine that dispenses an average of 15 candies per bag. Notice we don’t that the machine is defective. Maybe the data were miscounted. We don’t know. We have to limit our conclusion to what we know about the data presented to us, which is that the data presented to us did not come from a machine that dispense an average of 15 candies per bag.\nTo summarize the answer, the seven elements of this statistical test of hypothesis are:\n\nnull hypothesis \\(H_0: \\mu=15\\)\nalternative hypothesis \\(H_{a}: \\mu&gt;15\\)\ntest statistic \\(t_c=7.8262\\)\nlevel of significance \\(\\alpha=0.05\\)\nrejection region marker \\(t_{0.05,4} = 2.131847\\)\n\\(p\\)-value \\(P(t_{\\alpha,\\nu}&gt;t_c)=0.0007195\\)\nconclusion Reject the null hypothesis that these five bags came from a machine that dispenses an average of 15 candies per bag.\n\nLet’s return to the two pictures we started with. Notice that \\(p&lt;\\alpha\\) in this case, which is equivalent to saying that \\(t_c&gt;t_{\\alpha,\\nu}\\), so we are looking at the second of the two pictures.\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning, 2nd Edition. Springer New York.\n\n\nMendenhall, William, and Terry Sincich. 2012. A Second Course in Statistics, Regression Analysis, Seventh Edition. Boston, MA, USA: Prentice Hall."
  },
  {
    "objectID": "week07.html#recap-week-06",
    "href": "week07.html#recap-week-06",
    "title": "7  More about Inference",
    "section": "7.1 Recap Week 06",
    "text": "7.1 Recap Week 06\n\nggplot2\nFoundations for Inference\n\nPoint estimates\nConfidence Intervals\nHypothesis Tests\n\n\n\n7.1.1 A useful ggplot trick\nSome of you have struggled to produce boxplots of variables like price and odometer because they have ridiculous outliers. One way to overcome this that we’ve explored is to remove the extreme observations. A possibly better way, though, is illustrated below. Saying outlier.shape=NA in the geom_boxplot() function removes outliers before the dimensions of the plot are calculated. The result is a much more readable boxplot containing most of the cars.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.Rdata\"))\nlibrary(tidyverse)\ndf |&gt; ggplot(aes(condition,odometer))+geom_boxplot(outlier.shape=NA)+  scale_y_continuous(limits = quantile(df$odometer, c(0.1, 0.8),na.rm=TRUE))\n\nWarning: Removed 131132 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\ndf |&gt; ggplot(aes(condition,price))+geom_boxplot(outlier.shape=NA)+  scale_y_continuous(limits = quantile(df$price, c(0.1, 0.8),na.rm=TRUE))\n\nWarning: Removed 127440 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nNote that, in the above example, I’ve stored df in vehicles.Rdata using the save() function. Before saving it I converted condition to a factor and both price and odometer to numeric.\n\n\n7.1.2 Preparing data\nNote that you can use the above quantile trick in other ways. For example, here I’m filtering out the extreme values of price, without necessarily knowing what they are.\n\ndf |&gt;\n  filter(price &gt;= quantile(price,.10) & price &lt;= quantile(price,.90)) |&gt;\n  count(state) |&gt;\n  arrange(-n) |&gt;\n  as_tibble() |&gt;\n  print(n=12)\n\n# A tibble: 51 × 2\n   state     n\n   &lt;chr&gt; &lt;int&gt;\n 1 ca    39707\n 2 fl    23260\n 3 tx    18285\n 4 ny    15956\n 5 oh    15086\n 6 mi    14959\n 7 pa    11787\n 8 nc    11105\n 9 or    10875\n10 wi    10194\n11 tn     9466\n12 co     9335\n# ℹ 39 more rows\n\n\nOne group tabled price by paint color. This is not a good idea! There are too many individual prices. Instead, you can try using ranges with one of the cut_ functions, cut_interval (shown here), or cut_width. Then you can make tables or save the ranges to use in barcharts or other tables.\n\ntable(cut_interval(df$price[df$price&lt;100000],20,dig.lab=8))\n\n\n       [0,4999.95]   (4999.95,9999.9]  (9999.9,14999.85] (14999.85,19999.8] \n             95312              79652              53811              49107 \n(19999.8,24999.75] (24999.75,29999.7] (29999.7,34999.65] (34999.65,39999.6] \n             34812              34946              26552              23510 \n(39999.6,44999.55] (44999.55,49999.5] (49999.5,54999.45] (54999.45,59999.4] \n              9494               6643               3773               2948 \n(59999.4,64999.35] (64999.35,69999.3] (69999.3,74999.25] (74999.25,79999.2] \n              1535               1617                821                702 \n(79999.2,84999.15] (84999.15,89999.1] (89999.1,94999.05]   (94999.05,99999] \n               391                304                114                139 \n\ndfReduced &lt;- df[df$price&lt;100000,]\ndfReduced$priceRanges &lt;- cut_interval(dfReduced$price,20,dig.lab=8)\nggplot(dfReduced,aes(priceRanges)) + geom_bar(fill=\"lightblue\") + theme(axis.text.x = element_text(angle=45,vjust=1,hjust=1))\n\n\n\ntable(dfReduced$priceRanges[dfReduced$paint_color %in% c(\"red\",\"white\",\"black\",\"blue\")],dfReduced$paint_color[dfReduced$paint_color %in% c(\"red\",\"white\",\"black\",\"blue\")])\n\n                    \n                     black  blue   red white\n  [0,4999.95]         9977  6692  5544 13156\n  (4999.95,9999.9]    9764  7002  5592 10805\n  (9999.9,14999.85]   8345  3901  3952  9476\n  (14999.85,19999.8]  7567  3898  3746 10211\n  (19999.8,24999.75]  5906  2480  2455  7760\n  (24999.75,29999.7]  5783  2540  3195  8132\n  (29999.7,34999.65]  5106  1517  2081  6742\n  (34999.65,39999.6]  4934  1751  2111  5657\n  (39999.6,44999.55]  1807   554   636  2286\n  (44999.55,49999.5]  1214   313   346  1793\n  (49999.5,54999.45]   699   149   191   871\n  (54999.45,59999.4]   538   128   198   784\n  (59999.4,64999.35]   277    54    92   489\n  (64999.35,69999.3]   325    91   153   435\n  (69999.3,74999.25]   148    19    36   227\n  (74999.25,79999.2]   162    34    39   186\n  (79999.2,84999.15]   120    23    30    76\n  (84999.15,89999.1]    40    14    16    87\n  (89999.1,94999.05]    14     3     6    14\n  (94999.05,99999]      37    19    10    14\n\n\nCan we cut year into intervals? Yes, we can do year, odometer, or price because they are continuous.\n\nlibrary(tidyverse)\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehiclesSmall.Rdata\"))\ntable(cut_interval(dfSmall$year[dfSmall$year&gt;1999],22,dig.lab=6))\n\n\n[2000,2001] (2001,2002] (2002,2003] (2003,2004] (2004,2005] (2005,2006] \n       1919        1348        1720        2037        2527        2987 \n(2006,2007] (2007,2008] (2008,2009] (2009,2010] (2010,2011] (2011,2012] \n       3510        3998        2796        3763        4710        5686 \n(2012,2013] (2013,2014] (2014,2015] (2015,2016] (2016,2017] (2017,2018] \n       7123        6950        7436        7156        8595        8452 \n(2018,2019] (2019,2020] (2020,2021] (2021,2022] \n       5981        4606         537          27 \n\n\nGetting commas in long numbers:\n\nlibrary(scales)\ndfReduced &lt;- dfSmall[dfSmall$price&lt;100000,]\ndfReduced$priceRanges &lt;- cut_interval(dfReduced$price,20,dig.lab=8)\nggplot(dfReduced,aes(priceRanges)) + geom_bar(fill=\"lightblue\") + theme(axis.text.x = element_text(angle=45,vjust=1,hjust=1)) +\n  scale_y_continuous(labels=comma)\n\n\n\n\nIn a boxplot, Ford and Chevy look the same, but a violin plot reveals a bulge in Chevy that is not matched by Ford.\n\nggplot(df[df$price&lt;100000 & df$manufacturer %in% c(\"ford\",\"chevrolet\"),],aes(price,manufacturer))+geom_violin()\n\n\n\n\nThe only obvious numeric variables are price, odometer, and year. Look at how bunched up the odometer values are in the first plot. You can transform them by taking the log() function on them so that the lower values take up more space. By the way, there are only two thirds as many chevys as fords in the Tiny data.\n\nggplot(df[df$price&lt;100000 & df$manufacturer %in% c(\"ford\",\"chevrolet\"),],aes(price,odometer,color=manufacturer))+geom_jitter() + scale_color_manual(values=c('#999999','#E69F00'))\n\nWarning: Removed 1322 rows containing missing values (`geom_point()`).\n\n\n\n\nggplot(df[df$price&lt;100000 & df$manufacturer %in% c(\"ford\",\"chevrolet\"),],aes(price,log(odometer),color=manufacturer))+geom_jitter() + scale_color_manual(values=c('#999999','#E69F00'))\n\nWarning: Removed 1322 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNotice that you can choose any hex value for colors manually. You can check the hex value for any color you can name with google or by sites like https://www.colorhexa.com/.\n\nggplot(df[df$price&lt;100000 & df$manufacturer %in% c(\"ford\",\"chevrolet\"),],aes(price,year,color=manufacturer))+geom_jitter() + scale_color_manual(values=c('#999999','#E69F00'))\n\n\n\n\nYou can see if the three numeric variables are correlated wtih a heatmap.\n\ncol&lt;-colorRampPalette(c(\"red\",\"white\",\"blue\"))(20)\ndfReduced &lt;- df |&gt; drop_na(year,odometer,price)\nheatmap(cor(dfReduced[,c(\"year\",\"odometer\",\"price\")]),symm=TRUE,col=col)\n\n\n\n\nThe year and odometer columns are very inversely correlated. Both year and odometer are somewhat inversely correlated with price.\nYou can make a kind of heatmap with the ztable package.\nlibrary(ztable)\nmycolor=gradientColor(low=\"yellow\",mid=\"orange\",high=\"red\",n=20,plot=TRUE)\n\nztable(table(dfReduced$paint_color,dfReduced$manufacturer)) |&gt; makeHeatmap(mycolor=mycolor) |&gt; print()\n\n\n\n\n\n\n\n\n\n\n \n\nacura\nalfa-romeo\naston-martin\naudi\nbmw\nbuick\ncadillac\nchevrolet\nchrysler\ndatsun\ndodge\nferrari\nfiat\nford\ngmc\nharley-davidson\nhonda\nhyundai\ninfiniti\njaguar\njeep\nkia\nland rover\nlexus\nlincoln\nmazda\nmercedes-benz\nmercury\nmini\nmitsubishi\nmorgan\nnissan\npontiac\nporsche\nram\nrover\nsaturn\nsubaru\ntesla\ntoyota\nvolkswagen\nvolvo\n\n\n\nblack\n\n\n1001\n\n\n217\n\n\n5\n\n\n1926\n\n\n3790\n\n\n672\n\n\n1644\n\n\n7151\n\n\n805\n\n\n2\n\n\n2224\n\n\n9\n\n\n89\n\n\n8512\n\n\n2971\n\n\n86\n\n\n2729\n\n\n1229\n\n\n1064\n\n\n518\n\n\n3239\n\n\n1216\n\n\n1\n\n\n1245\n\n\n900\n\n\n814\n\n\n2997\n\n\n91\n\n\n288\n\n\n411\n\n\n0\n\n\n2785\n\n\n205\n\n\n250\n\n\n2342\n\n\n588\n\n\n107\n\n\n806\n\n\n72\n\n\n3548\n\n\n1647\n\n\n504\n\n\n\n\nblue\n\n\n307\n\n\n164\n\n\n3\n\n\n718\n\n\n1427\n\n\n292\n\n\n287\n\n\n3613\n\n\n506\n\n\n9\n\n\n937\n\n\n0\n\n\n31\n\n\n4664\n\n\n862\n\n\n5\n\n\n2096\n\n\n1161\n\n\n637\n\n\n206\n\n\n1003\n\n\n492\n\n\n1\n\n\n354\n\n\n345\n\n\n564\n\n\n643\n\n\n128\n\n\n372\n\n\n220\n\n\n0\n\n\n1170\n\n\n183\n\n\n99\n\n\n921\n\n\n69\n\n\n112\n\n\n1496\n\n\n81\n\n\n2290\n\n\n1040\n\n\n324\n\n\n\n\nbrown\n\n\n73\n\n\n9\n\n\n0\n\n\n65\n\n\n157\n\n\n233\n\n\n109\n\n\n1014\n\n\n94\n\n\n3\n\n\n83\n\n\n0\n\n\n19\n\n\n914\n\n\n441\n\n\n0\n\n\n414\n\n\n155\n\n\n178\n\n\n16\n\n\n249\n\n\n214\n\n\n0\n\n\n139\n\n\n53\n\n\n42\n\n\n154\n\n\n36\n\n\n53\n\n\n128\n\n\n1\n\n\n283\n\n\n32\n\n\n26\n\n\n175\n\n\n18\n\n\n18\n\n\n102\n\n\n2\n\n\n556\n\n\n77\n\n\n49\n\n\n\n\ncustom\n\n\n74\n\n\n17\n\n\n0\n\n\n44\n\n\n137\n\n\n128\n\n\n143\n\n\n838\n\n\n130\n\n\n2\n\n\n186\n\n\n2\n\n\n33\n\n\n1173\n\n\n240\n\n\n4\n\n\n440\n\n\n167\n\n\n74\n\n\n19\n\n\n313\n\n\n146\n\n\n0\n\n\n136\n\n\n61\n\n\n48\n\n\n126\n\n\n40\n\n\n36\n\n\n30\n\n\n0\n\n\n289\n\n\n47\n\n\n12\n\n\n240\n\n\n32\n\n\n21\n\n\n137\n\n\n1\n\n\n698\n\n\n95\n\n\n58\n\n\n\n\ngreen\n\n\n26\n\n\n4\n\n\n2\n\n\n51\n\n\n97\n\n\n57\n\n\n37\n\n\n741\n\n\n57\n\n\n2\n\n\n351\n\n\n0\n\n\n32\n\n\n1283\n\n\n144\n\n\n0\n\n\n365\n\n\n67\n\n\n30\n\n\n88\n\n\n685\n\n\n224\n\n\n1\n\n\n81\n\n\n39\n\n\n66\n\n\n48\n\n\n57\n\n\n119\n\n\n97\n\n\n0\n\n\n139\n\n\n59\n\n\n8\n\n\n151\n\n\n63\n\n\n46\n\n\n422\n\n\n2\n\n\n816\n\n\n102\n\n\n43\n\n\n\n\ngrey\n\n\n367\n\n\n11\n\n\n1\n\n\n560\n\n\n881\n\n\n218\n\n\n217\n\n\n2418\n\n\n379\n\n\n2\n\n\n906\n\n\n2\n\n\n36\n\n\n3263\n\n\n664\n\n\n3\n\n\n2100\n\n\n733\n\n\n278\n\n\n48\n\n\n960\n\n\n487\n\n\n3\n\n\n470\n\n\n94\n\n\n437\n\n\n626\n\n\n53\n\n\n85\n\n\n154\n\n\n0\n\n\n1643\n\n\n125\n\n\n75\n\n\n827\n\n\n111\n\n\n47\n\n\n690\n\n\n28\n\n\n2718\n\n\n646\n\n\n186\n\n\n\n\norange\n\n\n1\n\n\n1\n\n\n0\n\n\n4\n\n\n35\n\n\n2\n\n\n5\n\n\n250\n\n\n4\n\n\n4\n\n\n213\n\n\n0\n\n\n4\n\n\n330\n\n\n12\n\n\n2\n\n\n76\n\n\n63\n\n\n2\n\n\n0\n\n\n195\n\n\n38\n\n\n0\n\n\n4\n\n\n2\n\n\n6\n\n\n4\n\n\n1\n\n\n30\n\n\n190\n\n\n0\n\n\n45\n\n\n22\n\n\n3\n\n\n65\n\n\n3\n\n\n13\n\n\n71\n\n\n0\n\n\n52\n\n\n63\n\n\n3\n\n\n\n\npurple\n\n\n2\n\n\n0\n\n\n0\n\n\n2\n\n\n13\n\n\n18\n\n\n10\n\n\n95\n\n\n9\n\n\n0\n\n\n70\n\n\n0\n\n\n0\n\n\n77\n\n\n13\n\n\n0\n\n\n75\n\n\n11\n\n\n3\n\n\n1\n\n\n24\n\n\n34\n\n\n0\n\n\n1\n\n\n1\n\n\n10\n\n\n8\n\n\n2\n\n\n3\n\n\n3\n\n\n0\n\n\n42\n\n\n7\n\n\n2\n\n\n18\n\n\n0\n\n\n2\n\n\n9\n\n\n0\n\n\n37\n\n\n8\n\n\n1\n\n\n\n\nred\n\n\n239\n\n\n94\n\n\n0\n\n\n185\n\n\n375\n\n\n507\n\n\n431\n\n\n5230\n\n\n336\n\n\n7\n\n\n1092\n\n\n30\n\n\n71\n\n\n5134\n\n\n979\n\n\n4\n\n\n1130\n\n\n796\n\n\n97\n\n\n85\n\n\n1595\n\n\n660\n\n\n1\n\n\n408\n\n\n252\n\n\n597\n\n\n393\n\n\n97\n\n\n287\n\n\n331\n\n\n0\n\n\n1340\n\n\n289\n\n\n58\n\n\n1128\n\n\n71\n\n\n135\n\n\n553\n\n\n30\n\n\n2822\n\n\n610\n\n\n159\n\n\n\n\nsilver\n\n\n912\n\n\n28\n\n\n2\n\n\n950\n\n\n1497\n\n\n545\n\n\n674\n\n\n4859\n\n\n788\n\n\n3\n\n\n1066\n\n\n3\n\n\n24\n\n\n4895\n\n\n1067\n\n\n7\n\n\n2977\n\n\n1389\n\n\n475\n\n\n191\n\n\n1884\n\n\n1081\n\n\n1\n\n\n1362\n\n\n386\n\n\n460\n\n\n1558\n\n\n134\n\n\n145\n\n\n314\n\n\n0\n\n\n2424\n\n\n225\n\n\n125\n\n\n1141\n\n\n107\n\n\n96\n\n\n1208\n\n\n41\n\n\n5079\n\n\n1025\n\n\n384\n\n\n\n\nwhite\n\n\n1270\n\n\n149\n\n\n2\n\n\n968\n\n\n2426\n\n\n1008\n\n\n1257\n\n\n10941\n\n\n704\n\n\n4\n\n\n1965\n\n\n8\n\n\n133\n\n\n18448\n\n\n3756\n\n\n0\n\n\n2293\n\n\n1513\n\n\n586\n\n\n322\n\n\n2441\n\n\n1145\n\n\n1\n\n\n1517\n\n\n731\n\n\n618\n\n\n2223\n\n\n130\n\n\n322\n\n\n570\n\n\n1\n\n\n3135\n\n\n233\n\n\n220\n\n\n4742\n\n\n402\n\n\n83\n\n\n1104\n\n\n394\n\n\n5224\n\n\n1417\n\n\n454\n\n\n\n\nyellow\n\n\n2\n\n\n0\n\n\n0\n\n\n10\n\n\n20\n\n\n6\n\n\n20\n\n\n514\n\n\n22\n\n\n0\n\n\n55\n\n\n1\n\n\n6\n\n\n372\n\n\n61\n\n\n2\n\n\n40\n\n\n10\n\n\n16\n\n\n2\n\n\n214\n\n\n16\n\n\n0\n\n\n14\n\n\n8\n\n\n6\n\n\n29\n\n\n6\n\n\n31\n\n\n8\n\n\n0\n\n\n34\n\n\n33\n\n\n15\n\n\n27\n\n\n6\n\n\n9\n\n\n11\n\n\n1\n\n\n88\n\n\n68\n\n\n6\n\n\n\n\n\n\n\n\nThe above table is obviously too wide for a report. I suggest you make something like it while you’re working, then use it to figure out which columns to include, then do something like the following. Note that the expression #| output: asis must be the first line of the following chunk and it is not displayed in the html file. You must look at the qmd file to see it.\ndfFurtherReduced &lt;- dfReduced[dfReduced$manufacturer %in% c(\"bmw\",\"chevrolet\",\"ford\",\"honda\",\"nissan\",\"ram\",\"toyota\"),]\nztable(table(dfFurtherReduced$paint_color,dfFurtherReduced$manufacturer)) |&gt; makeHeatmap(mycolor=mycolor) |&gt; print()\n\n\n\n\n\n\n\n\n\n\n \n\nbmw\nchevrolet\nford\nhonda\nnissan\nram\ntoyota\n\n\n\nblack\n\n\n3790\n\n\n7151\n\n\n8512\n\n\n2729\n\n\n2785\n\n\n2342\n\n\n3548\n\n\n\n\nblue\n\n\n1427\n\n\n3613\n\n\n4664\n\n\n2096\n\n\n1170\n\n\n921\n\n\n2290\n\n\n\n\nbrown\n\n\n157\n\n\n1014\n\n\n914\n\n\n414\n\n\n283\n\n\n175\n\n\n556\n\n\n\n\ncustom\n\n\n137\n\n\n838\n\n\n1173\n\n\n440\n\n\n289\n\n\n240\n\n\n698\n\n\n\n\ngreen\n\n\n97\n\n\n741\n\n\n1283\n\n\n365\n\n\n139\n\n\n151\n\n\n816\n\n\n\n\ngrey\n\n\n881\n\n\n2418\n\n\n3263\n\n\n2100\n\n\n1643\n\n\n827\n\n\n2718\n\n\n\n\norange\n\n\n35\n\n\n250\n\n\n330\n\n\n76\n\n\n45\n\n\n65\n\n\n52\n\n\n\n\npurple\n\n\n13\n\n\n95\n\n\n77\n\n\n75\n\n\n42\n\n\n18\n\n\n37\n\n\n\n\nred\n\n\n375\n\n\n5230\n\n\n5134\n\n\n1130\n\n\n1340\n\n\n1128\n\n\n2822\n\n\n\n\nsilver\n\n\n1497\n\n\n4859\n\n\n4895\n\n\n2977\n\n\n2424\n\n\n1141\n\n\n5079\n\n\n\n\nwhite\n\n\n2426\n\n\n10941\n\n\n18448\n\n\n2293\n\n\n3135\n\n\n4742\n\n\n5224\n\n\n\n\nyellow\n\n\n20\n\n\n514\n\n\n372\n\n\n40\n\n\n34\n\n\n27\n\n\n88"
  },
  {
    "objectID": "week07.html#textbook-section-6.1-inference-for-a-single-proportion",
    "href": "week07.html#textbook-section-6.1-inference-for-a-single-proportion",
    "title": "7  More about Inference",
    "section": "7.2 Textbook section 6.1 Inference for a single proportion",
    "text": "7.2 Textbook section 6.1 Inference for a single proportion\n\n7.2.1 Is the sample proportion nearly normal?\nNote: The book uses \\(p\\) in two ways: as a \\(p\\)-value in a hypothesis test, and as \\(p\\), a population proportion. Related to the population proportion is the sample proportion, \\(\\hat{p}\\), pronounced p-hat. Too many \\(p\\)s!\nThe sampling distribution for \\(\\hat{p}\\) based on a sample of size \\(n\\) from a population with a true proportion \\(p\\) is nearly normal when:\n\nThe sample’s observations are independent, e.g., are from a simple random sample.\nWe expected to see at least 10 successes and 10 failures in the sample, i.e., \\(np \\geqslant 10\\) and \\(n(1 − p) \\geqslant 10\\). This is called the success-failure condition.\n\nWhen these conditions are met, then the sampling distribution of \\(\\hat{p}\\) is nearly normal with mean \\(p\\) and standard error \\(\\text{SE}=\\sqrt{p(1-p)/n}\\).\n\n\n7.2.2 Confidence interval for a proportion\nA confidence interval provides a range of plausible values for the parameter \\(p\\), and when \\(\\hat{p}\\) can be modeled using a normal distribution, the confidence interval for \\(p\\) takes the form\n\\[ \\hat{p} \\pm z^{*} \\times \\text{SE} \\]\nwhere \\(z^{*}\\) marks the \\(x\\)-axis for the selected confidence interval, e.g., 1.96 for a 95 percent confidence interval.\n\n\n7.2.3 Prepare, Check, Calculate, Conclude Cycle\nThe OpenIntro Stats book recommends a four step cycle for both confidence intervals and hypothesis tests. It differs a bit from the seven step hypothesis testing method given in Week 06, but achieves the same result.\n\nPrepare. Identify \\(\\hat{p}\\) and \\(n\\), and determine what confidence level you wish to use.\nCheck. Verify the conditions to ensure \\(\\hat{p}\\) is nearly normal. For one-proportion confidence intervals, use \\(\\hat{p}\\) in place of \\(p\\) to check the success-failure condition.\nCalculate. If the conditions hold, compute SE using \\(\\hat{p}\\), find \\(z^{*}\\), and construct the interval.\nConclude. Interpret the confidence interval in the context of the problem.\n\n\n\n7.2.4 Same cycle for hypothesis testing for a proportion\n\nPrepare. Identify the parameter of interest, list hypotheses, identify the significance level, and identify \\(\\hat{p}\\) and \\(n\\).\nCheck. Verify conditions to ensure \\(\\hat{p}\\) is nearly normal under \\(H_0\\). For one-proportion hypothesis tests, use the null value to check the success-failure condition.\nCalculate. If the conditions hold, compute the standard error, again using \\(p_0\\), compute the \\(Z\\)-score, and identify the \\(p\\)-value.\nConclude. Evaluate the hypothesis test by comparing the \\(p\\)-value to \\(\\alpha\\), and provide a conclusion in the context of the problem.\n\n\n\n7.2.5 Choosing sample size when estimating a proportion\nThis is probably the most important part of this section for practical purposes. The following expression denotes the margin of error:\n\\[ z^{*}\\sqrt{\\frac{p(1-p)}{n}} \\]\nYou have to choose the margin of error you want to report. The book gives an example of 0.04. So you want to find\n\\[ z^{*}\\sqrt{\\frac{p(1-p)}{n}} &lt; 0.04 \\]\nThe problem is that you don’t know \\(p\\). Since the worst-case scenario is \\(p=0.5\\), you have to use that unless you have some information about \\(p\\). Recall that \\(z^{*}\\) represents the \\(z\\)-score for the desired confidence level, so you have to choose that. The book gives an example where you want a 95 percent confidence level, so you choose 1.96. You could find this out in R by saying\n\nqnorm(0.025,lower.tail=FALSE)\n\n[1] 1.959964\n\n\nreturning the \\(z\\)-score for the upper tail. The reason for saying that 0.025 instead of 0.05 is that the probability of 0.05 is split between the tails. The complementary function is pnorm(1.959964,lower.tail=FALSE), which will return 0.025.\nOnce you have decided on values for \\(p\\) and \\(z^*\\), solve the above inequality for \\(n\\)."
  },
  {
    "objectID": "week07.html#textbook-section-6.2-difference-of-two-proportions",
    "href": "week07.html#textbook-section-6.2-difference-of-two-proportions",
    "title": "7  More about Inference",
    "section": "7.3 Textbook section 6.2 Difference of two proportions",
    "text": "7.3 Textbook section 6.2 Difference of two proportions\nIn this section, we’re just modifying the previous section to account for a difference instead of a single proportion.\nThe difference \\(\\hat{p}_1-\\hat{p}_2\\) can be modeled using the normal distribution when\n\nThe data are independent within and between the two groups (random samples or randomized experiment)\nThe success-failure condition holds for both groups (at least 10 successes and 10 failures in each sample)\n\n\n7.3.1 Confidence intervals for \\(p_1-p_2\\)\n\\[ \\text{point estimate } \\pm z^* \\times \\text{SE} \\rightarrow (\\hat{p}_1 - \\hat{p}_2) \\pm z^* \\times \\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}} \\]\n\n\n7.3.2 Hypothesis testing for \\(p_1-p_2\\)\nWhen the null hypothesis is that the proportions are equal, use the pooled proportion (\\(\\hat{p}_\\text{pooled}\\)) to verify the success-failure condition and estimate the standard error\n\\[ \\hat{p}_\\text{pooled} = \\frac{\\text{number of “successes”}}{\\text{number of cases}} = \\frac{\\hat{p}_1n_1+\\hat{p}_2n_2}{n_1+n_2} \\]\nHere \\(\\hat{p}_1n_1\\) represents the number of successes in sample 1 since\n\\[ \\hat{p}_1 = \\frac{\\text{number of successes in sample 1}}{n_1} \\]\nSimilarly, \\(\\hat{p}_2n_2\\) represents the number of successes in sample 2."
  },
  {
    "objectID": "week07.html#textbook-section-6.3-testing-goodness-of-fit-using-chi2",
    "href": "week07.html#textbook-section-6.3-testing-goodness-of-fit-using-chi2",
    "title": "7  More about Inference",
    "section": "7.4 Textbook section 6.3 Testing goodness of fit using \\(\\chi^2\\)",
    "text": "7.4 Textbook section 6.3 Testing goodness of fit using \\(\\chi^2\\)\nThe \\(\\chi^2\\) test, pronounced k\\(\\overline{\\text{i}}\\) square, is useful in many circumstances. The textbook treats two such circumstances:\n\nSuppose your sample can be divided into groups, as can the general population. Does your sample represent the general population?\nDoes your sample resemble a particular distribution, such as the normal distribution?\n\nFor the first circumstance, we could divide a sample of people into races or genders and we would like to examine all at once for resemblance to the general population, rather than in pairs. The \\(\\chi^2\\) statistic will permit an all-at-once comparison.\nThe \\(\\chi^2\\) statistic is given by the following formula for \\(g\\) groups.\n\\[ \\chi^2 = \\frac{(\\text{observed count}_1-\\text{null count}_1)^2}{\\text{null count}_1} + \\cdots + \\frac{(\\text{observed count}_g-\\text{null count}_g)^2}{\\text{null count}_g} \\]\nwhere the expression null count refers to the expected number of objects in the group. You have to be careful about how you determine the null count. For instance, the textbook gives an example of races of jurors. In such a case, the null counts should come from the population who can be selected as jurors. This might be a matter of some dispute since jurors are usually recruited through voting records and these records may not reflect the correct proportions. Statisticians tend to like things they can count, and some people are harder (more expensive) to count than others, particularly people in marginalized populations.\n\n7.4.1 The \\(\\chi^2\\) distribution\nThe \\(\\chi^2\\) distribution is sometimes used to characterize data sets and statistics that are always positive and typically right skewed. Recall a normal distribution had two parameters – mean and standard deviation – that could be used to describe its exact characteristics. The \\(\\chi^2\\) distribution has just one parameter called degrees of freedom (df), which influences the shape, center, and spread of the distribution. Here is a picture of the \\(\\chi^2\\) distribution for several values of df (1–8).\n\n\n\n\n\n\nIn the jurors example, we can calculate the appropriate \\(p\\)-value in R by using the \\(\\chi^2\\) statistic calculated from the sample, 5.89, and the parameter \\(k-1\\) which is the number of groups minus one, using R:\n\npchisq(5.89,3,lower.tail=FALSE)\n\n[1] 0.1170863\n\n\nThis is a relatively large \\(p\\)-value given our earlier choices of cutoffs of 0.1, 0.05, and 0.01.\n\n\n7.4.2 \\(\\chi^2\\) test\nThe \\(\\chi^2\\) test can be conducted in R for the juror example given in the book as follows.\n\no &lt;- c(205,26,25,19)\ne &lt;- c(198,19.25,33,24.75)/sum(o)\nchisq.test(o,p=e)\n\n\n    Chi-squared test for given probabilities\n\ndata:  o\nX-squared = 5.8896, df = 3, p-value = 0.1171\n\n\nNote that I had to make an adjustment in R. The R variable p is supposed to be a vector of probabilities summing to 1. The way the table in the book presented it, it was not a vector of probabilities summing to one. So I divided each element of the input vector for e by the sum of the vector o."
  },
  {
    "objectID": "week07.html#textbook-section-6.4-testing-independence-in-2-way-tables",
    "href": "week07.html#textbook-section-6.4-testing-independence-in-2-way-tables",
    "title": "7  More about Inference",
    "section": "7.5 Textbook section 6.4 Testing independence in 2-way tables",
    "text": "7.5 Textbook section 6.4 Testing independence in 2-way tables\nSuppose you have a two way table. Datacamp gives an example of gender and sport as the two ways. The following data frame lists the number of males and females who like the following three sports: archery, boxing, and cycling. The \\(\\chi^2\\) tests suggests that the genders are not independent for the three sports, meaning that the preferences may differ by gender.\n\nfemale &lt;- c(35,15,50)\nmale &lt;- c(10,30,60)\ndf &lt;- cbind(male,female)\nrownames(df) &lt;- c(\"archery\",\"boxing\",\"cycling\")\ndf\n\n        male female\narchery   10     35\nboxing    30     15\ncycling   60     50\n\nchisq.test(df)\n\n\n    Pearson's Chi-squared test\n\ndata:  df\nX-squared = 19.798, df = 2, p-value = 5.023e-05"
  },
  {
    "objectID": "week08.html#recap-week-07",
    "href": "week08.html#recap-week-07",
    "title": "8  Inference for Numerical Data",
    "section": "8.1 Recap Week 07",
    "text": "8.1 Recap Week 07\n\nInference for Categorical Data\n\nInference for a single proportion\nDifference of two proportions\nTesting goodness of fit using \\(\\chi^2\\)\nTesting independence of contingency tables"
  },
  {
    "objectID": "week08.html#inference-for-numerical-data",
    "href": "week08.html#inference-for-numerical-data",
    "title": "8  Inference for Numerical Data",
    "section": "8.2 Inference for numerical data",
    "text": "8.2 Inference for numerical data\n\nTextbook section 7.1 One-sample means with the t-distribution\nTextbook section 7.2 Paired data\nTextbook section 7.3 Difference of two means\nTextbook section 7.4 Power calculations for a difference of means\nTextbook section 7.5 Comparing many means with ANOVA\n\n\n8.2.1 Textbook Section 7.1 One-sample means with the \\(t\\)-distribution\nModeling \\(\\bar{x}\\), the mean of a random sample from a normally distributed population, requires that the sample elements are\n\nindependent—a random sample or a sample from a random process\nnormally distributed—sample drawn from a normally distributed population\n\n\n8.2.1.1 Rule of thumb for normality\n\n\\(n &lt; 30\\) and no outliers, assume data come from a normally distributed population\n\\(n \\geqslant 30\\) and no extreme outliers, assume \\(\\bar{x}\\sim N(\\mu,\\sigma)\\) even if data come from a not normally distributed population\n\n\n\n8.2.1.2 \\(t\\)-distribution\nThe \\(t\\)-distribution is useful for small samples (\\(n&lt;30\\)). It was discovered when a man named Gossett was trying to figure out how few samples of beer he could get away with in tests for the Guinness brewery about 120 years ago. He preferred to remain anonymous at the time because he didn’t want his employers to question his outside activities, otherwise this would probably be called the Gossett’s \\(t\\)-distribution. Instead, he referred to himself as “A Student” so it came to be known as the Student’s \\(t\\)-distribution.\nFor sample sizes over thirty, it converges to looking like the normal distribution, but for smaller samples, it gets more and more peaked and the tails get thicker and thicker. For example, here is the density function for a sample size of 4.\n\nplot(function(x) dt(x, df = 3), -11, 11, ylim = c(0, 0.50), main = \"t density function\", yaxs = \"i\")\n\n\n\n\nBear in mind that the t() function in R has nothing to do with the \\(t\\)-distribution (it’s for transposing matrices and data frames). Instead, the functions for handling the \\(t\\)-distribution are the letter t prefaced by d, q, p, or r. You may have noticed that we saw functions like pnorm() and dnorm() when working with the normal distribution. These functions are analogous.\nThe \\(t\\)-distribution has \\(n-1\\) degrees of freedom, so you can tell that the above example has \\(n=4\\) since \\(\\text{df}=3\\).\nAlso keep in mind that the mean is always zero for the \\(t\\)-distribution, so it just has one parameter, df. So in the above example, you could say \\(\\bar{x}\\sim t(3)\\).\nAnalogous to the pnorm() function, you can calculate regions of the \\(t\\)-distribution using the pt() function. For example, if you conduct a test that returns a \\(t\\)-statistic of \\(-2.10\\) and \\(n=19\\), you can use the following to find out that the area to the left of the statistic is 0.025. (This example and the two following are illustrated in textbook Figure 7.4.)\n\npt(-2.1,18)\n\n[1] 0.0250452\n\n\nSuppose you obtain a \\(t\\)-statistic of 1.65 on 20 degrees of freedom. How much of the probability is in the upper tail? There are two obvious ways to do this.\n\npt(1.65,20,lower.tail=FALSE)\n\n[1] 0.05728041\n\n1-pt(1.65,20)\n\n[1] 0.05728041\n\n\nFind the probability in both tails for a \\(t\\)-statistic of \\(\\pm 3\\) and two degrees of freedom.\n\npt(-3,2)+pt(3,2,lower.tail=FALSE)\n\n[1] 0.09546597\n\n\nTextbook example 7.8 asks you to calculate the \\(t\\)-statistic when you know the proportion. In this case, \\(df=18\\) and you want to know the \\(t\\)-statistic corresponding to 0.025 in the upper tail. You can use the qt() function where q stands for quantile. The region 0.025 in the upper tail corresponds to a 95 percent confidence interval because there will be 0.025 in each of the two tails for a total of five percent. The \\(t\\)-statistic for the lower tail would simply be the negative of the \\(t\\)-statistic for the upper tail.\n\nqt(0.025,18,lower.tail=FALSE)\n\n[1] 2.100922\n\n\nTo construct a confidence interval, you’ll generally choose 90 or 95 percent, depending on the sensitivity of the real world problem. Then you’ll plug that into the following formula.\n\\[\n\\bar{x} \\pm t^*_{df}\\frac{s}{\\sqrt{n}}\n\\]\nThis assumes you have already checked the normality and independence constraints.\n\n\n8.2.1.3 Calculating a confidence interval\nFor the textbook examples, you are given components of the formula. It is quite a bit simpler if you are given the raw data. For example, calculate a 95 percent confidence interval for the body mass in grams of the penguins in the Palmer penguins data frame.\n\nlibrary(palmerpenguins)\nmodel &lt;- lm(body_mass_g ~ 1,penguins)\nconfint(model,level=0.95)\n\n               2.5 %  97.5 %\n(Intercept) 4116.458 4287.05\n\n\nThe above incantation may seem a little mysterious but you’ll explore it in excruciating detail when you learn linear regression.\n\n\n8.2.1.4 One sample \\(t\\)-tests\nThe textbook gives a lengthy example of the runner times of the Cherry Blossom race. I assume that the data are the run10samp and run17samp data frames from the textbook website, so I downloaded the RData versions of them and loaded them as follows.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/run10samp.rda\"))\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/run17samp.rda\"))\nt.test(run17samp$clock_sec/60,mu=93.29,alternative=\"t\")\n\n\n    One Sample t-test\n\ndata:  run17samp$clock_sec/60\nt = 1.9776, df = 99, p-value = 0.05075\nalternative hypothesis: true mean is not equal to 93.29\n95 percent confidence interval:\n  93.26973 105.46227\nsample estimates:\nmean of x \n   99.366 \n\n\nNote that neither time measure, clock_sec nor net_sec correspond to the mean in the textbook. The \\(t\\)-statistic is smaller and the \\(p\\)-value is larger than that given in the textbook. With a \\(p\\)-value of 0.05075 it is unclear whether you would reject the null hypothesis. Certainly the old mean is within, though at the edge, of the confidence interval. Personally, I would fail to reject in this case.\n\n\n\n8.2.2 Textbook Section 7.2 Paired data\nSuppose you want to know if two data frames were drawn from the same distribution or if they differ.\nThe textbook example is of the mean prices of textbooks on Amazon and in the UCLA campus bookstore. The data appear to be the textbooks data frame on the textbook website, although the statistics are different and the textbook says that there were two such samples (only one is on the website that I could find). Because they have precomputed the difference as the diff column, you can do this the same way as for a one sample test.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/textbooks.rda\"))\nwith(textbooks,t.test(diff))\n\n\n    One Sample t-test\n\ndata:  diff\nt = 7.6488, df = 72, p-value = 6.928e-11\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n  9.435636 16.087652\nsample estimates:\nmean of x \n 12.76164 \n\n\nNote that the default is that the difference is 0 and the 95 percent confidence interval is quite far from including 0. Also, the \\(p\\)-value is infinitesimal. We definitely reject the null hypothesis that the stores have similar prices.\nIf you didn’t have the diff column, you could get the same result by saying the following.\n\nwith(textbooks,t.test(ucla_new,amaz_new,paired=TRUE))\n\n\n    Paired t-test\n\ndata:  ucla_new and amaz_new\nt = 7.6488, df = 72, p-value = 6.928e-11\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n  9.435636 16.087652\nsample estimates:\nmean difference \n       12.76164 \n\n\n\n\n8.2.3 Textbook Section 7.3 Difference of two means\nIn the previous section, you considered the means of the differences but in this section you consider the differences of the means. In the Amazon and UCLA example, the items were paired and we subtracted the price of a particular title sold by one seller from the price of the same title sold by the other seller. But what if we have data that is not paired like this? The textbook gives an example of a radical stem cell treatment given to sheep. One of two treatments is given to each sheep, but there is no correspondence between individual pairs of sheep.\nIn this case, there may be different variance between the two groups, as well as different means. So the standard error is calculated as\n\\[\\text{SE}=\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\]\nNotice that this formula implies that the samples could differ in size as well as variance.\nFor the sheep example, heart pumping capacity was measured, where more is better. The stem_cell data frame on the textbook website seems to be the appropriate data frame here. Conducting the test in R follows.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/stem_cell.rda\"))\nwith(stem_cell,t.test(after-before~trmt,var.equal=FALSE))\n\n\n    Welch Two Sample t-test\n\ndata:  after - before by trmt\nt = -4.0073, df = 12.225, p-value = 0.001678\nalternative hypothesis: true difference in means between group ctrl and group esc is not equal to 0\n95 percent confidence interval:\n -12.083750  -3.582916\nsample estimates:\nmean in group ctrl  mean in group esc \n         -4.333333           3.500000 \n\n\nThe result is a statistically significant difference between the sheep in the control group and the sheep treated with stem cells. The sheep in the stem cell group enjoyed a 3.5 unit increase in heart pumping capacity, while the poor sheep in the control group lost four and a third units. Of course, the practical question you have to ask yourself is whether these numbers have a practical significance. You would need domain knowledge to tell whether 3.5 units is a lot of heart pumping capacity!\n\n\n8.2.4 Textbook Section 7.4 Power calculations for a difference of means\nThe pictures in section 7.4, particularly the two on page 280 of the textbook, are essential for understanding power calculation, so let’s use the textbook exclusively for this section. To do the calculations in R, you can use the pwr package. However, for the weekly exercises, please use the method described in the Diez, Çetinkaya-Rundel, and Barr (2019) book instead of using the pwr package. This is simply so you understand the meaning of the calculations. After you’ve done it once, I’ll describe the pwr approach.\nAll I want to say to preface our look at the textbook is to define the terms power and \\(\\alpha\\). The statistical term power refers to the probability of rejecting the null hypothesis when it is false. The greek letter \\(\\alpha\\), pronounced alpha, refers to the probability of rejecting the null hypothesis when it is true. It is typical in practice to use \\(\\alpha = 0.05\\) except in life or death cases (where it is set to 0.01) and to use power\\(=0.8\\).\n\n\n8.2.5 Textbook Section 7.5 Comparing many means with ANOVA\nThe textbook makes three important observations about the diagnostics for ANOVA:\n\nIndependence is always important\nThe normality condition is especially important in the face of small sample sizes\nThe constant variance assumption is especially important in the face sample sizes differing between groups\n\nTextbook exercise 7.54 compares eight methods for loosening rusty bolts. Four samples were collected for each method and the results are in the penetrating_oil data frame on the textbook website. You can conduct an ANOVA test on the results using R as follows.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/penetrating_oil.rda\"))\nwith(penetrating_oil,anova(lm(torque~treatment)))\n\nAnalysis of Variance Table\n\nResponse: torque\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \ntreatment  7 3603.4  514.78  4.0263 0.005569 **\nResiduals 22 2812.8  127.85                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe table shows that the numerator degrees of freedom are \\(k-1=7\\). Keep in mind that \\(k\\) is the number of groups, which must therefore be eight. The denominator degrees of freedom are \\(n-k=22\\), so there must be a total of 30 samples. Yet there are eight groups, suggesting four samples per group. Examining the data closely, you will find that there are only two samples for heat and four for each of the others.\nHere you are comparing whether any of the eight means of torque differ. They certainly seem to, with a large \\(F\\)-statistic and a small \\(p\\)-value. It might be helpful to visualize the differences with a combination violin plot and boxplot. There are really too few samples for each treatment.\n\nlibrary(tidyverse)\npenetrating_oil |&gt;\n  ggplot(aes(torque,treatment)) +\n  geom_violin() +\n  geom_boxplot(width=0.1)\n\n\n\n\n\n\n\n\nDiez, David, Mine Çetinkaya-Rundel, and Cristopher D Barr. 2019. OpenIntro Statistics, Fourth Edition. self-published. https://openintro.org/os."
  },
  {
    "objectID": "week09.html#recap-week-8-inference-for-numerical-data",
    "href": "week09.html#recap-week-8-inference-for-numerical-data",
    "title": "9  Introduction to Linear Regression",
    "section": "9.1 Recap week 8: Inference for numerical data",
    "text": "9.1 Recap week 8: Inference for numerical data\n\nTextbook section 7.1 One-sample means with the t-distribution\nTextbook section 7.2 Paired data\nTextbook section 7.3 Difference of two means\nTextbook section 7.4 Power calculations for a difference of means\nTextbook section 7.5 Comparing many means with ANOVA"
  },
  {
    "objectID": "week09.html#linear-regression",
    "href": "week09.html#linear-regression",
    "title": "9  Introduction to Linear Regression",
    "section": "9.2 Linear Regression",
    "text": "9.2 Linear Regression\n\n9.2.1 Introduction\nLet’s start with an example. Suppose we know how many times a team has won and we can graph it as follows. I haven’t included a scale on the graph but the underlying numbers of wins are 9, 15, 18, 22.5 (a tie), and 23.\n\n\n\n\n\nWhat is our best prediction of the number of wins for a new team, irrespective of any other information. The answer is the mean of this number of wins, 17.5. It’s not a very good prediction but it’s the best we can do, given what little we know. Now suppose we know how much the team spends on player salaries. We’ll call that variable payroll and add it to the graph.\n\n\n\n\n\nNow we can see a pattern. Teams that spend more win more! But that’s not strictly true because there must be other variables at play. Right now, we don’t know what those variables are. But the best prediction we can make for the number of wins for a given new team depends in part on its payroll. We can now draw a diagonal line through this cloud of points and say that that line represents the best prediction for number of wins, like so.\n\n\n\n\n\nOne immediate question to ask is whether the diagonal line pictured above is the best line. People searched for ways of finding the best line for a long time before the most popular method, least squares, was published in 1805. That is the method used most frequently by software but other methods have emerged since then.\nThe best line is often called the least squares line and it is characterized by two numbers, the slope and the intercept. The intercept is the height at which it intersects the y axis and the slope is the ratio of its rise (its increaase or decrease on the y axis) over its run (its increase or decrease on the x axis). Both the slope and intercept can be positive or negative. A slope of zero or infinity is meaningless. In the statistics world, the intercept is usually called \\(\\beta_0\\), pronounced beta nought, and the slope is usually called \\(\\beta_1\\), pronounced beta one. The reason we use the numeral one is because additional slopes will be considered when we talk about multiple regression.\nThe Diez, Çetinkaya-Rundel, and Barr (2019) textbook differs a bit from standard practice, using the Latin letter b in place of the Greek \\(\\beta\\). This works well from my point of view because Latin letters are often used to denote estimates of parameters, while the parameters themselves are denoted by Greek letters. Keep in mind that we’re always working with samples so we’re estimating the true slope and intercept with \\(b_1\\) and \\(b_0\\). Other statistics books sometimes use \\(\\hat{\\beta}\\) to refer to estimates, which is kind of cumbersome.\nMost of our discussion of regression will focus on this least squares line and how good it is. Keep in mind that the word regression is more general than least squares. There are other methods and applications. Least squares is just the easiest way to introduce regression.\nWhat does the term least squares mean? In the following picture, we have added vertical lines connecting the dots to the least squares line. The squares of the lengths of these lines are the way we measure the quality of the line.\n\n\n\n\n\nIn the following pictures, the prediction line on the right is better than that on the left by an amount proportional to the difference between the total length of pink lines in the two pictures. Notice that both lines represent the very best possible prediction for that set of dots. It’s just that, on the right hand side, there’s a closer correspondence between payroll and wins.\n\n\n\n\n\n\n\n9.2.2 Correlation\nCorrelation is a concept that measures the strength of the linear relationship between two variables. We usually use Pearson’s correlation coefficient, \\(r\\), to measure this kind of relationship. Note that our textbook uses \\(R\\) instead of \\(r\\) to denote this relationship. This is unfortunate, because almost every other statistics book makes a distinction between \\(R\\) as the multiple correlation coefficient and \\(r\\) as Pearson’s correlation coefficient. They happen to be identical in the case of one \\(x\\) and one \\(y\\), but soon we will consider the case of more than one \\(x\\), where they differ.\n\\[r=\\frac{1}{n-1}\\sum^n_{i=1}\\frac{x_i-\\bar{x}}{s_x}\\frac{y_i-\\bar{y}}{s_y}\\]\n\n\n9.2.3 Least squares line\nDiez, Çetinkaya-Rundel, and Barr (2019) gives the following formulas for finding the least squares line.\n\nFind the slope.\n\n\\[b_1=\\frac{s_y}{s_x}r\\]\n\nFind the intercept.\n\n\\[b_0=\\bar{y}-b_1\\bar{x}\\]\nTypically, you use software to identify these numbers. For example, consider the payroll / wins example from above, calculated in R.\n\ny&lt;-c(9,18,15,23,22.5)\nx&lt;-c(5,10,15,20,25)\nsummary(lm(y~x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n   1    2    3    4    5 \n-2.1  3.7 -2.5  2.3 -1.4 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   7.9000     3.4039   2.321   0.1030  \nx             0.6400     0.2053   3.118   0.0526 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.246 on 3 degrees of freedom\nMultiple R-squared:  0.7642,    Adjusted R-squared:  0.6856 \nF-statistic: 9.722 on 1 and 3 DF,  p-value: 0.05256\n\n\nI input the number of wins as \\(y\\) and the amount of the payroll as \\(x\\). Then I constructed a linear model of \\(y\\) explained by \\(x\\). The lm() function constructs a linear model and the tilde character (\\(\\sim\\)) separates the response variable \\(y\\) from the explanatory variable \\(x\\). The tilde can be read aloud as is explained by so, in this case, “\\(y\\) is explained by \\(x\\)”. The summary() function is wrapped around the lm() function to provide the most commonly accessed values of the output of the lm() function.\nThe first value output by summary() is the call. This simply shows the formula entered, which in this case was \\(y\\sim x\\).\nThe next value output by summary() is a list of residuals. These are the differences between the predicted values and the actual values of wins.\nThe third value output by summary() is the coefficients table. The \\(b\\) values are listed in the Estimate column. Instead of being named \\(b_0\\) and \\(b_1\\) they are called (Intercept) and x. The remainder of this table consists of statistics about them. The second column is standard error, the third column is the \\(t\\)-statistic, which is the ratio of the estimate to its standard error. The third column is the \\(p\\)-value, which is the probability of seeing the preceding \\(t\\)-statistic or a larger one if the null hypothesis is true. The null hypothesis here is that \\(x\\) does not predict \\(y\\).\nBy the way, the \\(b\\) values are typically called betas, after the Greek letter \\(\\beta\\). There can be dozen or hundreds of beta values or betas, depending on the field. For example, in commercial websites there are often hundreds of variables collected about each shopper. There is a subdiscipline of statistics and data science that studies the situation where the number of betas approaches the number of observations. (If the number of betas equals the number of observations, a perfect albeit useless prediction results. It’s perfect because one beta is assigned to each observation—explaining it perfectly—and useless because it won’t apply to any future set of observations.)\nThe last column in the coefficents table contains the significance codes. In this case, \\(x\\) gets a significance code of dot (.). Below the coefficients table is a legend for the significance codes. That tells us that dot means that the \\(p\\)-value for \\(x\\) is below 0.1. The blank in the other row tells us that the significance code for the intercept is less than 1, which it must be because probabilities can be no larger than 1.\nBelow the coefficients table we see five important values expressed - Residual standard error, which is the square root of the expression RSS/df, where RSS is the residual sum of squares and df is the degrees of freedom, - Multiple R-squared, which is identical to \\(r^2\\) above (but only identical if there is just one explanatory variable—it becomes more interesting later when we have multiple explanatory variables), - Adjusted R-squared, which we will discuss when we discuss multiple linear regression, - the \\(F\\)-statistic, which we will also discuss under multiple linear regression, and - the \\(p\\)-value of the \\(F\\)-statistic.\nThese latter values are all more interesting in the case of multiple linear regression. For simple linear regression we have enough information in the body of the coefficients table to make a judgment about whether the linear model \\(y\\sim x\\) is sufficient to explain a team’s wins. That judgment depends on whether we are being casual, in which case the model is sufficient, or whether we have money riding on it, in which case the model is just barely insufficient.\n\n\n9.2.4 Assumptions\nBear in mind that we make four big assumptions in using this model at all. The assumptions are mentioned in the book as follows.\n\nLinearity: the data show a linear trend, identified by a scatterplot for instance\nNormally distributed residuals: identified by a Q-Q plot, to be described later\nConstant variability: \\(x\\) does not vary more or less depending on \\(y\\)\nIndependent observations: there is not a pattern like seasonality or growth or decline in the underlying phenomenon being analyzed (special statistics tools are used for that)\n\nMost textbooks use more technical terms for these concepts, especially homoscedasticity for constant variability and heteroscedasticity for non-constant variability. This book just doesn’t want to introduce too much terminology.\nThe most common assumptions violated in my experience are the first and third. There is often a curvilinear pattern in data that is not captured by a linear model. Also, graphs of data often exhibit a pattern like the cross-section of a horn, which is non-constant variability or heteroscedasticity.\n\n\n9.2.5 The Multiple Coefficient of Determination\n\\(R^2\\) is the most common measure of the strength of a linear relationship, partly because it varies between 0 and 1. It is the proportion of variability in the data explained by the model. It is very domain dependent. For a lot of cases, anything below 0.8 indicates a poor fit. On the other hand, there are areas of physics where 0.1 explains enough of the data to be valuable. You have to consider the domain when evaluating \\(R^2\\).\n\n\n9.2.6 Categorical Variables\nWith linear regression, the \\(y\\) variable must NEVER be categorical. If you try to do regression in R with the \\(y\\) variable as categorical, you’ll get an error message. There is another procedure you can do, called logistic regression, which has a categorical \\(y\\). We’ll discuss that later. But for now, bear in mind when you form a model, the outcome is always a continuously valued variable.\nOn the other hand, any or all of the input variables may be categorical. Note the book’s example of Mario Kart sales. The categorical variable condition has two levels or categories, whether the game is new or used. The outcome variable is the price, which is of course a continuous variable. When you input a categorical variable in R, it automatically encodes it as a number. In the Mario Kart case, the numbers are zero and one. The condition new is shown in the same place on the R output as \\(b_1\\) and the condition used is shown as the intercept. The main idea to understand here is the difference between the two, where the used condition is zero and the new condition is the difference between the price of new and used.\n\n\n9.2.7 Outliers\nBack in the nineteen seventies, several prominent statisticians said that statistics needed visualization. Their influece is actually responsible for my teaching visualization in this course. One of them, Francis Anscombe, published a data frame that showed how misleading elementary statistics can be without visualization. This data frame has come to be called Anscombe’s quartet and it is often shown to students. Here it is.\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n\n\nEach \\(x,y\\) pair of this quartet has the same basic statistics and the exact same least squares line. But look at a visualization of them.\n\n\n\n\n\nThe power of outliers can be seen in datasets 3 and 4 and the power of a nonlinear relationship can be seen in dataset 2."
  },
  {
    "objectID": "week09.html#inference-for-linear-regression",
    "href": "week09.html#inference-for-linear-regression",
    "title": "9  Introduction to Linear Regression",
    "section": "9.3 Inference for linear regression",
    "text": "9.3 Inference for linear regression\n\n9.3.1 Confidence intervals\nThe textbook gives formulas for computing confidence intervals. Another way to do so is to use software, such as R. You can use the confint() function to find confidence intervals for coefficients of a linear model. For example, consider our payroll / wins example above.\n\nconfint(lm(y~x))\n\n                  2.5 %    97.5 %\n(Intercept) -2.93279043 18.732790\nx           -0.01324184  1.293242\n\n\nWhat does this mean? It means that 95% of the time, the slope of payroll will fall within the given range. Unfortunately for anyone trying to predict wins from payroll with this data, the range includes zero (just barely). That means that 95% of the time, the slope could be zero. If the slope is zero, there is by definition no relationship between \\(x\\) and \\(y\\) or input and output or independent and dependent variable or explanatory variable and response variable. This fits in with our regression table in which the \\(p\\)-value for \\(x\\) was slightly greater than 0.05. If you said there was no evidence to reject the null hypothesis, you would be technically correct, which is the best kind of correct."
  },
  {
    "objectID": "week09.html#multiple-regression-intro-chapter-9",
    "href": "week09.html#multiple-regression-intro-chapter-9",
    "title": "9  Introduction to Linear Regression",
    "section": "9.4 Multiple regression intro (Chapter 9)",
    "text": "9.4 Multiple regression intro (Chapter 9)\nEverything we’ve done so far has assumed that we know one piece of information’s relationship to another piece of information. Take the example of teams, where we knew the payroll and want to know the number of wins. Suppose we also knew a number of other statistics that might affect wins. How would we incorporate them? The answer is simple. We add them. Because we’re using a linear equation, that is, the equation of a line to model the data, there’s no reason we can’t add terms to the equation. These terms are additive, meaning that we add each term and each term has a coefficient. So now, our estimate of \\(y\\), which we call \\(\\hat{y}\\), looks like this for \\(n\\) terms.\n\\[\\hat{y}=b_0+b_1x_1+b_2x_2+\\cdots+b_nx_n\\]\nIn R, we simply add the column names. For example, consider the built-in data frame mtcars where the outcome variable is mpg. We can construct a model of the relationship between mpg and two input variables we suspect of influencing mpg as follows.\n\nm &lt;- with(mtcars,lm(mpg ~ hp+wt))\nsummary(m)\n\n\nCall:\nlm(formula = mpg ~ hp + wt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\nThe output looks a bit different now. First, there are 32 residuals, so the individual residuals are not listed. Instead, you see summary statistics for the residuals.\nNext, look at the coefficients table. There are three rows now, for the intercept, for hp, and for wt. Notice that all three have significance codes at the end of the row. Normally, you shouldn’t be concerned about the significance code for the intercept, but the other two are interesting. The code for hp is two stars, meaning that it is less than 0.01, while the code for wt is three stars, meaning that it is less than 0.001. The \\(p\\)-value of 1.12e-06, which is abbreviated scientific notation, means to take 1.12 and shift the decimal point six places to the left, giving 0.00000112 as the decimal equivalent.\nThe residual standard error is 2.593 on 29 degrees of freedom. As mentioned before, the residual standard error (often abbreviated \\(R\\!S\\!E\\)) is the square root of the expression formed by the ratio of the residual sum of squares (often abbreviated \\(R\\!S\\!S\\)) divided by the degrees of freedom. Expressing this as a formula is as follows.\n\\[R\\!S\\!E=\\sqrt{\\frac{R\\!S\\!S}{\\text{df}}}\\]\n\\(R\\!S\\!S\\) can be found in R for the above model as follows.\n\n(rss &lt;- sum(residuals(m)^2))\n\n[1] 195.0478\n\n\nSimilarly, \\(R\\!S\\!E\\) can be found in R for the above model as follows.\n\n(rse &lt;- sqrt( sum(residuals(m)^2) / m$df.residual ))\n\n[1] 2.593412\n\n\nThe mathematical formulas for these values are as follows.\n\\[\\begin{align*}\nR\\!S\\!S&=\\sum_{i=1}^{n}(y_i-\\hat{y}_{i})^2 \\\\\n     &=S\\!S_{yy}-\\hat{\\beta}_1S\\!S_{xy}\n\\end{align*}\\]\nThe term \\(S\\!S_{xy}\\) in the above formula is only relevant for simple linear regression with one predictor, where \\(S\\!S_{xy}= \\sum(x_i-\\overline{x})(y_i-\\overline{y})\\). A formula for \\(\\hat{\\beta}_1\\) in that case would be the following.\n\\[\\hat{\\beta}_1=\\frac{S\\!S_{xy}}{S\\!S_{xx}}\\]\n\\(S\\!S_{xx}\\) in that case would be the sum of squares of deviations of the predictors from their mean: \\(\\sum(x_i-\\overline{x})^2\\).\n\\[R\\!S\\!E=\\sqrt{\\frac{\\sum_{i=1}^{n}(y_i-\\hat{y}_{i})^2}{n-(k+1)}} \\]\nwhere \\(k\\) is the number of parameters being estimated. For example, in the above model wt and hp are the two parameters being estimated.\nThe Multiple R-squared is 82 percent and the Adjusted R-squared is 81 percent. This is a good sign because the Adjusted R-squared is adjusted for the case where you have included too many variables on the right hand side of the linear model formula. If it’s similar to Multiple R-squared, that means you probably have not included too many variables.\n\\[ R^2=\\frac{S\\!S_{yy}-R\\!S\\!S}{S\\!S_{yy}}\n   =1-\\frac{R\\!S\\!S}{S\\!S_{yy}} \\]\n\\[ R^2_a=1-\\left(\\frac{n-1}{n-(k+1)}\\right)(1-R^2) \\]\nThe \\(F\\)-statistic is important now, because of its interpretation. The \\(F\\)-statistic tells you that at least one of the variables is significant, taken in combination with the others. The \\(t\\)-statistics only give the individual contribution of the variables, so it’s possible to have a significant \\(t\\)-statistic without a significant \\(F\\)-statistic. The first thing to check in regression output is the \\(F\\)-statistic. If it’s too small, i.e., has a large \\(p\\)-value, try a different model.\n\\[F_c=\\frac{(S\\!S_{yy}-R\\!S\\!S)/k}{R\\!S\\!S/[n-(k+1)]}\n=\\frac{R^2/k}{(1-R^2)/[n-(k+1)]}\\]\nThe \\(c\\) subscript after \\(F\\) simply signifies that it is the computed value of \\(F\\), the one you see in the regression table. This is to contrast it with \\(F_\\alpha\\), the value to which you compare it in making the hypothesis that one or more variables in the model are significant. You don’t need to compute \\(F_\\alpha\\) since the \\(p\\)-value associated with \\(F_c\\) gives you enough information to reject or fail to reject the null hypothesis that none of the variables in the model are significant. If you want to calculate it anyway, you can say the following in R:\n\nqf(0.05,2,29,lower.tail=FALSE)\n\n[1] 3.327654\n\n\nThe above calculation assumes you choose 0.05 as the alpha level and that there are \\(k=2\\) parameters estimated in the model and that \\(n-(k+1)=29\\). These are the numerator and denominator degrees of freedom.\nYou might think that including more variables results in a strictly better model. This is not true for reasons to be explored later. For now, try including all the variables in the data frame by the shorthand of a dot on the right hand side of the formula.\n\nsummary(lm(mpg ~ ., data=mtcars))\n\n\nCall:\nlm(formula = mpg ~ ., data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4506 -1.6044 -0.1196  1.2193  4.6271 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 12.30337   18.71788   0.657   0.5181  \ncyl         -0.11144    1.04502  -0.107   0.9161  \ndisp         0.01334    0.01786   0.747   0.4635  \nhp          -0.02148    0.02177  -0.987   0.3350  \ndrat         0.78711    1.63537   0.481   0.6353  \nwt          -3.71530    1.89441  -1.961   0.0633 .\nqsec         0.82104    0.73084   1.123   0.2739  \nvs           0.31776    2.10451   0.151   0.8814  \nam           2.52023    2.05665   1.225   0.2340  \ngear         0.65541    1.49326   0.439   0.6652  \ncarb        -0.19942    0.82875  -0.241   0.8122  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.65 on 21 degrees of freedom\nMultiple R-squared:  0.869, Adjusted R-squared:  0.8066 \nF-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07\n\n\nYou might find this output a bit surprising. You know from the \\(F\\)-statistic that at least one of the variables is contributing significantly to the model but individually, the contributions seem minimal based on the small \\(t\\)-statistics. The model is only a bit better, explaining 86 percent of the variability in the data, and the adjusted \\(R^2\\) value hasn’t improved at all, suggesting that you may have too many variables.\nAt this stage, you would probably remove some variables, perhaps by trial and error. How would you do this? You could start by running linear models over and over again. For example, you could construct one linear model for each variable and see which one has the largest contribution. Then you could try adding a second variable from among the remaining variables, and do that with each remaining variable, until you find one that adds the Largest contribution. You could continue in this way until you’ve accounted for all the variables, but would take forever to do. Luckily, R has functions to assist with this process and run regressions for you over and over again. I’m going to demonstrate one of them now for which we have to add the leaps package. I should point out that this involves doing some machine learning which is not strictly in the scope of this class, but will save you a lot of time.\n\nlibrary(caret)\nlibrary(leaps)\nset.seed(123)\ntrain.control &lt;- trainControl(method = \"cv\", number = 10)\nm &lt;- train(mpg ~ ., data = mtcars,\n                    method = \"leapBackward\",\n                    tuneGrid = data.frame(nvmax = 1:10),\n                    trControl = train.control\n                    )\nm$results\n\n   nvmax     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1      1 3.528852 0.8077208 3.015705 1.765926  0.2320177 1.529370\n2      2 3.104015 0.8306301 2.507496 1.355870  0.2108183 0.884356\n3      3 3.211552 0.8255871 2.700867 1.359334  0.2077318 1.033360\n4      4 3.148479 0.8296845 2.630645 1.354017  0.1908016 1.074414\n5      5 3.254928 0.8164973 2.737739 1.266874  0.2309531 1.044970\n6      6 3.259540 0.8212797 2.749594 1.227337  0.2493678 1.043727\n7      7 3.322310 0.8570599 2.787698 1.408879  0.1592820 1.153164\n8      8 3.297613 0.8666992 2.744000 1.364396  0.1529011 1.114000\n9      9 3.330123 0.8632282 2.751539 1.385199  0.1600841 1.111120\n10    10 3.286242 0.8588116 2.715828 1.362054  0.1739366 1.120088\n\nm$bestTune[,1]\n\n[1] 2\n\nsummary(m$finalModel)\n\nSubset selection object\n10 Variables  (and intercept)\n     Forced in Forced out\ncyl      FALSE      FALSE\ndisp     FALSE      FALSE\nhp       FALSE      FALSE\ndrat     FALSE      FALSE\nwt       FALSE      FALSE\nqsec     FALSE      FALSE\nvs       FALSE      FALSE\nam       FALSE      FALSE\ngear     FALSE      FALSE\ncarb     FALSE      FALSE\n1 subsets of each size up to 2\nSelection Algorithm: backward\n         cyl disp hp  drat wt  qsec vs  am  gear carb\n1  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n2  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \"*\"  \" \" \" \" \" \"  \" \" \n\ncoef(m$finalModel,m$bestTune[,1])\n\n(Intercept)          wt        qsec \n  19.746223   -5.047982    0.929198 \n\nsummary(lm(mpg~wt+qsec,data=mtcars))\n\n\nCall:\nlm(formula = mpg ~ wt + qsec, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3962 -2.1431 -0.2129  1.4915  5.7486 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  19.7462     5.2521   3.760 0.000765 ***\nwt           -5.0480     0.4840 -10.430 2.52e-11 ***\nqsec          0.9292     0.2650   3.506 0.001500 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.596 on 29 degrees of freedom\nMultiple R-squared:  0.8264,    Adjusted R-squared:  0.8144 \nF-statistic: 69.03 on 2 and 29 DF,  p-value: 9.395e-12\n\n\nThe preceding code uses a process of backward selection of models and arrives at a best model with two variables. Backward selection starts with all the variables and gradually removes the worst one at each iteration.\nThe following code uses a process of sequential selection, which combines both forward and backward. It takes longer to run, but can result in a better model. In this case, it chooses four variables.\n\nm &lt;- train(mpg ~ ., data = mtcars,\n                    method = \"leapSeq\",\n                    tuneGrid = data.frame(nvmax = 1:10),\n                    trControl = train.control\n                    )\nm$results\n\n   nvmax     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD\n1      1 3.338459 0.9001241 2.890096 1.0951033  0.1013687 0.9516300\n2      2 3.189923 0.8859776 2.582903 0.6838624  0.1267838 0.4331819\n3      3 2.941144 0.8620702 2.488212 0.9202376  0.1449517 0.6399695\n4      4 2.879207 0.8617366 2.480590 1.0315877  0.1534301 0.8871564\n5      5 3.132200 0.8965810 2.747981 1.0959380  0.1336679 0.9911532\n6      6 3.010670 0.9150866 2.656875 1.0023198  0.1083813 0.8800647\n7      7 2.919346 0.9260098 2.499461 0.8913467  0.1133718 0.7935931\n8      8 2.985337 0.9085432 2.585924 0.9516997  0.1248117 0.8376843\n9      9 3.022897 0.9194308 2.609043 1.1395931  0.1111588 1.0913428\n10    10 3.257194 0.8988626 2.811998 1.3089386  0.1423077 1.2115681\n\nm$bestTune[,1]\n\n[1] 4\n\nsummary(m$finalModel)\n\nSubset selection object\n10 Variables  (and intercept)\n     Forced in Forced out\ncyl      FALSE      FALSE\ndisp     FALSE      FALSE\nhp       FALSE      FALSE\ndrat     FALSE      FALSE\nwt       FALSE      FALSE\nqsec     FALSE      FALSE\nvs       FALSE      FALSE\nam       FALSE      FALSE\ngear     FALSE      FALSE\ncarb     FALSE      FALSE\n1 subsets of each size up to 4\nSelection Algorithm: 'sequential replacement'\n         cyl disp hp  drat wt  qsec vs  am  gear carb\n1  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n2  ( 1 ) \"*\" \"*\"  \" \" \" \"  \" \" \" \"  \" \" \" \" \" \"  \" \" \n3  ( 1 ) \"*\" \" \"  \"*\" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n4  ( 1 ) \" \" \" \"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n\ncoef(m$finalModel,m$bestTune[,1])\n\n(Intercept)          hp          wt        qsec          am \n17.44019110 -0.01764654 -3.23809682  0.81060254  2.92550394 \n\nsummary(lm(mpg~hp+wt+qsec+am,data=mtcars))\n\n\nCall:\nlm(formula = mpg ~ hp + wt + qsec + am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4975 -1.5902 -0.1122  1.1795  4.5404 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 17.44019    9.31887   1.871  0.07215 . \nhp          -0.01765    0.01415  -1.247  0.22309   \nwt          -3.23810    0.88990  -3.639  0.00114 **\nqsec         0.81060    0.43887   1.847  0.07573 . \nam           2.92550    1.39715   2.094  0.04579 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.435 on 27 degrees of freedom\nMultiple R-squared:  0.8579,    Adjusted R-squared:  0.8368 \nF-statistic: 40.74 on 4 and 27 DF,  p-value: 4.589e-11\n\n\nWhich model is better? The latter model has the best adjusted \\(R^2\\) value. But it also has what appears to be a spurious variable, hp. It could be that hp is contributing indirectly, by being collinear with one of the other variables. Should we take it out and try again or should we accept the two variable model? That depends on several factors.\nThere is a principle called Occam’s Razor, named after William of Occam (who didn’t invent it, by the way—things often get named after popularizers rather than inventors). The principle states that, if two explanations have the same explanatory power, you should accept the simpler one. In this context, simpler means fewer variables. The tricky part is what is meant by the same explanatory power. Here we have a comparison of 0.8368 adjusted \\(R^2\\) vs 0.8144. Are those close enough to be considered the same? It depends on the context. If you’re a car buyer I would say yes but if you’re a car manufacturer I would say no. Your opinion might differ. It’s easy to teach the mechanics of these methods (even if you don’t think so yet!) but much harder to come up with the insights to interpret them. (Actually, I would probably choose the three variable model of wt, qsec, and am, but you can test that for yourself.)"
  },
  {
    "objectID": "week09.html#feature-selection-with-qualitative-variables",
    "href": "week09.html#feature-selection-with-qualitative-variables",
    "title": "9  Introduction to Linear Regression",
    "section": "9.5 Feature selection with qualitative variables",
    "text": "9.5 Feature selection with qualitative variables\nUnfortunately, the leaps package is not much help with qualitative variables. Another approach is to use the olsrr package as follows.\n\nlibrary(olsrr)\nm &lt;- lm(price ~ carat + cut + color + clarity, data = diamonds)\nk &lt;- ols_step_best_subset(m)\nk\n\n       Best Subsets Regression        \n--------------------------------------\nModel Index    Predictors\n--------------------------------------\n     1         carat                   \n     2         carat clarity           \n     3         carat color clarity     \n     4         carat cut color clarity \n--------------------------------------\n\n                                                                  Subsets Regression Summary                                                                  \n--------------------------------------------------------------------------------------------------------------------------------------------------------------\n                       Adj.        Pred                                                                                                                        \nModel    R-Square    R-Square    R-Square       C(p)           AIC           SBIC            SBC              MSEP               FPE           HSP       APC  \n--------------------------------------------------------------------------------------------------------------------------------------------------------------\n  1        0.8493      0.8493      0.8493    42712.8488    945466.5323    792389.2852    945493.2192    129350491485.6166    2398132.8805    44.4601    0.1507 \n  2        0.8949      0.8948      0.8948    13512.4373    926075.5885    772987.2178    926164.5448     90268965134.5993    1673786.2236    31.0311    0.1052 \n  3        0.9140      0.9139      0.9139     1258.6557    915270.4554    762173.1841    915412.7855     73867441333.4421    1369818.0969    25.3957    0.0861 \n  4        0.9159      0.9159      0.9159       -9.0000    914023.0749    760919.9895    914200.9875     72169466027.5522    1338429.6533    24.8138    0.0841 \n--------------------------------------------------------------------------------------------------------------------------------------------------------------\nAIC: Akaike Information Criteria \n SBIC: Sawa's Bayesian Information Criteria \n SBC: Schwarz Bayesian Criteria \n MSEP: Estimated error of prediction, assuming multivariate normality \n FPE: Final Prediction Error \n HSP: Hocking's Sp \n APC: Amemiya Prediction Criteria \n\nplot(k)"
  },
  {
    "objectID": "week09.html#note-on-ordered-factors",
    "href": "week09.html#note-on-ordered-factors",
    "title": "9  Introduction to Linear Regression",
    "section": "9.6 Note on ordered factors",
    "text": "9.6 Note on ordered factors\nWhen you conduct a linear regression, you are generally looking for the linear effects of input variables on output variables. For example, consider the diamonds data frame that is automatically installed with ggplot. The price output variable may be influenced by a number of input variables, such as cut, carat, and color. It happens that R interprets two of these as ordered factors and does something interesting with them. Let’s conduct a regression of price on cut.\n\nsummary(lm(price~cut,data=diamonds))\n\n\nCall:\nlm(formula = price ~ cut, data = diamonds)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4258  -2741  -1494   1360  15348 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4062.24      25.40 159.923  &lt; 2e-16 ***\ncut.L        -362.73      68.04  -5.331  9.8e-08 ***\ncut.Q        -225.58      60.65  -3.719    2e-04 ***\ncut.C        -699.50      52.78 -13.253  &lt; 2e-16 ***\ncut^4        -280.36      42.56  -6.588  4.5e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3964 on 53935 degrees of freedom\nMultiple R-squared:  0.01286,   Adjusted R-squared:  0.01279 \nF-statistic: 175.7 on 4 and 53935 DF,  p-value: &lt; 2.2e-16\n\n\nNote that the output includes four independent variables instead of just cut. The first one, cut.L, represents cut. The L stands for linear and estimates the linear effect of cut on price. The other three are completely independent of the linear effect of cut on price. The next, cut.Q, estimates the quadratic effect of cut on price. The third, cut.C, estimates the cubic effect of cut on price. The fourth, cut^4 estimates the effect of a fourth degree polynomial contrast of cut on price. Together, all four of these terms are orthogonal polynomial contrasts. They are chosen by R to be independent of each other since a mixture could reveal spurious effects. Why did R stop at four such contrasts? Let’s examine cut further.\n\ntable(diamonds$cut)\n\n\n     Fair      Good Very Good   Premium     Ideal \n     1610      4906     12082     13791     21551 \n\n\nYou can see that cut has five levels. R automatically chooses \\(\\text{level}-1\\) polynomial contrasts when presented with an ordered factor. How can you make R stop doing this if you don’t care about the nonlinear effects? You can present the factor as unordered without altering the factor as it is stored. Then your regression output will list one term for each level of the factor, estimating the effect of that level on the output variable.\n\ndiamonds$cut &lt;- factor(diamonds$cut,ordered=FALSE)\nsummary(lm(price~cut,data=diamonds))\n\n\nCall:\nlm(formula = price ~ cut, data = diamonds)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4258  -2741  -1494   1360  15348 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4358.76      98.79  44.122  &lt; 2e-16 ***\ncutGood       -429.89     113.85  -3.776 0.000160 ***\ncutVery Good  -377.00     105.16  -3.585 0.000338 ***\ncutPremium     225.50     104.40   2.160 0.030772 *  \ncutIdeal      -901.22     102.41  -8.800  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3964 on 53935 degrees of freedom\nMultiple R-squared:  0.01286,   Adjusted R-squared:  0.01279 \nF-statistic: 175.7 on 4 and 53935 DF,  p-value: &lt; 2.2e-16\n\n\nBy the way, you can also say it in one line as:\n\nsummary(lm(price~factor(cut,ordered=F),data=diamonds))\n\n\nCall:\nlm(formula = price ~ factor(cut, ordered = F), data = diamonds)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4258  -2741  -1494   1360  15348 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                        4358.76      98.79  44.122  &lt; 2e-16 ***\nfactor(cut, ordered = F)Good       -429.89     113.85  -3.776 0.000160 ***\nfactor(cut, ordered = F)Very Good  -377.00     105.16  -3.585 0.000338 ***\nfactor(cut, ordered = F)Premium     225.50     104.40   2.160 0.030772 *  \nfactor(cut, ordered = F)Ideal      -901.22     102.41  -8.800  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3964 on 53935 degrees of freedom\nMultiple R-squared:  0.01286,   Adjusted R-squared:  0.01279 \nF-statistic: 175.7 on 4 and 53935 DF,  p-value: &lt; 2.2e-16\n\n\nNotice that generating the orthogonal polynomial contrasts does not alter the linear model in any way. It’s just extra information. Both models produce the same \\(R^2\\) and the same \\(F\\)-statistic and the same \\(p\\)-value of the \\(F\\)-statistic.\nIt is VERY important to note that there is an assumption in the generation of these orthogonal polynomial contrasts. They assume that the differences between the levels of the ordered factor are evenly spaced. If the levels are not evenly spaced, the information provided will be misleading. Take for instance the world’s top three female sprinters. I read an article claiming that the difference between the top two (Richardson and Jackson at the time of this writing) was much smaller than the difference between the second and third. There are many statistical tools that use ranks, such as 1, 2, and 3, as ordered factors. Here is a case where that would be misleading.\nHow can you use this information? In a basic course like this, the information is not particularly useful and we will not pursue it. If you go on in statistics or data science, you will soon encounter situations where nonlinear effects matter a great deal. For example, this often arises in age-period-cohort analysis, where you want to separate the effects of people’s ages, usually divided into evenly spaced levels, and other numerical factors such as income, also usually divided into evenly spaced levels, and the effects of significant periods, such as the economic collapse of 2008 or the pandemic beginning in 2019, and finally the effects of being a member of a cohort or identifiable group. This kind of analysis is often conducted by epidemiologists, people with very large groups of customers, policymakers, and others concerned with large groups of people, or any “objects” with large numbers of attributes.\n\n\n\n\nDiez, David, Mine Çetinkaya-Rundel, and Cristopher D Barr. 2019. OpenIntro Statistics, Fourth Edition. self-published. https://openintro.org/os."
  },
  {
    "objectID": "week10.html#recap-week-9-linear-regression",
    "href": "week10.html#recap-week-9-linear-regression",
    "title": "10  More Linear Regression; Logistic Regression",
    "section": "10.1 Recap week 9: Linear Regression",
    "text": "10.1 Recap week 9: Linear Regression\n\nTextbook section 8.2 Linear Regression\nTextbook section 8.4 Inference for Linear Regression\nTextbook section 9.1 Multiple Regression"
  },
  {
    "objectID": "week10.html#more-on-multiple-regression",
    "href": "week10.html#more-on-multiple-regression",
    "title": "10  More Linear Regression; Logistic Regression",
    "section": "10.2 More on Multiple regression",
    "text": "10.2 More on Multiple regression\nThe OpenIntro Stats book gives an example of multiple regression with the mariokart data frame from their website. This involves the sale of 143 copies of the game Mario Kart for the Wii platform on eBay. They first predict the price based on most of the variables, like so.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/mariokart.rda\"))\nm&lt;-(lm(total_pr~cond+stock_photo+duration+wheels,data=mariokart))\nsummary(m)\n\n\nCall:\nlm(formula = total_pr ~ cond + stock_photo + duration + wheels, \n    data = mariokart)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.485  -6.511  -2.530   1.836 263.025 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     43.5201     8.3701   5.199 7.05e-07 ***\ncondused        -2.5816     5.2272  -0.494 0.622183    \nstock_photoyes  -6.7542     5.1729  -1.306 0.193836    \nduration         0.3788     0.9388   0.403 0.687206    \nwheels           9.9476     2.7184   3.659 0.000359 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.4 on 138 degrees of freedom\nMultiple R-squared:  0.1235,    Adjusted R-squared:  0.09808 \nF-statistic:  4.86 on 4 and 138 DF,  p-value: 0.001069\n\nplot(m)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are four diagnostic plots in the above output. Each one gives us information about the quality of the model.\n\n10.2.1 Residuals vs Fitted\nThis plot tells you the magnitude of the difference between the residuals and the fitted values. There are three things to watch for here. First, are there any drastic outliers? Yes, there are two, points 65 and 20. (Those are row numbers in the data frame.) You need to investigate those and decide whether to omit them from further analysis. Were they typos? Mismeasurements? Or is the process from which they derive intrinsically subject to occasional extreme variation. In the third case, you probably don’t want to omit them.\nSecond, is the solid red line near the dashed zero line? Yes it is, indicating that the residuals have a mean of approximately zero. (The red line shows the mean of the residuals in the immediate region of the \\(x\\)-values of the observed data.)\nThird, is there a pattern to the residuals? No, there is not. The residuals appear to be of the same general magnitude at one end as the other. The things that would need action would be a curve or multiple curves, or a widening or narrowing shape, like the cross section of a horn.\n\n\n10.2.2 Normal Q-Q\nThis is an important plot. I see many students erroneously claiming that residuals are normally distributed because they have a vague bell shape. That is not good enough to detect normality. The Q-Q plot is the standard way to detect normality. If the points lie along the dashed line, you can be reasonably safe in an assumption of normality. If they deviate from the dashed line, the residuals are probably not normally distributed.\n\n\n10.2.3 Scale-Location\nLook for two things here. First, the red line should be approximately horizontal, meaning that there is not much variability in the standardized residuals. Second, look at the spread of the points around the red line. If they don’t show a pattrn, this reinforces the assumption of homoscedasticity that we already found evidence for in the first plot.\n\n\n10.2.4 Residuals vs Leverage\nThis shows you influential points that you may want to remove. Point 84 has high leverage (potential for influence) but is probably not actually very influential because it is so far from Cook’s Distance. Points 20 and 65 are outliers but only point 20 is more than Cook’s Distance away from the mean. In this case, you would likely remove point 20 from consideration unless there were a mitigating reason. For example, game collectors often pay extra for a game that has unusual attributes, such as shrink-wrapped original edition. As an example of a point you would definitely remove, draw a horizontal line from point 20 to a vertical line from point 84. Where they meet would be a high-leverage outlier that is unduly affecting the model no matter what it’s underlying cause. On the other hand, what if you have many such points? Unfortunately, that probably means the model isn’t very good.\n\n\n10.2.5 Removing offending observations\nSuppose we want to get rid of points 20 and 65 and rerun the regression. We could either do this using plain R or the tidyverse. I prefer the tidyverse method because of clarity of exposition.\n\ndf &lt;- mariokart |&gt;\n  filter(!row_number() %in% c(20, 65))\nm&lt;-(lm(total_pr~cond+stock_photo+duration+wheels,data=df))\nsummary(m)\n\n\nCall:\nlm(formula = total_pr ~ cond + stock_photo + duration + wheels, \n    data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.3788  -2.9854  -0.9654   2.6915  14.0346 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    41.34153    1.71167  24.153  &lt; 2e-16 ***\ncondused       -5.13056    1.05112  -4.881 2.91e-06 ***\nstock_photoyes  1.08031    1.05682   1.022    0.308    \nduration       -0.02681    0.19041  -0.141    0.888    \nwheels          7.28518    0.55469  13.134  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.901 on 136 degrees of freedom\nMultiple R-squared:  0.719, Adjusted R-squared:  0.7108 \nF-statistic: 87.01 on 4 and 136 DF,  p-value: &lt; 2.2e-16\n\nplot(m)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat a difference this makes in the output and the statistics and plots about the output! Keep in mind, though, that I just did this as an example. Points 20 and 65 may be totally legitimate in this case. Also, note that you could use plain R without the tidyverse to eliminate those rows by saying something like df &lt;- mariokart[-c(20,65),]. The bracket notation assumes anything before the comma refers to a row and anything after a comma refers to a column. In this case, I didn’t say anything about the columns, so the square brackets just have a dangling comma in them. The important point is that one method or another may seem more natural to you. For most students, the tidyverse approach is probably more natural, so I highlight that."
  },
  {
    "objectID": "week10.html#regression-gone-wrong",
    "href": "week10.html#regression-gone-wrong",
    "title": "10  More Linear Regression; Logistic Regression",
    "section": "10.3 Regression gone wrong",
    "text": "10.3 Regression gone wrong\nThere is a dataframe in both the MASS and ISLR2 called Boston. It illustrates the problem of systemic racism and how that can affect statistical models such as we are generating in logistic regression. It is instructive to look at this dataframe rather than ignore it so we can learn how such things happen and, perhaps, how to guard against them.\nThe Boston dataframe is popular in textbooks and statistics classes. I’ve used it myself without thinking too much about it. It has a column called black which I naively assumed could be used to illustrate racism in Boston, a city notorious for segregation of the black population into a ghetto.\nI was about to use it in this class when a student asked if the black column was removed in the ISLR2 package because of racism. I didn’t know. I looked at both copies, the one in MASS and the one in ISLR2 (a more recent package) and discovered that, indeed, the black column had been removed in the more recent package. Why? I searched for articles about the Boston dataframe and found two alarming articles analyzing it, one of which pointed out that some prominent statisticians had deleted it from their Python package after learning of the problem.\nFar from being used to illustrate racism, this dataframe may have actually been perpetuating racist activity. The original purpose of the dataframe is innocuous. It is meant to illustrate the relationship of air pollution to housing prices, which seems like a laudable goal. There is evidence, though, that in assembling the dataframe, the creators made some racist assumptions that have had a lasting negative effect. What we know about assembling the dataframe is incomplete, based on incomplete documentation and attempted reconstruction by M. Carlisle, a self-described Mathematics PhD posting on Medium. The following information comes from Carlisle’s post, although I am responsible for any errors of interpretation.\nIt appears that the creators of the dataframe committed an error you can easily avoid, thanks in part to contemporary tools that were not available in 1975. They took some data from the US Census, but did not record it directly. Instead, they subjected it to a non-invertible transformation and recorded the result. That means that we can’t be sure of the original data and we are stuck, in a sense, with their interpretation of that data. Critically, that interpretation encoded two racist assumptions that should have been testable to root out racism rather than encoded to perpetuate it. The transformation is \\(v=1000(B-0.63)^2\\), where \\(B\\) is the proportion of the Black population by town. By non-invertible it is meant that \\(B\\) can’t be derived from \\(v\\) except in some cases because squaring makes the number positive regardless of whether \\(B-0.63\\) is positive or negative. This is a very basic error you should not emulate. Luckily, tools like Quarto make it possible to document all your transformations so you can use the raw data and someone else can substitute a different transformation. This is a key point to remember.\nWhat about the equation itself? What social value is it encoding? Keep in mind that \\(v\\) is the contribution of “Blackness” to the median value of homes in a town. The shape of the function is a parabola as follows.\n\ncurve(1000*(x-0.63)^2,from=0,to=1,xlab=\"blackness\",y=\"value\")\n\n\n\n\nIt appears that the value is in squared thousands of dollars, meaning that the median value of a totally segregated white neighborhood is about 19.9223493K USD higher than that of a neighborhood with 63 percent black population (where the parabola bottoms out). In a completely black neighborhood, the median value of a home is about 11.7004273K USD more than in the neighborhood with 63 percent black population. In other words, the equation is telling current homeowners that the value of their home will be decreased by integration and that they are financially better off under segregation.\nWhat if this dataframe is used for inferring the appropriate sale price of new homes? This is really pernicious. It helps to perpetuate segregation by reinforcing the parabola. The fact that that this has become a textbook dataframe practically ensures that someone will use it to set prices or to make offers.\nWhen M. Carlisle set out to find out the original Census numbers used, it turned out to be impossible, presumably because of sloppiness in the original data collection. As you may guess, there are only two possibilities for \\(v\\) for every instance of \\(B\\), so some matching should be possible. But it wasn’t possible for every town! The numbers from the analysis didn’t match any numbers from the 1970 census in several cases.\nThere is another glaring problem with this dataframe, mentioned in the other article I found, at FairLearn. That article includes the following paragraph:\nThe definition of the LSTAT variable is also suspect. Harrison and Rubenfield define lower status as a function of the proportion of adults without some high school education and the proportion of male workers classified as laborers. They apply a logarithmic transformation to the variable with the assumption that resulting variable distribution reflects their understanding of socioeconomic distinctions. However, the categorization of a certain level of education and job category as indicative of “lower status” is reflective of social constructs of class and not objective fact. Again, the authors provide no evidence of a proposed relationship between LSTAT and MEDV and do not sufficiently justify its inclusion in the hedonic pricing model.\nThe lstat column is problematic for at least three reasons: (1) it represents a hypothesis with no empirical support, (2) it exhibits intersectionality with the black column because of the prevalance of black males in both categories, and (3) it is a jumble of three things (education, job classification, and logarithmic transformation) meant to reinforce a social conception of “class”. Contemporary toolkits like Quarto can partially ameliorate problems like (3) by keeping an audit trail from raw data through model building. But (1) requires you to drop stereotypes rather than turn them into unsupported hypotheses and (2) challenges you to more deeply understand data to spot potential for malfeasance.\nWhat about other ethical problems with this dataframe? The FairLearn article goes into more depth than I have here and unpacks the problems into different named constructs with precise definitions. It is well worth reading. It also references the discussion over what to do with this dataframe on GitHub. You can see evidence of what some people chose to do by comparing the MASS and ISLR2 versions, both of which are mistaken in their disposition in my opinion. Ultimately, the FairLearn article recommends abandoning this dataframe for predictive analysis exercises, so I have done that in our weekly exercises. Special thanks are owed to the student who got me searching for the above information!"
  },
  {
    "objectID": "week10.html#logistic-regression",
    "href": "week10.html#logistic-regression",
    "title": "10  More Linear Regression; Logistic Regression",
    "section": "10.4 Logistic Regression",
    "text": "10.4 Logistic Regression\nLogistic regression is a kind of classification rather than regression. The book doesn’t make this point, but most textbooks do. You can divide machine learning problems into problems of regression and problems of classification. In regression, the \\(y\\) variable is more or less continuous, whereas in the classification problem, \\(y\\) is a set of categories, ordered or not. The word logistic comes from the logistic function, which is illustrated below. This interesting function takes an input from \\(-\\infty\\) to \\(+\\infty\\) and gives an output between zero and one. It can be used to reduce wildly varying inputs into a yes / no decision. It is also known as the sigmoid function.\n\n\n\n\n\nNote that zero and one happen to be the boundaries of a probability measure. Hence, you can use the logistic function to reduce arbitrary numbers to a probability.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/resume.rda\"))\nnames(resume)\n\n [1] \"job_ad_id\"              \"job_city\"               \"job_industry\"          \n [4] \"job_type\"               \"job_fed_contractor\"     \"job_equal_opp_employer\"\n [7] \"job_ownership\"          \"job_req_any\"            \"job_req_communication\" \n[10] \"job_req_education\"      \"job_req_min_experience\" \"job_req_computer\"      \n[13] \"job_req_organization\"   \"job_req_school\"         \"received_callback\"     \n[16] \"firstname\"              \"race\"                   \"gender\"                \n[19] \"years_college\"          \"college_degree\"         \"honors\"                \n[22] \"worked_during_school\"   \"years_experience\"       \"computer_skills\"       \n[25] \"special_skills\"         \"volunteer\"              \"military\"              \n[28] \"employment_holes\"       \"has_email_address\"      \"resume_quality\"        \n\nwith(resume,table(race,received_callback))\n\n       received_callback\nrace       0    1\n  black 2278  157\n  white 2200  235\n\nwith(resume,table(gender,received_callback))\n\n      received_callback\ngender    0    1\n     f 3437  309\n     m 1041   83\n\nwith(resume,table(honors,received_callback))\n\n      received_callback\nhonors    0    1\n     0 4263  350\n     1  215   42\n\nsummary(glm(received_callback ~ honors,data=resume,family=\"binomial\"))\n\n\nCall:\nglm(formula = received_callback ~ honors, family = \"binomial\", \n    data = resume)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -2.4998     0.0556  -44.96  &lt; 2e-16 ***\nhonors        0.8668     0.1776    4.88 1.06e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2726.9  on 4869  degrees of freedom\nResidual deviance: 2706.7  on 4868  degrees of freedom\nAIC: 2710.7\n\nNumber of Fisher Scoring iterations: 5\n\nsummary(glm(received_callback ~ race,data=resume,family=\"binomial\"))\n\n\nCall:\nglm(formula = received_callback ~ race, family = \"binomial\", \n    data = resume)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.67481    0.08251 -32.417  &lt; 2e-16 ***\nracewhite    0.43818    0.10732   4.083 4.45e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2726.9  on 4869  degrees of freedom\nResidual deviance: 2709.9  on 4868  degrees of freedom\nAIC: 2713.9\n\nNumber of Fisher Scoring iterations: 5\n\nsummary(glm(received_callback ~ gender,data=resume,family=\"binomial\"))\n\n\nCall:\nglm(formula = received_callback ~ gender, family = \"binomial\", \n    data = resume)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.40901    0.05939 -40.562   &lt;2e-16 ***\ngenderm     -0.12008    0.12859  -0.934     0.35    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2726.9  on 4869  degrees of freedom\nResidual deviance: 2726.0  on 4868  degrees of freedom\nAIC: 2730\n\nNumber of Fisher Scoring iterations: 5\n\n\nOne easy way to compare these models is by comparing the values of AIC, the Akaike Information Criterion. This measures the loss of information in each model and the model with the lowest value of AIC has lost the least. In comparing the AIC values, it is typical to calculate \\(\\exp((\\text{AIC}_{\\text{min}}-\\text{AIC}_{\\text{alternative}})/2)\\). This value, the relative likelihood, is the likelihood that the alternative model minimizes the information loss. Keep in mind that the AIC is only a tool to compare models, not an absolute measure. There is no such thing as an absolutely good AIC value. In the above cases, the relative likelihood for the two models with higher AIC values is vanishingly small. The model using honors loses far less information than do the other two.\nAnother approach is to calculate AICc, Delta_AICc, and AICcWt using R.\n\nm1 &lt;- glm(received_callback ~ honors,data=resume,family=\"binomial\")\nm2 &lt;- glm(received_callback ~ race,data=resume,family=\"binomial\")\nm3 &lt;- glm(received_callback ~ gender,data=resume,family=\"binomial\")\nlibrary(AICcmodavg)\nmodels &lt;- list(m1,m2,m3)\nmodel_names &lt;- c(\"honors\", \"race\", \"gender\")\naictab(cand.set = models, modnames = model_names)\n\n\nModel selection based on AICc:\n\n       K    AICc Delta_AICc AICcWt Cum.Wt       LL\nhonors 2 2710.73       0.00   0.83   0.83 -1353.36\nrace   2 2713.94       3.21   0.17   1.00 -1354.97\ngender 2 2730.03      19.31   0.00   1.00 -1363.02\n\n\nA common rule, discredited by Anderson (2008) is that, if Delta_AICc is greater than 2, the model should be discarded. But it is not the case that any model can be judged as good or bad based on AIC. Instead, the AIC tells us only relative information about the models: which is better from an information loss standpoint. The AICcWt or Akaike weight, is the probability that the given model is the best of the models in the table from an information loss standpoint. Here there is a clear winner. The difference between 83 percent and seventeen percent is striking.\n\n10.4.1 tidymodels approach\nDatacamp shows a different way, using tidymodels in one of their tutorials. In this example, the bank wants to divide customers into those likely to buy and those unlikely to buy some banking product. They would like to divide the customers into these two groups using logistic regression, with a cutoff point of fifty-fifty. If there’s better than a fifty-fifty chance, they will send a salesperson but if there’s less than a fifty-fifty chance, they won’t send a salesperson.\n\nlibrary(tidymodels)\n\n#. Read the dataset and convert the target variable to a factor\nbank_df &lt;- read_csv2(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/bank-full.csv\"))\nbank_df$y = as.factor(bank_df$y)\n\n#. Plot job occupation against the target variable\nggplot(bank_df, aes(job, fill = y)) +\n    geom_bar() +\n    coord_flip()\n\n\n\n\nA crucial concept you’ll learn if you take a more advanced class, say 310D, is the notion of dividing data into two data frames, a training frame and a test frame. This is the conventional way to test machine learning models, of which logistic regression is one. You train the model on one set of data, then test it on another, previously unseen set. That’s the next thing done in this example.\n\n#. Split data into train and test\nset.seed(421)\nsplit &lt;- initial_split(bank_df, prop = 0.8, strata = y)\ntrain &lt;- split |&gt; \n         training()\ntest &lt;- split |&gt;\n        testing()\n#. Train a logistic regression model\nm &lt;- logistic_reg(mixture = double(1), penalty = double(1)) |&gt;\n  set_engine(\"glmnet\") |&gt;\n  set_mode(\"classification\") |&gt;\n  fit(y ~ ., data = train)\n\n#. Model summary\ntidy(m)\n\n# A tibble: 43 × 3\n   term              estimate penalty\n   &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)      -2.59           0\n 2 age              -0.000477       0\n 3 jobblue-collar   -0.183          0\n 4 jobentrepreneur  -0.206          0\n 5 jobhousemaid     -0.270          0\n 6 jobmanagement    -0.0190         0\n 7 jobretired        0.360          0\n 8 jobself-employed -0.101          0\n 9 jobservices      -0.105          0\n10 jobstudent        0.415          0\n# ℹ 33 more rows\n\n#. Class Predictions\npred_class &lt;- predict(m,\n                      new_data = test,\n                      type = \"class\")\n\n#. Class Probabilities\npred_proba &lt;- predict(m,\n                      new_data = test,\n                      type = \"prob\")\nresults &lt;- test |&gt;\n           select(y) |&gt;\n           bind_cols(pred_class, pred_proba)\n\naccuracy(results, truth = y, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.902\n\n\n\n\n10.4.2 Hyperparameter tuning\nThere are aspects of this approach, called hyperparameters, that influence the quality of the model. It can be tedious to adjust these aspects, called penalty and mixture, so here’s a technique for doing it automatically. You’ll learn about this and similar techniques if you take a more advanced course like 310D, Intro to Data Science.\n\n#. Define the logistic regression model with penalty and mixture hyperparameters\nlog_reg &lt;- logistic_reg(mixture = tune(), penalty = tune(), engine = \"glmnet\")\n\n#. Define the grid search for the hyperparameters\ngrid &lt;- grid_regular(mixture(), penalty(), levels = c(mixture = 4, penalty = 3))\n\n#. Define the workflow for the model\nlog_reg_wf &lt;- workflow() |&gt;\n  add_model(log_reg) |&gt;\n  add_formula(y ~ .)\n\n#. Define the resampling method for the grid search\nfolds &lt;- vfold_cv(train, v = 5)\n\n#. Tune the hyperparameters using the grid search\nlog_reg_tuned &lt;- tune_grid(\n  log_reg_wf,\n  resamples = folds,\n  grid = grid,\n  control = control_grid(save_pred = TRUE)\n)\n\nselect_best(log_reg_tuned, metric = \"roc_auc\")\n\n# A tibble: 1 × 3\n       penalty mixture .config              \n         &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.0000000001       0 Preprocessor1_Model01\n\n\n\n#. Fit the model using the optimal hyperparameters\nlog_reg_final &lt;- logistic_reg(penalty = 0.0000000001, mixture = 0) |&gt;\n                 set_engine(\"glmnet\") |&gt;\n                 set_mode(\"classification\") |&gt;\n                 fit(y~., data = train)\n\n#. Evaluate the model performance on the testing set\npred_class &lt;- predict(log_reg_final,\n                      new_data = test,\n                      type = \"class\")\nresults &lt;- test |&gt;\n  select(y) |&gt;\n  bind_cols(pred_class, pred_proba)\n\n#. Create confusion matrix\nconf_mat(results, truth = y,\n         estimate = .pred_class)\n\n          Truth\nPrediction   no  yes\n       no  7838  738\n       yes  147  320\n\nprecision(results, truth = y,\n          estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 precision binary         0.914\n\nrecall(results, truth = y,\n          estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 recall  binary         0.982\n\n\n\n\n10.4.3 Evaluation metrics\nFollowing are two tables from James et al. (2021) that you can use to evaluate a classification model.\n\n\nAnother view is provided at Wikipedia in the following picture\n\n\n\n\n\n\ncoeff &lt;- tidy(log_reg_final) |&gt;\n  arrange(desc(abs(estimate))) |&gt;\n  filter(abs(estimate) &gt; 0.5)\ncoeff\n\n# A tibble: 10 × 3\n   term            estimate      penalty\n   &lt;chr&gt;              &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)       -2.59  0.0000000001\n 2 poutcomesuccess    2.08  0.0000000001\n 3 monthmar           1.62  0.0000000001\n 4 monthoct           1.08  0.0000000001\n 5 monthsep           1.03  0.0000000001\n 6 contactunknown    -1.01  0.0000000001\n 7 monthdec           0.861 0.0000000001\n 8 monthjan          -0.820 0.0000000001\n 9 housingyes        -0.550 0.0000000001\n10 monthnov          -0.517 0.0000000001\n\nggplot(coeff, aes(x = term, y = estimate, fill = term)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n\n\nAnderson, David. 2008. Model Based Inference in the Life Sciences: A Primer on Evidence. New York, NY: Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning, 2nd Edition. Springer New York."
  },
  {
    "objectID": "week11.html#recap-week-10-multiple-regression-logistic-regression",
    "href": "week11.html#recap-week-10-multiple-regression-logistic-regression",
    "title": "11  Coping with Time and Joins",
    "section": "11.1 Recap week 10: Multiple Regression; Logistic Regression",
    "text": "11.1 Recap week 10: Multiple Regression; Logistic Regression\n\nMultiple regression: one \\(y\\) and multiple \\(x\\) variables\nLogistic regression: \\(y\\) is a factor and multiple \\(x\\) variables"
  },
  {
    "objectID": "week11.html#milestones",
    "href": "week11.html#milestones",
    "title": "11  Coping with Time and Joins",
    "section": "11.2 Milestones",
    "text": "11.2 Milestones\nMilestone 4 will be graded partly on the diagnostic plots and their explanations and partly on improvements to the rest of the report.\n\n11.2.1 Tips for Milestone 4\n\nDon’t use fread() or data.table any more\nInstead use read_csv (not read.csv)\nLabel the r chunks\nBreak your file up into smaller files to prepare, then assemble\nMake plots look better\n\nInclude a title\nDon’t use scientific notation\ndon’t overprint labels\n\n\n\n\n11.2.2 More tips\n\nDon’t produce long outputs that a manager would be unable to use\n\nExample: a list of 50 states with some statistic about them\n\nDon’t produce barplots where all bars are roughly the same size\nDon’t produce stem-and-leaf plots with so much output that no one can read them\nSort barplots for easier comparison\nIt’s hard to use tables with more than about 30 rows\n\n\n\n11.2.3 Why is this plot unsuccessful?\n\n\n\n11.2.4 Don’t include barplots with all same bars\n\n(just say there was little difference along this dimension)\n\n\n11.2.5 More tips\n\nDon’t include names of group members in sections of the report, only in the header\nPut titles on plots even if you have section titles as well\nUse small alpha values to reduce the impact of overplotting\n\n\n\n11.2.6 Significant overplotting\n\n\n\n11.2.7 Don’t use barplots with two bars for presentations\n\n(There may be exceptions in exploration but not presentation)\n\n\n11.2.8 Don’t use barplots with effectively one bar\n\n(Also don’t include the blank entries!)\n\n\n11.2.9 Unreadable stem-and-leaf plot\n\n\n\n11.2.10 This one has no meaning that I can see"
  },
  {
    "objectID": "week11.html#dates-and-times",
    "href": "week11.html#dates-and-times",
    "title": "11  Coping with Time and Joins",
    "section": "11.3 Dates and times",
    "text": "11.3 Dates and times\nFor the final exam, you will have to create the \\(y\\) variable as a time span. Chapter 18 of Wickham, Çetinkaya-Rundel, and Grolemund (2023) tells you how to do this. Let’s review that chapter.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\ntoday()\n\n[1] \"2023-12-22\"\n\nnow()\n\n[1] \"2023-12-22 16:49:58 CST\"\n\n\nSuppose you have a comma-separated-values (csv) file containing ISO-formatted dates or date-times. It’s automatically recognized.\n\ncsv &lt;- \"\n  date,datetime\n  2022-01-02,2022-01-02 05:12\n\"\nread_csv(csv)\n\n# A tibble: 1 × 2\n  date       datetime           \n  &lt;date&gt;     &lt;dttm&gt;             \n1 2022-01-02 2022-01-02 05:12:00\n\n\nSuppose your input has dates not in standard format. You can do this for an ambiguous format:\n\ncsv &lt;- \"\n  date\n  01/02/15\n\"\n\nread_csv(csv, col_types = cols(date = col_date(\"%m/%d/%y\")))\n\n# A tibble: 1 × 1\n  date      \n  &lt;date&gt;    \n1 2015-01-02\n\nread_csv(csv, col_types = cols(date = col_date(\"%d/%m/%y\")))\n\n# A tibble: 1 × 1\n  date      \n  &lt;date&gt;    \n1 2015-02-01\n\nread_csv(csv, col_types = cols(date = col_date(\"%y/%m/%d\")))\n\n# A tibble: 1 × 1\n  date      \n  &lt;date&gt;    \n1 2001-02-15\n\n\nThe letters after the percent signs are format specifiers. Wickham, Çetinkaya-Rundel, and Grolemund (2023) has a long list of them in Chapter 18.\nAn alternative to the above approach is to use the helpers in the lubridate package, which is part of the tidyverse collection of packages. There are two kinds of helpers. First are the date helpers.\n\nymd(\"2017-01-31\")\n\n[1] \"2017-01-31\"\n\nmdy(\"January 31st, 2017\")\n\n[1] \"2017-01-31\"\n\ndmy(\"31-Jan-2017\")\n\n[1] \"2017-01-31\"\n\n\nSecond are the date-time helpers.\n\nymd_hms(\"2017-01-31 20:11:59\")\n\n[1] \"2017-01-31 20:11:59 UTC\"\n\nmdy_hm(\"01/31/2017 08:01\")\n\n[1] \"2017-01-31 08:01:00 UTC\"\n\n\nThe nycflights13 data frame, which we loaded above, contains information about 336,000 flights originating from the three NYC area airports in 2013. It contains dats and times spread across different columns.\n\nflights |&gt;\n  select(year, month, day, hour, minute)\n\n# A tibble: 336,776 × 5\n    year month   day  hour minute\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  2013     1     1     5     15\n 2  2013     1     1     5     29\n 3  2013     1     1     5     40\n 4  2013     1     1     5     45\n 5  2013     1     1     6      0\n 6  2013     1     1     5     58\n 7  2013     1     1     6      0\n 8  2013     1     1     6      0\n 9  2013     1     1     6      0\n10  2013     1     1     6      0\n# ℹ 336,766 more rows\n\n\nYou can handle this kind of input by using the make_datetime() function.\n\nflights |&gt;\n  select(year, month, day, hour, minute) |&gt;\n  mutate(departure = make_datetime(year, month, day, hour, minute))\n\n# A tibble: 336,776 × 6\n    year month   day  hour minute departure          \n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dttm&gt;             \n 1  2013     1     1     5     15 2013-01-01 05:15:00\n 2  2013     1     1     5     29 2013-01-01 05:29:00\n 3  2013     1     1     5     40 2013-01-01 05:40:00\n 4  2013     1     1     5     45 2013-01-01 05:45:00\n 5  2013     1     1     6      0 2013-01-01 06:00:00\n 6  2013     1     1     5     58 2013-01-01 05:58:00\n 7  2013     1     1     6      0 2013-01-01 06:00:00\n 8  2013     1     1     6      0 2013-01-01 06:00:00\n 9  2013     1     1     6      0 2013-01-01 06:00:00\n10  2013     1     1     6      0 2013-01-01 06:00:00\n# ℹ 336,766 more rows\n\n\nYou can do the same with the other time columns.\n\nmake_datetime_100 &lt;- function(year, month, day, time) {\n  make_datetime(year, month, day, time %/% 100, time %% 100)\n}\n\nflights_dt &lt;- flights |&gt;\n  filter(!is.na(dep_time), !is.na(arr_time)) |&gt;\n  mutate(\n    dep_time = make_datetime_100(year, month, day, dep_time),\n    arr_time = make_datetime_100(year, month, day, arr_time),\n    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),\n    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)\n  ) |&gt;\n  select(origin, dest, ends_with(\"delay\"), ends_with(\"time\"))\n\nflights_dt\n\n# A tibble: 328,063 × 9\n   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n 1 EWR    IAH           2        11 2013-01-01 05:17:00 2013-01-01 05:15:00\n 2 LGA    IAH           4        20 2013-01-01 05:33:00 2013-01-01 05:29:00\n 3 JFK    MIA           2        33 2013-01-01 05:42:00 2013-01-01 05:40:00\n 4 JFK    BQN          -1       -18 2013-01-01 05:44:00 2013-01-01 05:45:00\n 5 LGA    ATL          -6       -25 2013-01-01 05:54:00 2013-01-01 06:00:00\n 6 EWR    ORD          -4        12 2013-01-01 05:54:00 2013-01-01 05:58:00\n 7 EWR    FLL          -5        19 2013-01-01 05:55:00 2013-01-01 06:00:00\n 8 LGA    IAD          -3       -14 2013-01-01 05:57:00 2013-01-01 06:00:00\n 9 JFK    MCO          -3        -8 2013-01-01 05:57:00 2013-01-01 06:00:00\n10 LGA    ORD          -2         8 2013-01-01 05:58:00 2013-01-01 06:00:00\n# ℹ 328,053 more rows\n# ℹ 3 more variables: arr_time &lt;dttm&gt;, sched_arr_time &lt;dttm&gt;, air_time &lt;dbl&gt;\n\n\nHere are the departure times for January 2nd, 2013.\n\nflights_dt |&gt;\n  filter(dep_time &lt; ymd(20130102)) |&gt;\n  ggplot(aes(x = dep_time)) +\n  geom_freqpoly(binwidth = 600) # 600 s = 10 minutes"
  },
  {
    "objectID": "week11.html#time-spans",
    "href": "week11.html#time-spans",
    "title": "11  Coping with Time and Joins",
    "section": "11.4 Time spans",
    "text": "11.4 Time spans\n\nDurations, which represent an exact number of seconds.\nPeriods, which represent human units like weeks and months.\nIntervals, which represent a starting and ending point.\n\n\n11.4.1 Durations\nBase R provides a problematic construct for durations, the difftime object.\n\n#. How old is Hadley?\nh_age &lt;- today() - ymd(\"1979-10-14\")\nh_age\n\nTime difference of 16140 days\n\n\nThe lubridate package provides a construct called duration.\n\nas.duration(h_age)\n\n[1] \"1394496000s (~44.19 years)\"\n\n\nThere are numerous duration constructors.\n\ndseconds(15)\n\n[1] \"15s\"\n\ndminutes(10)\n\n[1] \"600s (~10 minutes)\"\n\ndhours(c(12, 24))\n\n[1] \"43200s (~12 hours)\" \"86400s (~1 days)\"  \n\nddays(0:5)\n\n[1] \"0s\"                \"86400s (~1 days)\"  \"172800s (~2 days)\"\n[4] \"259200s (~3 days)\" \"345600s (~4 days)\" \"432000s (~5 days)\"\n\ndweeks(3)\n\n[1] \"1814400s (~3 weeks)\"\n\ndyears(1)\n\n[1] \"31557600s (~1 years)\"\n\n\nYou can add and multiply durations.\n\n2 * dyears(1)\n\n[1] \"63115200s (~2 years)\"\n\ndyears(1) + dweeks(12) + dhours(15)\n\n[1] \"38869200s (~1.23 years)\"\n\n\nYou can add and subtract durations to and from days.\n\ntomorrow &lt;- today() + ddays(1)\nlast_year &lt;- today() - dyears(1)\n\nProblem! Add one day to this particular date as a duration, but this particular date only has 23 hours because of daylight savings time.\n\none_am &lt;- ymd_hms(\"2026-03-08 01:00:00\", tz = \"America/New_York\")\n\none_am\n\n[1] \"2026-03-08 01:00:00 EST\"\n\none_am + ddays(1)\n\n[1] \"2026-03-09 02:00:00 EDT\""
  },
  {
    "objectID": "week11.html#periods",
    "href": "week11.html#periods",
    "title": "11  Coping with Time and Joins",
    "section": "11.5 Periods",
    "text": "11.5 Periods\nThis construct gets over some problems with durations, which are always exact numbers of seconds and take into account time zones and daylight savings time and leap years.\nPeriods have constructors, too.\n\nhours(c(12, 24))\n\n[1] \"12H 0M 0S\" \"24H 0M 0S\"\n\ndays(7)\n\n[1] \"7d 0H 0M 0S\"\n\nmonths(1:6)\n\n[1] \"1m 0d 0H 0M 0S\" \"2m 0d 0H 0M 0S\" \"3m 0d 0H 0M 0S\" \"4m 0d 0H 0M 0S\"\n[5] \"5m 0d 0H 0M 0S\" \"6m 0d 0H 0M 0S\"\n\n\nYou can add and multiply periods.\n\n10 * (months(6) + days(1))\n\n[1] \"60m 10d 0H 0M 0S\"\n\ndays(50) + hours(25) + minutes(2)\n\n[1] \"50d 25H 2M 0S\"\n\n\nAdd them to dates and get the results you expect in the case of daylight savings time and leap years.\n\n#. A leap year\nymd(\"2024-01-01\") + dyears(1)\n\n[1] \"2024-12-31 06:00:00 UTC\"\n\nymd(\"2024-01-01\") + years(1)\n\n[1] \"2025-01-01\"\n\n#. Daylight Savings Time\none_am + ddays(1)\n\n[1] \"2026-03-09 02:00:00 EDT\"\n\none_am + days(1)\n\n[1] \"2026-03-09 01:00:00 EDT\"\n\n\nPeriods can fix the problem that some planes appear to arrive before they depart.\n\nflights_dt |&gt;\n  filter(arr_time &lt; dep_time)\n\n# A tibble: 10,633 × 9\n   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n 1 EWR    BQN           9        -4 2013-01-01 19:29:00 2013-01-01 19:20:00\n 2 JFK    DFW          59        NA 2013-01-01 19:39:00 2013-01-01 18:40:00\n 3 EWR    TPA          -2         9 2013-01-01 20:58:00 2013-01-01 21:00:00\n 4 EWR    SJU          -6       -12 2013-01-01 21:02:00 2013-01-01 21:08:00\n 5 EWR    SFO          11       -14 2013-01-01 21:08:00 2013-01-01 20:57:00\n 6 LGA    FLL         -10        -2 2013-01-01 21:20:00 2013-01-01 21:30:00\n 7 EWR    MCO          41        43 2013-01-01 21:21:00 2013-01-01 20:40:00\n 8 JFK    LAX          -7       -24 2013-01-01 21:28:00 2013-01-01 21:35:00\n 9 EWR    FLL          49        28 2013-01-01 21:34:00 2013-01-01 20:45:00\n10 EWR    FLL          -9       -14 2013-01-01 21:36:00 2013-01-01 21:45:00\n# ℹ 10,623 more rows\n# ℹ 3 more variables: arr_time &lt;dttm&gt;, sched_arr_time &lt;dttm&gt;, air_time &lt;dbl&gt;\n\n\nThese are overnight flights so fix the problem by adding a day.\n\nflights_dt &lt;- flights_dt |&gt;\n  mutate(\n    overnight = arr_time &lt; dep_time,\n    arr_time = arr_time + days(!overnight),\n    sched_arr_time = sched_arr_time + days(overnight)\n  )\n\n\n11.5.1 Intervals\nIntervals are like durations but with a specific starting point. They get around the problem that, for example, some years are longer than others, so that a year on average is 365.25 days. With an interval you can have a specific year of 365 days or a specific leap year of 366 days.\n\ny2023 &lt;- ymd(\"2023-01-01\") %--% ymd(\"2024-01-01\")\ny2024 &lt;- ymd(\"2024-01-01\") %--% ymd(\"2025-01-01\")\n\ny2023\n\n[1] 2023-01-01 UTC--2024-01-01 UTC\n\ny2024\n\n[1] 2024-01-01 UTC--2025-01-01 UTC\n\ny2023 / days(1)\n\n[1] 365\n\ny2024 / days(1)\n\n[1] 366\n\n\nThe book also provides extensive information about time zones but for the final exam you’ll only have one time zone, so that discussion is not strictly necessary for us."
  },
  {
    "objectID": "week11.html#joins",
    "href": "week11.html#joins",
    "title": "11  Coping with Time and Joins",
    "section": "11.6 Joins",
    "text": "11.6 Joins\nThe nycflights13 package provides five data frames that can be joined together.\n\nWhy would you store data this way? (Think about using the data over a long term and think about maintenance of the data.)\nYou can add the airline names to the flights by a left_join() function. It’s easier to see if you first limit the flights data frame to a few essential columns.\n\nflights2 &lt;- flights |&gt;\n  select(year, time_hour, origin, dest, tailnum, carrier)\nflights2 |&gt; left_join(airlines)\n\n# A tibble: 336,776 × 7\n    year time_hour           origin dest  tailnum carrier name                  \n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                 \n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      United Air Lines Inc. \n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      United Air Lines Inc. \n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      American Airlines Inc.\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      JetBlue Airways       \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Delta Air Lines Inc.  \n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      United Air Lines Inc. \n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      JetBlue Airways       \n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      ExpressJet Airlines I…\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      JetBlue Airways       \n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      American Airlines Inc.\n# ℹ 336,766 more rows\n\n\nThere are several different join functions described in Wickham, Çetinkaya-Rundel, and Grolemund (2023) in Chapter 20. You’ll only need the left join for this week’s exercises, but reading Chapter 20 is still a very good idea.\nYou should also read about sqldf, a package for running SQL statements on R data frames. Following is an example of its use.\n\nlibrary(sqldf)\nsqldf(\"SELECT carrier, COUNT(*)\n         FROM flights\n         GROUP BY carrier\n         ORDER BY 2 DESC;\")\n\n   carrier COUNT(*)\n1       UA    58665\n2       B6    54635\n3       EV    54173\n4       DL    48110\n5       AA    32729\n6       MQ    26397\n7       US    20536\n8       9E    18460\n9       WN    12275\n10      VX     5162\n11      FL     3260\n12      AS      714\n13      F9      685\n14      YV      601\n15      HA      342\n16      OO       32\n\nsqlFlightsWnames &lt;- sqldf(\"SELECT fl.carrier, name\n                             FROM flights fl\n                             LEFT join airlines ai\n                             ON fl.carrier=ai.carrier;\")\nsqldf(\"SELECT name, COUNT(*)\n         FROM sqlFlightsWnames\n         GROUP BY name\n         ORDER BY 2 DESC;\")\n\n                          name COUNT(*)\n1        United Air Lines Inc.    58665\n2              JetBlue Airways    54635\n3     ExpressJet Airlines Inc.    54173\n4         Delta Air Lines Inc.    48110\n5       American Airlines Inc.    32729\n6                    Envoy Air    26397\n7              US Airways Inc.    20536\n8            Endeavor Air Inc.    18460\n9       Southwest Airlines Co.    12275\n10              Virgin America     5162\n11 AirTran Airways Corporation     3260\n12        Alaska Airlines Inc.      714\n13      Frontier Airlines Inc.      685\n14          Mesa Airlines Inc.      601\n15      Hawaiian Airlines Inc.      342\n16       SkyWest Airlines Inc.       32\n\nsort(table(flights$carrier),decreasing=TRUE)\n\n\n   UA    B6    EV    DL    AA    MQ    US    9E    WN    VX    FL    AS    F9 \n58665 54635 54173 48110 32729 26397 20536 18460 12275  5162  3260   714   685 \n   YV    HA    OO \n  601   342    32 \n\nflightsWnames &lt;- flights |&gt; left_join(airlines)\nsort(table(flightsWnames$name),decreasing=TRUE)\n\n\n      United Air Lines Inc.             JetBlue Airways \n                      58665                       54635 \n   ExpressJet Airlines Inc.        Delta Air Lines Inc. \n                      54173                       48110 \n     American Airlines Inc.                   Envoy Air \n                      32729                       26397 \n            US Airways Inc.           Endeavor Air Inc. \n                      20536                       18460 \n     Southwest Airlines Co.              Virgin America \n                      12275                        5162 \nAirTran Airways Corporation        Alaska Airlines Inc. \n                       3260                         714 \n     Frontier Airlines Inc.          Mesa Airlines Inc. \n                        685                         601 \n     Hawaiian Airlines Inc.       SkyWest Airlines Inc. \n                        342                          32 \n\n\nThe only difference between the output of these two approaches is that the native R plus tidyverse version uses more horizontal space in the output because of its use of a variable that records how wide your display is. The SQL version is piped (by default) to SQLite3, which doesn’t know the width of your display and which returns a single column response. You can substitute other database engines for SQLite3, such as PostgreSQL and MySQL. SQLite3 is an extremely fast, tiny database engine which is useful for single-user applications. For example, most smartphone applications (including all Android and iOS) use SQLite3 to store information, making SQLite3 the world’s most popular database (by some measures). SQLite3 is also used by most web browsers to store information.\nAs another example, here is a join between two tables that have synonymous columns with different names. The flights table includes two columns with FAA airport codes: origin and dest, while the airports table contains a single column with FAA airport codes: faa. To join them, we have to tell R the join key. That’s shown below, along with an sqldf version of the same thing.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\nflights |&gt;\n  left_join(airports, join_by(dest == faa)) |&gt;\n  select(year,month,day,dep_time,name) |&gt;\n  head()\n\n# A tibble: 6 × 5\n   year month   day dep_time name                           \n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt; &lt;chr&gt;                          \n1  2013     1     1      517 George Bush Intercontinental   \n2  2013     1     1      533 George Bush Intercontinental   \n3  2013     1     1      542 Miami Intl                     \n4  2013     1     1      544 &lt;NA&gt;                           \n5  2013     1     1      554 Hartsfield Jackson Atlanta Intl\n6  2013     1     1      554 Chicago Ohare Intl             \n\n\nNow the (slightly different) sqldf version.\n\nlibrary(sqldf)\nlibrary(nycflights13)\nresult &lt;- sqldf(\"\n  SELECT f.year, f.month, f.day, f.dep_time, a.name\n  FROM flights f\n  LEFT JOIN airports a ON f.dest = a.faa\n  LIMIT 5\n\")\nresult\n\n  year month day dep_time                            name\n1 2013     1   1      517    George Bush Intercontinental\n2 2013     1   1      533    George Bush Intercontinental\n3 2013     1   1      542                      Miami Intl\n4 2013     1   1      544                            &lt;NA&gt;\n5 2013     1   1      554 Hartsfield Jackson Atlanta Intl\n\n\n\n11.6.1 sqldf keywords\nAccording to a post from R-Pubs on sqldf, the following SQL keywords are available. Note that these may only be used in the context of a SELECT statement.\n\nSELECT: the primary function sqldf refers to. Specifies the columns in the database that the user wants to query\nFROM: specifies which table to select or delete data from\nWHERE: filters results to include only records that satisfy the given condition\nDELETE: deletes rows from a table\nDROP: deletes a column, constraint, database, index, table, or view\nGROUP BY: groups rows that have the same values into summary rows, frequently used with aggregate functions, such as COUNT(), MAX(), MIN(), SUM(), and AVG().\nORDER BY: used to sort the result-set in ascending or descending order, ascending is default so use the DESC keyword to sort in descending order\nCASE: similar to a series of if, else if, and else statements in R. Case statements go through a set of conditions and execute the relevant code once a condition is met.\n\nWHEN: part of a CASE statement, the explicit condition to be met\nTHEN: follows a WHEN statement, the code to execute if the condition is met\nELSE: what to do if no condition is met, if there is no ELSE statement and no condition is met, the series will return NULL\nEND: signifies the end of a CASE statement\n\nJOIN: there are many different methods to join SQL tables together, here are two common examples (note: sqldf does not support right join, the equivalent is to swap the table order and use LEFT JOIN)\n\nINNER JOIN: selects records that have matches in both tables and merges specified together, must specify which column to merge on in each table\nLEFT JOIN: The result of LEFT JOIN shall be the same as the result of INNER JOIN + we’ll have rows, from the “left” table, without a pair in the “right” table"
  },
  {
    "objectID": "week11.html#sqldf-vs-base-r",
    "href": "week11.html#sqldf-vs-base-r",
    "title": "11  Coping with Time and Joins",
    "section": "11.7 sqldf vs base R",
    "text": "11.7 sqldf vs base R\nA person named Massimo Franceschet did an interesting exercise in using sqldf vs base R and I have slightly adapted it below.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\nlibrary(sqldf)\n\n\n11.7.1 add id to flights\n\nflights &lt;- mutate(flights, id = 1:nrow(flights)) |&gt;\n  select(id, everything())\n\n\n\n11.7.2 flights on Christmas\n\nsqldf(\"\n  select *\n  from flights \n  where month = 12 and day = 25\n  limit 5;\n\")\n\n      id year month day dep_time sched_dep_time dep_delay arr_time\n1 105233 2013    12  25      456            500        -4      649\n2 105234 2013    12  25      524            515         9      805\n3 105235 2013    12  25      542            540         2      832\n4 105236 2013    12  25      546            550        -4     1022\n5 105237 2013    12  25      556            600        -4      730\n  sched_arr_time arr_delay carrier flight tailnum origin dest air_time distance\n1            651        -2      US   1895  N156UW    EWR  CLT       98      529\n2            814        -9      UA   1016  N32404    EWR  IAH      203     1400\n3            850       -18      AA   2243  N5EBAA    JFK  MIA      146     1089\n4           1027        -5      B6    939  N665JB    JFK  BQN      191     1576\n5            745       -15      AA    301  N3JLAA    LGA  ORD      123      733\n  hour minute           time_hour\n1    5      0 2013-12-25 04:00:00\n2    5     15 2013-12-25 04:00:00\n3    5     40 2013-12-25 04:00:00\n4    5     50 2013-12-25 04:00:00\n5    6      0 2013-12-25 05:00:00\n\n#. R\nflights |&gt;\n  subset(month == 12 & day == 25 ) |&gt;\n  head(5)\n\n# A tibble: 5 × 20\n      id  year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n1 105233  2013    12    25      456            500        -4      649\n2 105234  2013    12    25      524            515         9      805\n3 105235  2013    12    25      542            540         2      832\n4 105236  2013    12    25      546            550        -4     1022\n5 105237  2013    12    25      556            600        -4      730\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n11.7.3 flights that have a delay (either on departure or on arrival)\n\nsqldf(\"\n  select *\n  from flights \n  where dep_delay is not null or arr_delay is not null\n  limit 5;\n\")\n\n  id year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n1  1 2013     1   1      517            515         2      830            819\n2  2 2013     1   1      533            529         4      850            830\n3  3 2013     1   1      542            540         2      923            850\n4  4 2013     1   1      544            545        -1     1004           1022\n5  5 2013     1   1      554            600        -6      812            837\n  arr_delay carrier flight tailnum origin dest air_time distance hour minute\n1        11      UA   1545  N14228    EWR  IAH      227     1400    5     15\n2        20      UA   1714  N24211    LGA  IAH      227     1416    5     29\n3        33      AA   1141  N619AA    JFK  MIA      160     1089    5     40\n4       -18      B6    725  N804JB    JFK  BQN      183     1576    5     45\n5       -25      DL    461  N668DN    LGA  ATL      116      762    6      0\n            time_hour\n1 2013-01-01 04:00:00\n2 2013-01-01 04:00:00\n3 2013-01-01 04:00:00\n4 2013-01-01 04:00:00\n5 2013-01-01 05:00:00\n\n#. R\nflights |&gt;\n  subset(!is.na(dep_delay) | !is.na(arr_delay)) |&gt;\n  head(5)\n\n# A tibble: 5 × 20\n     id  year month   day dep_time sched_dep_time dep_delay arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n1     1  2013     1     1      517            515         2      830\n2     2  2013     1     1      533            529         4      850\n3     3  2013     1     1      542            540         2      923\n4     4  2013     1     1      544            545        -1     1004\n5     5  2013     1     1      554            600        -6      812\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n11.7.4 flights that were not cancelled\n(that is, those with valid departure and arrival times)\n\nsqldf(\"\n  select *\n  from flights \n  where dep_time is not null and arr_time is not null\n  limit 5;\n\")\n\n  id year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n1  1 2013     1   1      517            515         2      830            819\n2  2 2013     1   1      533            529         4      850            830\n3  3 2013     1   1      542            540         2      923            850\n4  4 2013     1   1      544            545        -1     1004           1022\n5  5 2013     1   1      554            600        -6      812            837\n  arr_delay carrier flight tailnum origin dest air_time distance hour minute\n1        11      UA   1545  N14228    EWR  IAH      227     1400    5     15\n2        20      UA   1714  N24211    LGA  IAH      227     1416    5     29\n3        33      AA   1141  N619AA    JFK  MIA      160     1089    5     40\n4       -18      B6    725  N804JB    JFK  BQN      183     1576    5     45\n5       -25      DL    461  N668DN    LGA  ATL      116      762    6      0\n            time_hour\n1 2013-01-01 04:00:00\n2 2013-01-01 04:00:00\n3 2013-01-01 04:00:00\n4 2013-01-01 04:00:00\n5 2013-01-01 05:00:00\n\n#. R\nflights |&gt;\n  subset(!is.na(dep_time) & !is.na(arr_time)) |&gt;\n  head(5)\n\n# A tibble: 5 × 20\n     id  year month   day dep_time sched_dep_time dep_delay arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n1     1  2013     1     1      517            515         2      830\n2     2  2013     1     1      533            529         4      850\n3     3  2013     1     1      542            540         2      923\n4     4  2013     1     1      544            545        -1     1004\n5     5  2013     1     1      554            600        -6      812\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n11.7.5 flights with a departure delay sorted by delay\n\nsqldf(\"\n  select *\n  from flights \n  where dep_delay &gt; 0\n  order by dep_delay desc\n  limit 5;\n\")\n\n      id year month day dep_time sched_dep_time dep_delay arr_time\n1   7073 2013     1   9      641            900      1301     1242\n2 235779 2013     6  15     1432           1935      1137     1607\n3   8240 2013     1  10     1121           1635      1126     1239\n4 327044 2013     9  20     1139           1845      1014     1457\n5 270377 2013     7  22      845           1600      1005     1044\n  sched_arr_time arr_delay carrier flight tailnum origin dest air_time distance\n1           1530      1272      HA     51  N384HA    JFK  HNL      640     4983\n2           2120      1127      MQ   3535  N504MQ    JFK  CMH       74      483\n3           1810      1109      MQ   3695  N517MQ    EWR  ORD      111      719\n4           2210      1007      AA    177  N338AA    JFK  SFO      354     2586\n5           1815       989      MQ   3075  N665MQ    JFK  CVG       96      589\n  hour minute           time_hour\n1    9      0 2013-01-09 08:00:00\n2   19     35 2013-06-15 18:00:00\n3   16     35 2013-01-10 15:00:00\n4   18     45 2013-09-20 17:00:00\n5   16      0 2013-07-22 15:00:00\n\n#. R\ndf &lt;- flights |&gt;\n  subset(dep_delay &gt; 0)\ndf[order(desc(df$dep_delay)), ] |&gt;\n  head(5)\n\n# A tibble: 5 × 20\n      id  year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n1   7073  2013     1     9      641            900      1301     1242\n2 235779  2013     6    15     1432           1935      1137     1607\n3   8240  2013     1    10     1121           1635      1126     1239\n4 327044  2013     9    20     1139           1845      1014     1457\n5 270377  2013     7    22      845           1600      1005     1044\n# ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\n\n11.7.6 flights that managed to catch up during the flight sorted by catch up time\n\nsqldf(\"\n  select id, dep_delay, arr_delay, (dep_delay - arr_delay) as catchup\n  from flights\n  where catchup &gt; 0\n  order by catchup desc\n  limit 5;\n\")\n\n      id dep_delay arr_delay catchup\n1 234103       235       126     109\n2 133682        60       -27      87\n3 131144       206       126      80\n4 205313        17       -62      79\n5 134563        24       -52      76\n\n#. R\ndf &lt;- flights |&gt;\n  transform(catchup = dep_delay - arr_delay) |&gt;\n  subset(subset = (catchup &gt; 0), select = c(id, dep_delay, arr_delay, catchup))\ndf[order(desc(df$catchup)), ] |&gt;\n  head(5)\n\n           id dep_delay arr_delay catchup\n234103 234103       235       126     109\n133682 133682        60       -27      87\n131144 131144       206       126      80\n205313 205313        17       -62      79\n134563 134563        24       -52      76\n\n\n\n\n11.7.7 The number of flights per day\n\nsqldf(\"\n  select month, day, count(*) as count\n  from flights\n  group by month, day\n  limit 5\n\")\n\n  month day count\n1     1   1   842\n2     1   2   943\n3     1   3   914\n4     1   4   915\n5     1   5   720\n\n#. R\ndf &lt;- aggregate(id ~ day + month, flights, FUN = length)\ncolnames(df)[3] = c(\"count\")\ndf |&gt; head(5)\n\n  day month count\n1   1     1   842\n2   2     1   943\n3   3     1   914\n4   4     1   915\n5   5     1   720\n\n\n\n\n11.7.8 The busy days (with more than 1000 flights)\n\nsqldf(\"\n  select month, day, count(*) as count\n  from flights\n  group by month, day\n  having count &gt; 1000\n  limit 5;\n\")\n\n  month day count\n1     7   8  1004\n2     7   9  1001\n3     7  10  1004\n4     7  11  1006\n5     7  12  1002\n\n#. R\ndf &lt;- aggregate(id ~ day + month, flights, FUN = length)\ncolnames(df)[3] = \"count\"\ndf |&gt;\n  subset(count &gt; 1000) |&gt;\n  head(5)\n\n    day month count\n189   8     7  1004\n190   9     7  1001\n191  10     7  1004\n192  11     7  1006\n193  12     7  1002\n\n\n\n\n11.7.9 The number of flights per destination\n\nsqldf(\"\n  select dest, count(*) as count\n  from flights\n  group by dest\n  limit 5;\n\")\n\n  dest count\n1  ABQ   254\n2  ACK   265\n3  ALB   439\n4  ANC     8\n5  ATL 17215\n\n#. R\ndf &lt;- aggregate(id ~ dest, flights, FUN = length)\ncolnames(df)[2] = \"count\"\nhead(df,5)\n\n  dest count\n1  ABQ   254\n2  ACK   265\n3  ALB   439\n4  ANC     8\n5  ATL 17215\n\n\n\n\n11.7.10 The popular destinations\n(with more than 365 flights) sorted by number of flights in descending order\n\nsqldf(\"\n  select dest, count(*) as count\n  from flights\n  group by dest\n  having count &gt; 365\n  order by count desc\n  limit 5;\n\")\n\n  dest count\n1  ORD 17283\n2  ATL 17215\n3  LAX 16174\n4  BOS 15508\n5  MCO 14082\n\n#. R\ndf &lt;- aggregate(id ~ dest, flights, FUN = length)\ncolnames(df)[2] = \"count\"\ndf &lt;- df |&gt; subset(count &gt; 365)\nhead(df[order(desc(df$count)), ],5)\n\n   dest count\n70  ORD 17283\n5   ATL 17215\n50  LAX 16174\n12  BOS 15508\n55  MCO 14082\n\n\n\n\n11.7.11 the mean departure delay per day sorted in decreasing order of all flights on busy days of summer\n\nsqldf(\"\n  select month, day, count(*) as count, avg(dep_delay) as avg_delay\n  from flights\n  where month = 6 or month = 7 or month = 8\n  group by month, day\n  having count &gt; 1000\n  order by avg_delay desc\n  limit 5;\n\")\n\n  month day count avg_delay\n1     7  10  1004  52.86070\n2     8   8  1001  43.34995\n3     7   8  1004  37.29665\n4     7   9  1001  30.71150\n5     7  12  1002  25.09615\n\n#. R (from stack overflow)\ndf &lt;- aggregate(dep_delay ~ day + month, flights, \n               FUN = function(x) cbind(count = length(x), avg_delay = mean(x, na.rm = TRUE)), \n               na.action = NULL)\n\n#. df$dep_delay is a matrix\nstr(df)\n\n'data.frame':   365 obs. of  3 variables:\n $ day      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ month    : int  1 1 1 1 1 1 1 1 1 1 ...\n $ dep_delay: num [1:365, 1:2] 842 943 914 915 720 832 933 899 902 932 ...\n\n#. unnest\ndf &lt;- cbind(df[,1:2], df[,3])\ncolnames(df)[3:4] = c(\"count\", \"avg_delay\")\n\n#. filter\ndf &lt;- subset(df, count &gt; 1000)\n\n#. order\nhead(df[order(desc(df$avg_delay)), ],5)\n\n    day month count avg_delay\n191  10     7  1004  52.86070\n220   8     8  1001  43.34995\n189   8     7  1004  37.29665\n190   9     7  1001  30.71150\n193  12     7  1002  25.09615\n\n\n\n\n11.7.12 Flights that flew with a plane manufactured by BOEING\n\nsqldf(\"\nselect flights.id, flights.tailnum, planes.manufacturer\nfrom flights, planes\nwhere flights.tailnum = planes.tailnum and planes.manufacturer = 'BOEING'\nlimit 5;\n\")\n\n    id tailnum manufacturer\n1  746  N11206       BOEING\n2 1162  N11206       BOEING\n3 2105  N11206       BOEING\n4 5499  N11206       BOEING\n5 6769  N11206       BOEING\n\n#. R\n#. all.x = TRUE --&gt; left join\n#. all.y = TRUE --&gt; right join\n#. all = TRUE   --&gt; full join\n#. all = FALSE  --&gt; inner join\ndf &lt;- flights |&gt;\n  merge(planes, by=\"tailnum\", all.x = TRUE)\nsubset(df, subset = (manufacturer == \"BOEING\"), select = c(\"id\", \"tailnum\", \"manufacturer\")) |&gt;\n  head(5)\n\n         id tailnum manufacturer\n4426 220945  N11206       BOEING\n4427 252368  N11206       BOEING\n4428  12885  N11206       BOEING\n4429 132376  N11206       BOEING\n4430 302467  N11206       BOEING\n\n\n\n\n11.7.13 Flights to a destination at altitude greater than 6000 feet\nsorted by altitude\n\nsqldf(\"\n  select flights.id, flights.dest, airports.name, airports.alt\n  from flights, airports\n  where flights.dest = airports.faa and airports.alt &gt; 6000\n  order by airports.alt\n  limit 5;\n\")\n\n      id dest                 name  alt\n1    153  JAC Jackson Hole Airport 6451\n2   1068  JAC Jackson Hole Airport 6451\n3 100067  JAC Jackson Hole Airport 6451\n4 101062  JAC Jackson Hole Airport 6451\n5 101913  JAC Jackson Hole Airport 6451\n\n#. R\ndf &lt;- merge(flights, airports, by.x = \"dest\", by.y = \"faa\", all.x = TRUE)\ndf &lt;- subset(df, subset = (alt &gt; 6000), select = c(\"id\", \"dest\", \"name\", \"alt\"))\nhead(df[order(df$alt), ],5)\n\n           id dest                 name  alt\n154976    153  JAC Jackson Hole Airport 6451\n154977 157037  JAC Jackson Hole Airport 6451\n154978 102007  JAC Jackson Hole Airport 6451\n154979 131009  JAC Jackson Hole Airport 6451\n154980 104658  JAC Jackson Hole Airport 6451\n\n\n\n\n11.7.14 Flights that took off with a plane with 4 engines and a visibility lower than 3 miles\n\nsqldf(\"\n  select flights.id, planes.engines, weather.visib\n  from flights, weather, planes\n  where flights.month = weather.month and flights.day = weather.day and flights.hour = weather.hour and \n      flights.origin = weather.origin and weather.visib &lt; 3 and flights.tailnum = planes.tailnum and planes.engines = 4\n  limit 5;\n\")\n\n      id engines visib\n1  25295       4  0.00\n2 182566       4  0.12\n3  10066       4  2.50\n4  11278       4  0.25\n5 120234       4  0.06\n\n#. R\ndf &lt;- merge(flights, weather, all.x = TRUE)\ndf &lt;- merge(df, planes, by = \"tailnum\", all.x = TRUE)\nsubset(df, subset = (visib &lt; 3 & engines == 4), select = c(\"id\", \"engines\", \"visib\")) |&gt;\n  head(5)\n\n           id engines visib\n120872  25295       4  0.00\n235911 182566       4  0.12\n292212 120734       4  0.06\n292225  10066       4  2.50\n292232  11278       4  0.25\n\n\n\n\n11.7.15 Flights with destination and origin airports with an altitude difference of more than 6000 feet\n\nsqldf(\"\n  select flights.id, airports2.alt, airports1.alt, (airports2.alt - airports1.alt) as altdelta\n  from flights, airports as airports1, airports as airports2\n  where flights.origin = airports1.faa and flights.dest = airports2.faa and altdelta &gt; 6000\n  limit 5;\n\")\n\n    id  alt alt altdelta\n1  153 6451  18     6433\n2  194 6540  18     6522\n3  571 6540  13     6527\n4 1068 6451  18     6433\n5 1095 6540  18     6522\n\n#. R\ndf &lt;- merge(flights, airports, by.x = \"origin\", by.y = \"faa\", all.x = TRUE)\ndf &lt;- merge(df, airports, by.x = \"dest\", by.y = \"faa\", all.x = TRUE)\ndf &lt;- subset(df, select = c(\"id\", \"alt.x\", \"alt.y\"))\ndf &lt;- transform(df, altdelta = alt.y - alt.x)\nsubset(df, altdelta &gt; 6000) |&gt;\n  head(5)\n\n           id alt.x alt.y altdelta\n121549 155570    13  6540     6527\n121550 149990    13  6540     6527\n121551 125764    13  6540     6527\n121552 149018    13  6540     6527\n121553 165081    18  6540     6522"
  },
  {
    "objectID": "week11.html#sqldf-tutorial",
    "href": "week11.html#sqldf-tutorial",
    "title": "11  Coping with Time and Joins",
    "section": "11.8 sqldf tutorial",
    "text": "11.8 sqldf tutorial\nOn August 19, 2016, a blogger variously known as Jasmine Dumas or Jasmine Daly, published an sqldf tutorial, which I’ve reprinted as follows. It may still be available at its original URL.\n\n11.8.1 Example Data\n\n#. packages\nlibrary(sqldf)\n\ndata(\"UCBAdmissions\")\n\n#. must be a data frame\nucb &lt;- as.data.frame(UCBAdmissions)\n\nsqldf(\"select * from ucb\")\n\n      Admit Gender Dept Freq\n1  Admitted   Male    A  512\n2  Rejected   Male    A  313\n3  Admitted Female    A   89\n4  Rejected Female    A   19\n5  Admitted   Male    B  353\n6  Rejected   Male    B  207\n7  Admitted Female    B   17\n8  Rejected Female    B    8\n9  Admitted   Male    C  120\n10 Rejected   Male    C  205\n11 Admitted Female    C  202\n12 Rejected Female    C  391\n13 Admitted   Male    D  138\n14 Rejected   Male    D  279\n15 Admitted Female    D  131\n16 Rejected Female    D  244\n17 Admitted   Male    E   53\n18 Rejected   Male    E  138\n19 Admitted Female    E   94\n20 Rejected Female    E  299\n21 Admitted   Male    F   22\n22 Rejected   Male    F  351\n23 Admitted Female    F   24\n24 Rejected Female    F  317\n\nmajors &lt;- data.frame(major = c(\"math\", \"biology\", \"engineering\", \"computer science\", \"history\", \"architecture\"), Dept = c(LETTERS[1:5], \"Other\"), Faculty = round(runif(6, min = 10, max = 30)))\n\nsqldf(\"select * from majors\")\n\n             major  Dept Faculty\n1             math     A      13\n2          biology     B      16\n3      engineering     C      27\n4 computer science     D      16\n5          history     E      19\n6     architecture Other      11\n\n\n\n\n11.8.2 General Queries\n\n#. Return Female student admission result\nsqldf(\"select * from ucb where Gender = 'Female'\")\n\n      Admit Gender Dept Freq\n1  Admitted Female    A   89\n2  Rejected Female    A   19\n3  Admitted Female    B   17\n4  Rejected Female    B    8\n5  Admitted Female    C  202\n6  Rejected Female    C  391\n7  Admitted Female    D  131\n8  Rejected Female    D  244\n9  Admitted Female    E   94\n10 Rejected Female    E  299\n11 Admitted Female    F   24\n12 Rejected Female    F  317\n\n#. Return the admitted students\nsqldf(\"select * from ucb where Admit = 'Admitted'\")\n\n      Admit Gender Dept Freq\n1  Admitted   Male    A  512\n2  Admitted Female    A   89\n3  Admitted   Male    B  353\n4  Admitted Female    B   17\n5  Admitted   Male    C  120\n6  Admitted Female    C  202\n7  Admitted   Male    D  138\n8  Admitted Female    D  131\n9  Admitted   Male    E   53\n10 Admitted Female    E   94\n11 Admitted   Male    F   22\n12 Admitted Female    F   24\n\n#. order admissions per department\nsqldf(\"select * from ucb where Admit = 'Admitted' order by Freq DESC\")\n\n      Admit Gender Dept Freq\n1  Admitted   Male    A  512\n2  Admitted   Male    B  353\n3  Admitted Female    C  202\n4  Admitted   Male    D  138\n5  Admitted Female    D  131\n6  Admitted   Male    C  120\n7  Admitted Female    E   94\n8  Admitted Female    A   89\n9  Admitted   Male    E   53\n10 Admitted Female    F   24\n11 Admitted   Male    F   22\n12 Admitted Female    B   17\n\n#. how many departments are in this table\nsqldf(\"select distinct Dept from ucb\")\n\n  Dept\n1    A\n2    B\n3    C\n4    D\n5    E\n6    F\n\n\n\n\n11.8.3 Aggregate Queries\n\n#. total admitted studets\nsqldf(\"select sum(Freq) from ucb where Admit = 'Admitted'\")\n\n  sum(Freq)\n1      1755\n\n#. total rejected students\nsqldf(\"select sum(Freq) from ucb where Admit = 'Rejected'\")\n\n  sum(Freq)\n1      2771\n\n#. return total admitted males\nsqldf(\"select sum(Freq) as total_dudes from ucb where Admit = 'Admitted' AND Gender = 'Male'\")\n\n  total_dudes\n1        1198\n\n#. return total reject females\nsqldf(\"select sum(Freq) as total_ladies from ucb where Admit = 'Rejected' AND Gender = 'Female'\")\n\n  total_ladies\n1         1278\n\n#. average number of admitted student by department (usually mean)\nsqldf(\"select Dept, avg(Freq) as average_admitted from ucb where Admit = 'Admitted' group by Dept\")\n\n  Dept average_admitted\n1    A            300.5\n2    B            185.0\n3    C            161.0\n4    D            134.5\n5    E             73.5\n6    F             23.0\n\n#. how many majors are there\nsqldf(\"select count(major) from majors\")\n\n  count(major)\n1            6\n\n#. minimum amount of studets rejected\nsqldf(\"select min(Freq) from ucb where Admit = 'Rejected'\")\n\n  min(Freq)\n1         8\n\n\n\n\n11.8.4 Wild card match Queries\n\nsqldf(\"select * from ucb where Freq between 20 AND 100\")\n\n     Admit Gender Dept Freq\n1 Admitted Female    A   89\n2 Admitted   Male    E   53\n3 Admitted Female    E   94\n4 Admitted   Male    F   22\n5 Admitted Female    F   24\n\nsqldf(\"select * from ucb where Gender Like 'Fe%'\")\n\n      Admit Gender Dept Freq\n1  Admitted Female    A   89\n2  Rejected Female    A   19\n3  Admitted Female    B   17\n4  Rejected Female    B    8\n5  Admitted Female    C  202\n6  Rejected Female    C  391\n7  Admitted Female    D  131\n8  Rejected Female    D  244\n9  Admitted Female    E   94\n10 Rejected Female    E  299\n11 Admitted Female    F   24\n12 Rejected Female    F  317\n\nsqldf(\"select * from ucb where Gender Like '%male%'\")\n\n      Admit Gender Dept Freq\n1  Admitted   Male    A  512\n2  Rejected   Male    A  313\n3  Admitted Female    A   89\n4  Rejected Female    A   19\n5  Admitted   Male    B  353\n6  Rejected   Male    B  207\n7  Admitted Female    B   17\n8  Rejected Female    B    8\n9  Admitted   Male    C  120\n10 Rejected   Male    C  205\n11 Admitted Female    C  202\n12 Rejected Female    C  391\n13 Admitted   Male    D  138\n14 Rejected   Male    D  279\n15 Admitted Female    D  131\n16 Rejected Female    D  244\n17 Admitted   Male    E   53\n18 Rejected   Male    E  138\n19 Admitted Female    E   94\n20 Rejected Female    E  299\n21 Admitted   Male    F   22\n22 Rejected   Male    F  351\n23 Admitted Female    F   24\n24 Rejected Female    F  317\n\nsqldf(\"select * from ucb where Gender Like 'Ma%'\")\n\n      Admit Gender Dept Freq\n1  Admitted   Male    A  512\n2  Rejected   Male    A  313\n3  Admitted   Male    B  353\n4  Rejected   Male    B  207\n5  Admitted   Male    C  120\n6  Rejected   Male    C  205\n7  Admitted   Male    D  138\n8  Rejected   Male    D  279\n9  Admitted   Male    E   53\n10 Rejected   Male    E  138\n11 Admitted   Male    F   22\n12 Rejected   Male    F  351\n\nsqldf(\"select * from ucb where Gender = 'Female' AND Freq &gt;= 100 \")\n\n     Admit Gender Dept Freq\n1 Admitted Female    C  202\n2 Rejected Female    C  391\n3 Admitted Female    D  131\n4 Rejected Female    D  244\n5 Rejected Female    E  299\n6 Rejected Female    F  317\n\nsqldf(\"select * from ucb where Gender Like '_ale'\")\n\n      Admit Gender Dept Freq\n1  Admitted   Male    A  512\n2  Rejected   Male    A  313\n3  Admitted   Male    B  353\n4  Rejected   Male    B  207\n5  Admitted   Male    C  120\n6  Rejected   Male    C  205\n7  Admitted   Male    D  138\n8  Rejected   Male    D  279\n9  Admitted   Male    E   53\n10 Rejected   Male    E  138\n11 Admitted   Male    F   22\n12 Rejected   Male    F  351\n\nsqldf(\"select * from ucb where Gender NOT Like 'M_l_'\")\n\n      Admit Gender Dept Freq\n1  Admitted Female    A   89\n2  Rejected Female    A   19\n3  Admitted Female    B   17\n4  Rejected Female    B    8\n5  Admitted Female    C  202\n6  Rejected Female    C  391\n7  Admitted Female    D  131\n8  Rejected Female    D  244\n9  Admitted Female    E   94\n10 Rejected Female    E  299\n11 Admitted Female    F   24\n12 Rejected Female    F  317\n\n\n\n\n11.8.5 Manipulation & Nested Queries\n\n#. Which department had the most admitted students = A\nsqldf(\"select Dept from ucb where Freq = (select max(Freq) from ucb where Admit = 'Admitted')\")\n\n  Dept\n1    A\n\n#. which department had the most admitted Female student = C\nsqldf(\"select Dept from ucb where Freq = (select max(Freq) from ucb where Gender = 'Female')\")\n\n  Dept\n1    C\n\n#. department with most faculty \nsqldf(\"select Dept from majors where Faculty = (select max(Faculty) from majors)\")\n\n  Dept\n1    C\n\n\n\n\n11.8.6 Join Queries\n\n#. join the two tables together by the common key\nsqldf(\"select * from ucb \n      inner join majors on ucb.Dept = majors.Dept\")\n\n      Admit Gender Dept Freq            major Dept Faculty\n1  Admitted   Male    A  512             math    A      13\n2  Rejected   Male    A  313             math    A      13\n3  Admitted Female    A   89             math    A      13\n4  Rejected Female    A   19             math    A      13\n5  Admitted   Male    B  353          biology    B      16\n6  Rejected   Male    B  207          biology    B      16\n7  Admitted Female    B   17          biology    B      16\n8  Rejected Female    B    8          biology    B      16\n9  Admitted   Male    C  120      engineering    C      27\n10 Rejected   Male    C  205      engineering    C      27\n11 Admitted Female    C  202      engineering    C      27\n12 Rejected Female    C  391      engineering    C      27\n13 Admitted   Male    D  138 computer science    D      16\n14 Rejected   Male    D  279 computer science    D      16\n15 Admitted Female    D  131 computer science    D      16\n16 Rejected Female    D  244 computer science    D      16\n17 Admitted   Male    E   53          history    E      19\n18 Rejected   Male    E  138          history    E      19\n19 Admitted Female    E   94          history    E      19\n20 Rejected Female    E  299          history    E      19\n\n#. join the table on the left with resultant nulls's on the right table\nsqldf(\"select * from ucb left join majors on ucb.Dept = majors.Dept\")\n\n      Admit Gender Dept Freq            major Dept Faculty\n1  Admitted   Male    A  512             math    A      13\n2  Rejected   Male    A  313             math    A      13\n3  Admitted Female    A   89             math    A      13\n4  Rejected Female    A   19             math    A      13\n5  Admitted   Male    B  353          biology    B      16\n6  Rejected   Male    B  207          biology    B      16\n7  Admitted Female    B   17          biology    B      16\n8  Rejected Female    B    8          biology    B      16\n9  Admitted   Male    C  120      engineering    C      27\n10 Rejected   Male    C  205      engineering    C      27\n11 Admitted Female    C  202      engineering    C      27\n12 Rejected Female    C  391      engineering    C      27\n13 Admitted   Male    D  138 computer science    D      16\n14 Rejected   Male    D  279 computer science    D      16\n15 Admitted Female    D  131 computer science    D      16\n16 Rejected Female    D  244 computer science    D      16\n17 Admitted   Male    E   53          history    E      19\n18 Rejected   Male    E  138          history    E      19\n19 Admitted Female    E   94          history    E      19\n20 Rejected Female    E  299          history    E      19\n21 Admitted   Male    F   22             &lt;NA&gt; &lt;NA&gt;      NA\n22 Rejected   Male    F  351             &lt;NA&gt; &lt;NA&gt;      NA\n23 Admitted Female    F   24             &lt;NA&gt; &lt;NA&gt;      NA\n24 Rejected Female    F  317             &lt;NA&gt; &lt;NA&gt;      NA\n\n#. join the table on the right with the left\nsqldf(\"select * from ucb right join majors on ucb.Dept = majors.Dept\")\n\n      Admit Gender Dept Freq            major  Dept Faculty\n1  Admitted   Male    A  512             math     A      13\n2  Rejected   Male    A  313             math     A      13\n3  Admitted Female    A   89             math     A      13\n4  Rejected Female    A   19             math     A      13\n5  Admitted   Male    B  353          biology     B      16\n6  Rejected   Male    B  207          biology     B      16\n7  Admitted Female    B   17          biology     B      16\n8  Rejected Female    B    8          biology     B      16\n9  Admitted   Male    C  120      engineering     C      27\n10 Rejected   Male    C  205      engineering     C      27\n11 Admitted Female    C  202      engineering     C      27\n12 Rejected Female    C  391      engineering     C      27\n13 Admitted   Male    D  138 computer science     D      16\n14 Rejected   Male    D  279 computer science     D      16\n15 Admitted Female    D  131 computer science     D      16\n16 Rejected Female    D  244 computer science     D      16\n17 Admitted   Male    E   53          history     E      19\n18 Rejected   Male    E  138          history     E      19\n19 Admitted Female    E   94          history     E      19\n20 Rejected Female    E  299          history     E      19\n21     &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;   NA     architecture Other      11\n\n\n\n\n11.8.7 Resources\n\nhttps://cran.r-project.org/web/packages/sqldf/sqldf.pdf\nhttps://github.com/ggrothendieck/sqldf\nhttp://www.w3schools.com/sql/default.asp\n\nfin.\n\n\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd ed. O’Reilly Media, Inc. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "week12.html#recap-week-11-time-joins",
    "href": "week12.html#recap-week-11-time-joins",
    "title": "12  Milestone 4; Take-home Exam",
    "section": "12.1 Recap week 11: Time; Joins",
    "text": "12.1 Recap week 11: Time; Joins\n\nUse the lubridate package to do arithmetic with dates and times; you’ll need to create a variable in the final exam for a time period\nYou often need to join tables or data frames together in the workplace; two facilities for doing so are sqldf and the dplyr _join functions"
  },
  {
    "objectID": "week12.html#working-on-m4",
    "href": "week12.html#working-on-m4",
    "title": "12  Milestone 4; Take-home Exam",
    "section": "12.2 Working on m4",
    "text": "12.2 Working on m4\nFor this milestone, you need to do two things and you should work on them together with coordination. It’s easy to split tasks apart, but then the report doesn’t look coherent. You have to try to work as a team. That’s why I’m giving you class time to work together. The two things are the regression diagnostics and fixing the earlier problems.\n\n12.2.1 Some tips on m4\n\nName the files m4.qmd and m4.html\nWhen I download them, I will automatically strip off any -1 or similar things that Canvas has put on\nThe title in the report header should be “Final Report” and must include the names of all contributing team members\nIt will be graded on the full contents, not just the regression diagnostics\n\n\n\n12.2.2 Strategy\n\nI previously suggested that you append your initials to df to name data frames, but almost no one did that.\nI suggest you work together on a first code chunk that massages the original data frame into what you want, then have individual work on chunks where you give the modified data frames unique names that don’t overlap.\nDon’t keep reading the original file in over and over again during your report. This overwrites any previous modifications you made to it.\nDon’t leave the final report to one group member. You must all look it over.\n\n\n\n12.2.3 More tips\n\nIf you have a Mac or if you have WSL2 on Windows, you can have a terminal\nYou can use the terminal to find out whether you are overwriting code with other code\nsay grep -n '&lt;-' m4.qmd at the terminal prompt and the output will be the lines and line numbers of all the places you assign names. You can then sort this output by extending the previous command as follows\nsay grep -n '&lt;-' m4.qmd | sort -k 2 -t : at the terminal prompt to get the assignments sorted by variable. That makes it easy to see if you’ve assigned to the same variable multiple times."
  },
  {
    "objectID": "week12.html#a-word-about-an-ai-conference",
    "href": "week12.html#a-word-about-an-ai-conference",
    "title": "12  Milestone 4; Take-home Exam",
    "section": "12.3 A word about an AI conference",
    "text": "12.3 A word about an AI conference\n\nI attended a conference on ethical AI last semester\nThere was some discussion of the recent open letter suggesting a pause on AI development\n\nSome signatures were bogus, some were by people such as Elon Musk who are probably privately urging their employees to do the opposite of what the letter says\n\nThere is a lot of room for data scientists and ux / ui designers to help with the ethical ai problem\nThe recent paper on ChatGPT4 gave no details, leading one participant to say OpenAI should be renamed ClosedAI\nUnder-resourced communities are under-measured—this is a critical problem\nAttribution of sources by generative AI may be a big problem\nUsing generative AI requires some skill—ChatGPT hallucinates names of court cases and academic publications that don’t exist"
  },
  {
    "objectID": "week13.html#how-many-prefer-this-over-that-tests-of-proportions",
    "href": "week13.html#how-many-prefer-this-over-that-tests-of-proportions",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.1 How many prefer this over that? (Tests of proportions)",
    "text": "13.1 How many prefer this over that? (Tests of proportions)\n\n13.1.1 How many prefer website A over B? (One sample test of proportions in two categories)\nSixty subjects were asked whether they preferred website A or B. Their answer and a subject ID were recorded. Read the data and describe it.\n\nprefsAB &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/prefsAB.csv\"))\ntail(prefsAB) # displays the last few rows of the data frame\n\n# A tibble: 6 × 2\n  Subject Pref \n    &lt;dbl&gt; &lt;chr&gt;\n1      55 A    \n2      56 B    \n3      57 A    \n4      58 B    \n5      59 B    \n6      60 A    \n\nprefsAB$Subject &lt;- factor(prefsAB$Subject) # convert to nominal factor\nprefsAB$Pref &lt;- factor(prefsAB$Pref) # convert to nominal factor\nsummary(prefsAB)\n\n    Subject   Pref  \n 1      : 1   A:14  \n 2      : 1   B:46  \n 3      : 1         \n 4      : 1         \n 5      : 1         \n 6      : 1         \n (Other):54         \n\nggplot(prefsAB,aes(Pref)) +\n  geom_bar(width=0.5,alpha=0.4,fill=\"lightskyblue1\") +\n  theme_tufte(base_size=7)\n\n\n\n\nIs the difference between preferences significant? A default \\(\\chi^2\\) test examines the proportions in two bins, expecting them to be equally apportioned.\nTo do the \\(\\chi^2\\) test, first crosstabulate the data with xtabs().\n\n#. Pearson chi-square test\nprfs &lt;- xtabs( ~ Pref, data=prefsAB)\nprfs # show counts\n\nPref\n A  B \n14 46 \n\nchisq.test(prfs)\n\n\n    Chi-squared test for given probabilities\n\ndata:  prfs\nX-squared = 17.067, df = 1, p-value = 3.609e-05\n\n\nWe don’t really need an exact binomial test yet because the \\(\\chi^2\\) test told us enough: that the difference is not likely due to chance. That was only because there are only two choices. If there were more than two, we’d need a binomial test for every pair if the \\(\\chi^2\\) test turned up a significant difference. This binomial test just foreshadows what we’ll need when we face three categories.\n\n#. binomial test\n#. binom.test(prfs,split.table=Inf)\nbinom.test(prfs)\n\n\n    Exact binomial test\n\ndata:  prfs\nnumber of successes = 14, number of trials = 60, p-value = 4.224e-05\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.1338373 0.3603828\nsample estimates:\nprobability of success \n             0.2333333 \n\n\n\n\n13.1.2 How many prefer website A, B, or C? (One sample test of proportions in three categories)\nFirst, read in and describe the data. Convert Subject to a factor because R reads any numerical data as, well, numeric, but we don’t want to treat it as such. R interprets any data with characters as a factor. We want Subject to be treated as a factor.\n\nprefsABC &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/prefsABC.csv\"))\nhead(prefsABC) # displays the first few rows of the data frame\n\n# A tibble: 6 × 2\n  Subject Pref \n    &lt;dbl&gt; &lt;chr&gt;\n1       1 C    \n2       2 C    \n3       3 B    \n4       4 C    \n5       5 C    \n6       6 B    \n\nprefsABC$Subject &lt;- factor(prefsABC$Subject)\nprefsABC$Pref &lt;- factor(prefsABC$Pref)\nsummary(prefsABC)\n\n    Subject   Pref  \n 1      : 1   A: 8  \n 2      : 1   B:21  \n 3      : 1   C:31  \n 4      : 1         \n 5      : 1         \n 6      : 1         \n (Other):54         \n\npar(pin=c(2.75,1.25),cex=0.5)\nggplot(prefsABC,aes(Pref))+\n  geom_bar(width=0.5,alpha=0.4,fill=\"lightskyblue1\")+\n  theme_tufte(base_size=7)\n\n\n\n\nYou can think of the three websites as representing three bins and the preferences as filling up those bins. Either each bin gets one third of the preferences or there is a discrepancy. The Pearson \\(\\chi^2\\) test functions as an omnibus test to tell whether there is any discrepancy in the proportions of the three bins.\n\nprfs &lt;- xtabs( ~ Pref, data=prefsABC)\nprfs # show counts\n\nPref\n A  B  C \n 8 21 31 \n\nchisq.test(prfs)\n\n\n    Chi-squared test for given probabilities\n\ndata:  prfs\nX-squared = 13.3, df = 2, p-value = 0.001294\n\n\nA multinomial test can test for other than an even distribution across bins. Here’s an example with a one third distribution in each bin.\n\nlibrary(XNomial)\nxmulti(prfs, c(1/3, 1/3, 1/3), statName=\"Prob\")\n\n\nP value (Prob) = 0.0008024\n\n\nNow we don’t know which pair(s) differed so it makes sense to conduct post hoc binomial tests with correction for multiple comparisons. The correction, made by p.adjust(), is because the more hypotheses we check, the higher the probability of a Type I error, a false positive. That is, the more hypotheses we test, the higher the probability that one will appear true by chance. Wikipedia has more detail in its “Multiple Comparisons Problem” article.\nHere, we test separately for whether each one has a third of the preferences.\n\naa &lt;- binom.test(sum(prefsABC$Pref == \"A\"),\n        nrow(prefsABC), p=1/3)\nbb &lt;- binom.test(sum(prefsABC$Pref == \"B\"),\n        nrow(prefsABC), p=1/3)\ncc &lt;- binom.test(sum(prefsABC$Pref == \"C\"),\n        nrow(prefsABC), p=1/3)\np.adjust(c(aa$p.value, bb$p.value, cc$p.value), method=\"holm\")\n\n[1] 0.001659954 0.785201685 0.007446980\n\n\nThe adjusted \\(p\\)-values tell us that A and C differ significantly from a third of the preferences.\n\n\n13.1.3 How many males vs females prefer website A over B? (Two-sample tests of proportions in two categories)\nRevisit our data file with 2 response categories, but now with sex (M/F).\n\nprefsABsex &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/prefsABsex.csv\"))\ntail(prefsABsex)\n\n# A tibble: 6 × 3\n  Subject Pref  Sex  \n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1      55 A     M    \n2      56 B     F    \n3      57 A     M    \n4      58 B     M    \n5      59 B     M    \n6      60 A     M    \n\nprefsABsex$Subject &lt;- factor(prefsABsex$Subject)\nprefsABsex$Pref &lt;- factor(prefsABsex$Pref)\nprefsABsex$Sex &lt;- factor(prefsABsex$Sex)\nsummary(prefsABsex)\n\n    Subject   Pref   Sex   \n 1      : 1   A:14   F:31  \n 2      : 1   B:46   M:29  \n 3      : 1                \n 4      : 1                \n 5      : 1                \n 6      : 1                \n (Other):54                \n\n\nPlotting is slightly more complicated by the fact that we want to represent two groups. There are many ways to do this, including stacked bar charts, side-by-side bars, or the method chosen here, using facet_wrap(~Sex) to cause two separate plots based on Sex to be created.\n\nggplot(prefsABsex,aes(Pref)) +\n  geom_bar(width=0.5,alpha=0.4,fill=\"lightskyblue1\") +\n  facet_wrap(~Sex) +\n  theme_tufte(base_size=7)\n\n\n\n\nAlthough we can guess by looking at the above plot that the difference for females is significant and the difference for males is not, a Pearson chi-square test provides some statistical evidence for this hunch.\n\nprfs &lt;- xtabs( ~ Pref + Sex, data=prefsABsex) # the '+' sign indicates two vars\nprfs\n\n    Sex\nPref  F  M\n   A  2 12\n   B 29 17\n\nchisq.test(prfs)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  prfs\nX-squared = 8.3588, df = 1, p-value = 0.003838\n\n\n\n\n13.1.4 What if the data are lopsided? (G-test, alternative to chi-square)\nWikipedia tells us that the \\(G\\)-test dominates the \\(\\chi^2\\) test when \\(O_i&gt;2E_i\\) in the formula\n\\[\\chi^2=\\sum_i \\frac{(O_i-E_i)^2}{E_i}\\]\nwhere \\(O_i\\) is the observed and \\(E_i\\) is the expected proportion in the \\(i\\)th bin. This situation may occur in small sample sizes. For large sample sizes, both tests give the same conclusion. In our case, we’re on the borderline for this rule in the bin where 29 females prefer B. All females would have to prefer B for the rule to dictate a switch to the \\(G\\)-test.\n\nlibrary(RVAideMemoire)\n\n*** Package RVAideMemoire v 0.9-83-7 ***\n\nG.test(prfs)\n\n\n    G-test\n\ndata:  prfs\nG = 11.025, df = 1, p-value = 0.0008989\n\n#. Fisher's exact test\nfisher.test(prfs)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  prfs\np-value = 0.001877\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.009898352 0.537050159\nsample estimates:\nodds ratio \n 0.1015763 \n\n\n\n\n13.1.5 How many males vs females prefer website A, B, or C? (Two-sample tests of proportions in three categories)\nRevisit our data file with 3 response categories, but now with sex (M/F).\n\nprefsABCsex &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/prefsABCsex.csv\"))\nhead(prefsABCsex)\n\n# A tibble: 6 × 3\n  Subject Pref  Sex  \n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1       1 C     F    \n2       2 C     M    \n3       3 B     M    \n4       4 C     M    \n5       5 C     M    \n6       6 B     F    \n\nprefsABCsex$Subject &lt;- factor(prefsABCsex$Subject)\nprefsABCsex$Pref &lt;- factor(prefsABCsex$Pref)\nprefsABCsex$Sex &lt;- factor(prefsABCsex$Sex)\nsummary(prefsABCsex)\n\n    Subject   Pref   Sex   \n 1      : 1   A: 8   F:29  \n 2      : 1   B:21   M:31  \n 3      : 1   C:31         \n 4      : 1                \n 5      : 1                \n 6      : 1                \n (Other):54                \n\nggplot(prefsABCsex,aes(Pref)) +\n  geom_bar(width=0.5,alpha=0.4,fill=\"lightskyblue1\") +\n  facet_wrap(~Sex) +\n  theme_tufte(base_size=7)\n\n\n\n#. Pearson chi-square test\nprfs &lt;- xtabs( ~ Pref + Sex, data=prefsABCsex)\nprfs\n\n    Sex\nPref  F  M\n   A  3  5\n   B 15  6\n   C 11 20\n\nchisq.test(prfs)\n\nWarning in chisq.test(prfs): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  prfs\nX-squared = 6.9111, df = 2, p-value = 0.03157\n\n#. G-test\nG.test(prfs)\n\n\n    G-test\n\ndata:  prfs\nG = 7.0744, df = 2, p-value = 0.02909\n\n#. Fisher's exact test\nfisher.test(prfs)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  prfs\np-value = 0.03261\nalternative hypothesis: two.sided\n\n\nNow conduct manual post hoc binomial tests for (m)ales—do any prefs for A–C significantly differ from chance for males?\n\nma &lt;- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"A\"),\n        nrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\nmb &lt;- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"B\"),\n        nrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\nmc &lt;- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"C\"),\n        nrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\n#. correct for multiple comparisons\np.adjust(c(ma$p.value, mb$p.value, mc$p.value), method=\"holm\")\n\n[1] 0.109473564 0.126622172 0.001296754\n\n\nNext, conduct manual post hoc binomial tests for (f)emales—do any prefs for A–C significantly differ from chance for females?\n\nfa &lt;- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"A\"),\n        nrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\nfb &lt;- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"B\"),\n        nrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\nfc &lt;- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"C\"),\n        nrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\n#. correct for multiple comparisons\np.adjust(c(fa$p.value, fb$p.value, fc$p.value), method=\"holm\")\n\n[1] 0.02703274 0.09447821 0.69396951"
  },
  {
    "objectID": "week13.html#how-do-groups-compare-in-reading-performance-independent-samples-t-test",
    "href": "week13.html#how-do-groups-compare-in-reading-performance-independent-samples-t-test",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.2 How do groups compare in reading performance? (Independent samples \\(t\\)-test)",
    "text": "13.2 How do groups compare in reading performance? (Independent samples \\(t\\)-test)\nHere we are asking which group read more pages on a particular website.\n\npgviews &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/pgviews.csv\"))\npgviews$Subject &lt;- factor(pgviews$Subject)\npgviews$Site &lt;- factor(pgviews$Site)\nsummary(pgviews)\n\n    Subject    Site        Pages       \n 1      :  1   A:245   Min.   : 1.000  \n 2      :  1   B:255   1st Qu.: 3.000  \n 3      :  1           Median : 4.000  \n 4      :  1           Mean   : 3.958  \n 5      :  1           3rd Qu.: 5.000  \n 6      :  1           Max.   :11.000  \n (Other):494                           \n\ntail(pgviews)\n\n# A tibble: 6 × 3\n  Subject Site  Pages\n  &lt;fct&gt;   &lt;fct&gt; &lt;dbl&gt;\n1 495     A         3\n2 496     B         6\n3 497     B         6\n4 498     A         3\n5 499     A         4\n6 500     B         6\n\n#. descriptive statistics by Site\nplyr::ddply(pgviews, ~ Site, function(data) summary(data$Pages))\n\n  Site Min. 1st Qu. Median     Mean 3rd Qu. Max.\n1    A    1       3      3 3.404082       4    6\n2    B    1       3      4 4.490196       6   11\n\nplyr::ddply(pgviews, ~ Site, summarise, Pages.mean=mean(Pages), Pages.sd=sd(Pages))\n\n  Site Pages.mean Pages.sd\n1    A   3.404082 1.038197\n2    B   4.490196 2.127552\n\n#. graph histograms and a boxplot\nggplot(pgviews,aes(Pages,fill=Site,color=Site)) +\n  geom_bar(alpha=0.5,position=\"identity\",color=\"white\") +\n  scale_color_grey() +\n  scale_fill_grey() +\n  theme_tufte(base_size=7)\n\n\n\nggplot(pgviews,aes(Site,Pages,fill=Site)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n\n\n\n#. independent-samples t-test\nt.test(Pages ~ Site, data=pgviews, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  Pages by Site\nt = -7.2083, df = 498, p-value = 2.115e-12\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -1.3821544 -0.7900745\nsample estimates:\nmean in group A mean in group B \n       3.404082        4.490196"
  },
  {
    "objectID": "week13.html#anova",
    "href": "week13.html#anova",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.3 ANOVA",
    "text": "13.3 ANOVA\nANOVA stands for analysis of variance and is a way to generalize the \\(t\\)-test to more groups.\n\n13.3.1 How long does it take to perform tasks on two IDEs?\n\nide2 &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/ide2.csv\"))\nide2$Subject &lt;- factor(ide2$Subject) # convert to nominal factor\nide2$IDE &lt;- factor(ide2$IDE) # convert to nominal factor\nsummary(ide2)\n\n    Subject        IDE          Time      \n 1      : 1   Eclipse:20   Min.   :155.0  \n 2      : 1   VStudio:20   1st Qu.:271.8  \n 3      : 1                Median :313.5  \n 4      : 1                Mean   :385.1  \n 5      : 1                3rd Qu.:422.0  \n 6      : 1                Max.   :952.0  \n (Other):34                               \n\n#. view descriptive statistics by IDE\nplyr::ddply(ide2, ~ IDE, function(data) summary(data$Time))\n\n      IDE Min. 1st Qu. Median   Mean 3rd Qu. Max.\n1 Eclipse  232  294.75  393.5 468.15  585.50  952\n2 VStudio  155  246.50  287.0 302.10  335.25  632\n\nplyr::ddply(ide2, ~ IDE, summarise, Time.mean=mean(Time), Time.sd=sd(Time))\n\n      IDE Time.mean  Time.sd\n1 Eclipse    468.15 218.1241\n2 VStudio    302.10 101.0778\n\n#. graph histograms and a boxplot\nggplot(ide2,aes(Time,fill=IDE)) +\n  geom_histogram(binwidth=50,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Paired\") +\n  theme_tufte(base_size=7)\n\n\n\nggplot(ide2,aes(IDE,Time,fill=IDE)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n\n\n\n#. independent-samples t-test (suitable? maybe not, because...)\nt.test(Time ~ IDE, data=ide2, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  Time by IDE\nt = 3.0889, df = 38, p-value = 0.003745\nalternative hypothesis: true difference in means between group Eclipse and group VStudio is not equal to 0\n95 percent confidence interval:\n  57.226 274.874\nsample estimates:\nmean in group Eclipse mean in group VStudio \n               468.15                302.10 \n\n\n\n\n13.3.2 Testing ANOVA assumptions\nThe null hypothesis of the Shapiro-Wilk test is that the data are drawn from a normal distribution. A small \\(p\\)-value indicates that they are not.\nThe null hypothesis of the Levene test (using means) and of the Brown-Forsythe test (using medians) is that the variances of the groups are the same. A small \\(p\\)-value indicates that the variances are not the same.\nIn all the above cases, a logarithmic transformation may solve the problem (both non-normality and unequal variances).\n\n#. Shapiro-Wilk normality test on response\nshapiro.test(ide2[ide2$IDE == \"VStudio\",]$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ide2[ide2$IDE == \"VStudio\", ]$Time\nW = 0.84372, p-value = 0.004191\n\nshapiro.test(ide2[ide2$IDE == \"Eclipse\",]$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ide2[ide2$IDE == \"Eclipse\", ]$Time\nW = 0.87213, p-value = 0.01281\n\n#. but really what matters most is the residuals\nm = aov(Time ~ IDE, data=ide2) # fit model\nshapiro.test(residuals(m)) # test residuals\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.894, p-value = 0.001285\n\npar(pin=c(2.75,1.25),cex=0.5)\nqqnorm(residuals(m)); qqline(residuals(m)) # plot residuals\n\n\n\n\n\n\n13.3.3 Kolmogorov-Smirnov test for log-normality\nFit the distribution to a log-normal to estimate fit parameters then supply those to a K-S test with the log-normal distribution fn (see ?plnorm). See ?distributions for many other named probability distributions.\nThe Kolmogorov-Smirnov test can be used with any continuous distribution function. Here, the specification of log-normal is given by the string with the name of the cumulative distribution function of the log-normal, specifically plnorm. (The Kolmogorov-Smirnov test is so frequently used with pnorm, the cumulative distribution function of the normal distribution, that it is often called the Kolmogorov-Smirnov normality test). The remainder of the function call gives the parameters of the log-normal distribution function, mean and standard deviation. The null hypothesis in this case is that the data are drawn from a log-normal distribution. A small \\(p\\)-value indicates that they are not.\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nfit &lt;- fitdistr(ide2[ide2$IDE == \"VStudio\",]$Time,\n           \"lognormal\")$estimate\nks.test(ide2[ide2$IDE == \"VStudio\",]$Time, \"plnorm\",\n    meanlog=fit[1], sdlog=fit[2], exact=TRUE)\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  ide2[ide2$IDE == \"VStudio\", ]$Time\nD = 0.13421, p-value = 0.8181\nalternative hypothesis: two-sided\n\nfit &lt;- fitdistr(ide2[ide2$IDE == \"Eclipse\",]$Time,\n           \"lognormal\")$estimate\nks.test(ide2[ide2$IDE == \"Eclipse\",]$Time, \"plnorm\",\n    meanlog=fit[1], sdlog=fit[2], exact=TRUE)\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  ide2[ide2$IDE == \"Eclipse\", ]$Time\nD = 0.12583, p-value = 0.871\nalternative hypothesis: two-sided\n\n#. tests for homoscedasticity (homogeneity of variance)\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nleveneTest(Time ~ IDE, data=ide2, center=mean) # Levene's test\n\nLevene's Test for Homogeneity of Variance (center = mean)\n      Df F value   Pr(&gt;F)   \ngroup  1  11.959 0.001356 **\n      38                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nleveneTest(Time ~ IDE, data=ide2, center=median) # Brown-Forsythe test\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)  \ngroup  1  5.9144 0.01984 *\n      38                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#. Welch t-test for unequal variances handles\n#. the violation of homoscedasticity. but not\n#. the violation of normality.\nt.test(Time ~ IDE, data=ide2, var.equal=FALSE) # Welch t-test\n\n\n    Welch Two Sample t-test\n\ndata:  Time by IDE\nt = 3.0889, df = 26.8, p-value = 0.004639\nalternative hypothesis: true difference in means between group Eclipse and group VStudio is not equal to 0\n95 percent confidence interval:\n  55.71265 276.38735\nsample estimates:\nmean in group Eclipse mean in group VStudio \n               468.15                302.10 \n\n\n\n\n13.3.4 Data transformation\n\n#. create a new column in ide2 defined as log(Time)\nide2$logTime &lt;- log(ide2$Time) # log transform\nhead(ide2) # verify\n\n# A tibble: 6 × 4\n  Subject IDE      Time logTime\n  &lt;fct&gt;   &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 1       VStudio   341    5.83\n2 2       VStudio   291    5.67\n3 3       VStudio   283    5.65\n4 4       VStudio   155    5.04\n5 5       VStudio   271    5.60\n6 6       VStudio   270    5.60\n\n#. explore for intuition-building\nggplot(ide2,aes(logTime,fill=IDE)) +\n  geom_histogram(binwidth=0.2,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Paired\") +\n  theme_tufte(base_size=7)\n\n\n\nggplot(ide2,aes(IDE,logTime,fill=IDE)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n\n\n\n#. re-test for normality\nshapiro.test(ide2[ide2$IDE == \"VStudio\",]$logTime)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ide2[ide2$IDE == \"VStudio\", ]$logTime\nW = 0.95825, p-value = 0.5094\n\nshapiro.test(ide2[ide2$IDE == \"Eclipse\",]$logTime)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ide2[ide2$IDE == \"Eclipse\", ]$logTime\nW = 0.93905, p-value = 0.23\n\nm &lt;- aov(logTime ~ IDE, data=ide2) # fit model\nshapiro.test(residuals(m)) # test residuals\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.96218, p-value = 0.1987\n\npar(pin=c(2.75,1.25),cex=0.5)\nqqnorm(residuals(m)); qqline(residuals(m)) # plot residuals\n\n\n\n#. re-test for homoscedasticity\nleveneTest(logTime ~ IDE, data=ide2, center=median) # Brown-Forsythe test\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)  \ngroup  1  3.2638 0.07875 .\n      38                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#. independent-samples t-test (now suitable for logTime)\nt.test(logTime ~ IDE, data=ide2, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  logTime by IDE\nt = 3.3121, df = 38, p-value = 0.002039\nalternative hypothesis: true difference in means between group Eclipse and group VStudio is not equal to 0\n95 percent confidence interval:\n 0.1514416 0.6276133\nsample estimates:\nmean in group Eclipse mean in group VStudio \n             6.055645              5.666118 \n\n\n\n\n13.3.5 What if ANOVA assumptions don’t hold? (Nonparametric equivalent of independent-samples t-test)\n\n\n13.3.6 Mann-Whitney U test\nThis is a nonparametric test of the null hypothesis that, for randomly selected values from two populations, the probability of the first being greater than the second is equal to the probability of the second being greater than the first. A small \\(p\\)-value indicates that it is not. (The test is also called the Wilcoxon-Mann-Whitney test, hence the function name given below.)\n\nlibrary(coin)\n\nLoading required package: survival\n\nwilcox_test(Time ~ IDE, data=ide2, distribution=\"exact\")\n\n\n    Exact Wilcoxon-Mann-Whitney Test\n\ndata:  Time by IDE (Eclipse, VStudio)\nZ = 2.9487, p-value = 0.002577\nalternative hypothesis: true mu is not equal to 0\n\nwilcox_test(logTime ~ IDE, data=ide2, distribution=\"exact\") # note: same result\n\n\n    Exact Wilcoxon-Mann-Whitney Test\n\ndata:  logTime by IDE (Eclipse, VStudio)\nZ = 2.9487, p-value = 0.002577\nalternative hypothesis: true mu is not equal to 0\n\n\n\n\n13.3.7 How long does it take to do tasks on one of three tools? (One-way ANOVA preparation)\n\n#. read in a data file with task completion times (min) now from 3 tools\nide3 &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/ide3.csv\"))\nide3$Subject &lt;- factor(ide3$Subject) # convert to nominal factor\nide3$IDE &lt;- factor(ide3$IDE) # convert to nominal factor\nsummary(ide3)\n\n    Subject        IDE          Time      \n 1      : 1   Eclipse:20   Min.   :143.0  \n 2      : 1   PyCharm:20   1st Qu.:248.8  \n 3      : 1   VStudio:20   Median :295.0  \n 4      : 1                Mean   :353.9  \n 5      : 1                3rd Qu.:391.2  \n 6      : 1                Max.   :952.0  \n (Other):54                               \n\n#. view descriptive statistics by IDE\nplyr::ddply(ide3, ~ IDE, function(data) summary(data$Time))\n\n      IDE Min. 1st Qu. Median   Mean 3rd Qu. Max.\n1 Eclipse  232  294.75  393.5 468.15  585.50  952\n2 PyCharm  143  232.25  279.5 291.45  300.00  572\n3 VStudio  155  246.50  287.0 302.10  335.25  632\n\nplyr::ddply(ide3, ~ IDE, summarise, Time.mean=mean(Time), Time.sd=sd(Time))\n\n      IDE Time.mean  Time.sd\n1 Eclipse    468.15 218.1241\n2 PyCharm    291.45 106.8922\n3 VStudio    302.10 101.0778\n\nide3 |&gt;\n  group_by(IDE) |&gt;\n  summarize(median=median(Time),mean=mean(Time),sd=sd(Time))\n\n# A tibble: 3 × 4\n  IDE     median  mean    sd\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Eclipse   394.  468.  218.\n2 PyCharm   280.  291.  107.\n3 VStudio   287   302.  101.\n\n#. explore new response distribution\nggplot(ide3,aes(Time,fill=IDE)) +\n  geom_histogram(binwidth=50,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n\n\n\nggplot(ide3,aes(IDE,Time,fill=IDE)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n\n\n\n#. test normality for new IDE\nshapiro.test(ide3[ide3$IDE == \"PyCharm\",]$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ide3[ide3$IDE == \"PyCharm\", ]$Time\nW = 0.88623, p-value = 0.02294\n\nm &lt;- aov(Time ~ IDE, data=ide3) # fit model\nshapiro.test(residuals(m)) # test residuals\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.89706, p-value = 0.000103\n\npar(pin=c(2.75,1.25),cex=0.5)\nqqnorm(residuals(m)); qqline(residuals(m)) # plot residuals\n\n\n\n#. test log-normality of new IDE\nfit &lt;- fitdistr(ide3[ide3$IDE == \"PyCharm\",]$Time, \"lognormal\")$estimate\nks.test(ide3[ide3$IDE == \"PyCharm\",]$Time,\n    \"plnorm\", meanlog=fit[1], sdlog=fit[2], exact=TRUE) # lognormality\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  ide3[ide3$IDE == \"PyCharm\", ]$Time\nD = 0.1864, p-value = 0.4377\nalternative hypothesis: two-sided\n\n#. compute new log(Time) column and re-test\nide3$logTime &lt;- log(ide3$Time) # add new column\nshapiro.test(ide3[ide3$IDE == \"PyCharm\",]$logTime)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ide3[ide3$IDE == \"PyCharm\", ]$logTime\nW = 0.96579, p-value = 0.6648\n\nm &lt;- aov(logTime ~ IDE, data=ide3) # fit model\nshapiro.test(residuals(m)) # test residuals\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.96563, p-value = 0.08893\n\npar(pin=c(2.75,1.25),cex=0.5)\nqqnorm(residuals(m)); qqline(residuals(m)) # plot residuals\n\n\n\n#. test homoscedasticity\nleveneTest(logTime ~ IDE, data=ide3, center=median) # Brown-Forsythe test\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2  1.7797 0.1779\n      57               \n\n\n\n\n13.3.8 Can we transform data so it fits assumptions? (One-way ANOVA, suitable now to logTime)\n\nm &lt;- aov(logTime ~ IDE, data=ide3) # fit model\nanova(m) # report anova\n\nAnalysis of Variance Table\n\nResponse: logTime\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nIDE        2 2.3064  1.1532   8.796 0.0004685 ***\nResiduals 57 7.4729  0.1311                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#. post hoc independent-samples t-tests\nlibrary(multcomp)\n\nLoading required package: mvtnorm\n\n\nLoading required package: TH.data\n\n\n\nAttaching package: 'TH.data'\n\n\nThe following object is masked from 'package:MASS':\n\n    geyser\n\nsummary(glht(m, mcp(IDE=\"Tukey\")), test=adjusted(type=\"holm\")) # Tukey means compare all pairs\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = logTime ~ IDE, data = ide3)\n\nLinear Hypotheses:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \nPyCharm - Eclipse == 0  -0.4380     0.1145  -3.826 0.000978 ***\nVStudio - Eclipse == 0  -0.3895     0.1145  -3.402 0.002458 ** \nVStudio - PyCharm == 0   0.0485     0.1145   0.424 0.673438    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n#. note: equivalent to this using lsm instead of mcp\nlibrary(emmeans)\nsummary(glht(m, lsm(pairwise ~ IDE)), test=adjusted(type=\"holm\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: aov(formula = logTime ~ IDE, data = ide3)\n\nLinear Hypotheses:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \nEclipse - PyCharm == 0   0.4380     0.1145   3.826 0.000978 ***\nEclipse - VStudio == 0   0.3895     0.1145   3.402 0.002458 ** \nPyCharm - VStudio == 0  -0.0485     0.1145  -0.424 0.673438    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\n\n\n13.3.9 What if we can’t transform data to fit ANOVA assumptions? (Nonparametric equivalent of one-way ANOVA)\n\n#. Kruskal-Wallis test\nkruskal_test(Time ~ IDE, data=ide3, distribution=\"asymptotic\") # can't do exact with 3 levels\n\n\n    Asymptotic Kruskal-Wallis Test\n\ndata:  Time by IDE (Eclipse, PyCharm, VStudio)\nchi-squared = 12.17, df = 2, p-value = 0.002277\n\nkruskal_test(logTime ~ IDE, data=ide3, distribution=\"asymptotic\") # note: same result\n\n\n    Asymptotic Kruskal-Wallis Test\n\ndata:  logTime by IDE (Eclipse, PyCharm, VStudio)\nchi-squared = 12.17, df = 2, p-value = 0.002277\n\n#. for reporting Kruskal-Wallis as chi-square, we can get N with nrow(ide3)\n\n#. manual post hoc Mann-Whitney U pairwise comparisons\n#. note: wilcox_test we used above doesn't take two data vectors, so use wilcox.test\nvs.ec &lt;- wilcox.test(ide3[ide3$IDE == \"VStudio\",]$Time,\n            ide3[ide3$IDE == \"Eclipse\",]$Time, exact=FALSE)\nvs.py &lt;- wilcox.test(ide3[ide3$IDE == \"VStudio\",]$Time,\n            ide3[ide3$IDE == \"PyCharm\",]$Time, exact=FALSE)\nec.py &lt;- wilcox.test(ide3[ide3$IDE == \"Eclipse\",]$Time,\n            ide3[ide3$IDE == \"PyCharm\",]$Time, exact=FALSE)\np.adjust(c(vs.ec$p.value, vs.py$p.value, ec.py$p.value), method=\"holm\")\n\n[1] 0.007681846 0.588488864 0.007681846\n\n#. alternative approach is using PMCMRplus for nonparam pairwise comparisons\nlibrary(PMCMRplus)\nkwAllPairsConoverTest(Time ~ IDE, data=ide3, p.adjust.method=\"holm\")\n\nWarning in kwAllPairsConoverTest.default(c(341, 291, 283, 155, 271, 270, : Ties\nare present. Quantiles were corrected for ties.\n\n\n\n    Pairwise comparisons using Conover's all-pairs test\n\n\ndata: Time by IDE\n\n\n        Eclipse PyCharm\nPyCharm 0.0025  -      \nVStudio 0.0062  0.6620 \n\n\n\nP value adjustment method: holm\n\n\nThe above test was reported by W. J. Conover and R. L. Iman (1979), On multiple-comparisons procedures, Tech. Rep. LA-7677-MS, Los Alamos Scientific Laboratory.\n\n\n13.3.10 Another example of tasks using two tools (More on oneway ANOVA)\nThe designtime data records task times in minutes to complete the same project in Illustrator or InDesign.\nRead the designtime data into R. Determine how many subjects participated.\n\ndt &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/designtime.csv\"))\n#. convert Subject to a factor\ndt$Subject&lt;-as.factor(dt$Subject)\ndt$Tool&lt;-as.factor(dt$Tool)\nsummary(dt)\n\n    Subject            Tool         Time       \n 1      : 1   Illustrator:30   Min.   : 98.19  \n 2      : 1   InDesign   :30   1st Qu.:149.34  \n 3      : 1                    Median :205.54  \n 4      : 1                    Mean   :275.41  \n 5      : 1                    3rd Qu.:361.99  \n 6      : 1                    Max.   :926.15  \n (Other):54                                    \n\nlength(dt$Subject)\n\n[1] 60\n\ntail(dt)\n\n# A tibble: 6 × 3\n  Subject Tool         Time\n  &lt;fct&gt;   &lt;fct&gt;       &lt;dbl&gt;\n1 55      Illustrator  218.\n2 56      InDesign     180.\n3 57      Illustrator  170.\n4 58      InDesign     186.\n5 59      Illustrator  241.\n6 60      InDesign     159.\n\n\nWe see from the summary that there are sixty observations. We can see the same by checking the length() of the Subject (or any other) variable in the data.\nCreate a boxplot of the task time for each tool and comment on the medians and variances.\n\nggplot(dt,aes(Tool,Time,fill=Tool)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n\n\n\n\nBoth the median and the variance is much larger for Illustrator than for InDesign.\nConduct a Shapiro-Wilk test for normality for each tool and comment.\n\nshapiro.test(dt[dt$Tool==\"Illustrator\",]$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dt[dt$Tool == \"Illustrator\", ]$Time\nW = 0.90521, p-value = 0.01129\n\nshapiro.test(dt[dt$Tool==\"InDesign\",]$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dt[dt$Tool == \"InDesign\", ]$Time\nW = 0.95675, p-value = 0.2553\n\n\nIn the case of InDesign, we fail to reject the null hypothesis that the data are drawn from a normal distribution. In the case of Illustrator, we reject the null hypothesis at the five percent level but not at the one percent level (just barely).\nConduct a Shapiro-Wilk test for normality on the residuals and comment.\n\nm&lt;-aov(Time~Tool,data=dt)\nshapiro.test(residuals(m))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.85077, p-value = 3.211e-06\n\n\nWe reject the null hypothesis that the residuals are normally distributed.\nConduct a Brown-Forsythe test of homoscedasticity.\n\nleveneTest(Time~Tool,data=dt,center=median)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value    Pr(&gt;F)    \ngroup  1  20.082 3.545e-05 ***\n      58                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe reject the null hypothesis that the two samples are drawn from populations with equal variance.\nFit a lognormal distribution to the Time response for each Tool. Conduct a Kolmogorov-Smirnov goodness-of-fit test and comment.\n\nfit&lt;-fitdistr(dt[dt$Tool==\"Illustrator\",]$Time,\n    \"lognormal\")$estimate\ntst&lt;-ks.test(dt[dt$Tool==\"Illustrator\",]$Time,\n    \"plnorm\",meanlog=fit[1],sdlog=fit[2],exact=TRUE)\ntst\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  dt[dt$Tool == \"Illustrator\", ]$Time\nD = 0.093358, p-value = 0.9344\nalternative hypothesis: two-sided\n\nfit&lt;-fitdistr(dt[dt$Tool==\"InDesign\",]$Time,\n    \"lognormal\")$estimate\ntst&lt;-ks.test(dt[dt$Tool==\"InDesign\",]$Time,\n    \"plnorm\",meanlog=fit[1],sdlog=fit[2],exact=TRUE)\ntst\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  dt[dt$Tool == \"InDesign\", ]$Time\nD = 0.10005, p-value = 0.8958\nalternative hypothesis: two-sided\n\n\nWe fail to reject the null hypothesis that the Illustrator sample is drawn from a lognormal distribution. We fail to reject the null hypothesis that the InDesign sample is drawn from a lognormal distribution.\nCreate a log-transformed Time response column. Compute the mean for each tool and comment.\n\ndt$logTime&lt;-log(dt$Time)\nmean(dt$logTime[dt$Tool==\"Illustrator\"])\n\n[1] 5.894288\n\nmean(dt$logTime[dt$Tool==\"InDesign\"])\n\n[1] 5.03047\n\ndt |&gt;\n  group_by(Tool) |&gt;\n  summarize(mean=mean(logTime),sd=sd(logTime))\n\n# A tibble: 2 × 3\n  Tool         mean    sd\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Illustrator  5.89 0.411\n2 InDesign     5.03 0.211\n\n\nThe mean for Illustrator appears to be larger than the mean for InDesign.\nConduct an independent-samples \\(t\\)-test on the log-transformed Time response, using the Welch version for unequal variances and comment.\n\nt.test(logTime~Tool,data=dt,var.equal=FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  logTime by Tool\nt = 10.23, df = 43.293, p-value = 3.98e-13\nalternative hypothesis: true difference in means between group Illustrator and group InDesign is not equal to 0\n95 percent confidence interval:\n 0.6935646 1.0340718\nsample estimates:\nmean in group Illustrator    mean in group InDesign \n                 5.894288                  5.030470 \n\n\nWe reject the null hypothesis that the true difference in means is equal to 0.\nConduct an exact nonparametric Mann-Whitney \\(U\\) test on the Time response and comment.\n\nwilcox_test(Time~Tool,data=dt,distribution=\"exact\")\n\n\n    Exact Wilcoxon-Mann-Whitney Test\n\ndata:  Time by Tool (Illustrator, InDesign)\nZ = 6.3425, p-value = 5.929e-14\nalternative hypothesis: true mu is not equal to 0\n\n\nWe reject the null hypothesis that the samples were drawn from populations with the same distribution.\n\n\n13.3.11 Differences in writing speed among three tools (Three levels of a factor in ANOVA)\nWe’ll examine three levels of a factor, which is an alphabet system used for writing. The three levels are named for the text entry systems, EdgeWrite, Graffiti, and Unistrokes.\n\nalpha &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/alphabets.csv\"))\nalpha$Subject&lt;-as.factor(alpha$Subject)\nalpha$Alphabet&lt;-as.factor(alpha$Alphabet)\nsummary(alpha)\n\n    Subject         Alphabet       WPM        \n 1      : 1   EdgeWrite :20   Min.   : 3.960  \n 2      : 1   Graffiti  :20   1st Qu.: 9.738  \n 3      : 1   Unistrokes:20   Median :13.795  \n 4      : 1                   Mean   :14.517  \n 5      : 1                   3rd Qu.:18.348  \n 6      : 1                   Max.   :28.350  \n (Other):54                                   \n\n\nPlot the three text entry systems. ::: {.cell}\nggplot(alpha,aes(Alphabet,WPM,fill=Alphabet)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n\n\n\n:::\nIdentify the average words per minute written with EdgeWrite.\n\nmean(alpha[alpha$Alphabet==\"EdgeWrite\",]$WPM)\n\n[1] 17.14\n\n\nConduct a Shapiro-Wilk test for normality on each method.\n\nshapiro.test(alpha$WPM[alpha$Alphabet==\"EdgeWrite\"])\n\n\n    Shapiro-Wilk normality test\n\ndata:  alpha$WPM[alpha$Alphabet == \"EdgeWrite\"]\nW = 0.95958, p-value = 0.5355\n\nshapiro.test(alpha$WPM[alpha$Alphabet==\"Graffiti\"])\n\n\n    Shapiro-Wilk normality test\n\ndata:  alpha$WPM[alpha$Alphabet == \"Graffiti\"]\nW = 0.94311, p-value = 0.2743\n\nshapiro.test(alpha$WPM[alpha$Alphabet==\"Unistrokes\"])\n\n\n    Shapiro-Wilk normality test\n\ndata:  alpha$WPM[alpha$Alphabet == \"Unistrokes\"]\nW = 0.94042, p-value = 0.2442\n\n\nConduct a Shapiro-Wilk test for normality on the residuals of an ANOVA model stipulating that Alphabet affects WPM.\n\nm&lt;-aov(WPM~Alphabet,data=alpha)\nshapiro.test(residuals(m))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.97762, p-value = 0.3363\n\n\nTest for homoscedasticity.\n\nleveneTest(alpha$WPM~alpha$Alphabet,center=\"median\")\n\nLevene's Test for Homogeneity of Variance (center = \"median\")\n      Df F value Pr(&gt;F)\ngroup  2  1.6219 0.2065\n      57               \n\n\nNow test all three. The mcp function tests multiple means. The keyword Tukey means to do all the possible pairwise comparisons of Alphabet, i.e., Graffiti and EdgeWrite, Graffiti and Unistrokes, and EdgeWrite and Unistrokes. m is the oneway ANOVA model we created above.\n\nsummary(multcomp::glht(m,multcomp::mcp(Alphabet=\"Tukey\")),test=adjusted(type=\"holm\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = WPM ~ Alphabet, data = alpha)\n\nLinear Hypotheses:\n                            Estimate Std. Error t value Pr(&gt;|t|)   \nGraffiti - EdgeWrite == 0     -2.101      1.693  -1.241  0.21982   \nUnistrokes - EdgeWrite == 0   -5.769      1.693  -3.407  0.00363 **\nUnistrokes - Graffiti == 0    -3.668      1.693  -2.166  0.06894 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\nConduct a nonparametric oneway ANOVA using the Kruskal-Wallis test to see if the samples have the same distribution. The null hypothesis is that the samples come from the same distribution.\n\nkruskal_test(alpha$WPM~alpha$Alphabet,distribution=\"asymptotic\")\n\n\n    Asymptotic Kruskal-Wallis Test\n\ndata:  alpha$WPM by\n     alpha$Alphabet (EdgeWrite, Graffiti, Unistrokes)\nchi-squared = 9.7019, df = 2, p-value = 0.007821\n\n\nConduct manual post hoc Mann-Whitney pairwise comparisons and adjust the \\(p\\)-values to take into account the possibility of false discovery.\n\newgf&lt;-wilcox.test(alpha$WPM[alpha$Alphabet==\"EdgeWrite\"],alpha$WPM[alpha$Alphabet==\"Graffiti\"],paired=FALSE,exact=FALSE)\newun&lt;-wilcox.test(alpha$WPM[alpha$Alphabet==\"EdgeWrite\"],alpha$WPM[alpha$Alphabet==\"Unistrokes\"],paired=FALSE,exact=FALSE)\ngfun&lt;-wilcox.test(alpha$WPM[alpha$Alphabet==\"Graffiti\"],alpha$WPM[alpha$Alphabet==\"Unistrokes\"],paired=FALSE,exact=FALSE)\np.adjust(c(ewgf$p.value,ewun$p.value,gfun$p.value),method=\"holm\")\n\n[1] 0.20358147 0.01810677 0.04146919"
  },
  {
    "objectID": "week13.html#same-person-using-two-different-tools-paired-samples-t-test",
    "href": "week13.html#same-person-using-two-different-tools-paired-samples-t-test",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.4 Same person using two different tools (Paired samples \\(t\\)-test)",
    "text": "13.4 Same person using two different tools (Paired samples \\(t\\)-test)\nIs it better to search or scroll for contacts in a smartphone contacts manager? Which takes more time? Which takes more effort? Which is more error-prone? Start by reading in data, converting to factors, and summarizing.\n\nsrchscrl &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/srchscrl.csv\"))\nsrchscrl$Subject &lt;- factor(srchscrl$Subject)\nsrchscrl$Order   &lt;- factor(srchscrl$Order)\nsrchscrl$Technique   &lt;- factor(srchscrl$Technique)\n#. srchscrl$Errors   &lt;- factor(srchscrl$Errors,ordered=TRUE,levels=c(0,1,2,3,4))\nsummary(srchscrl)\n\n    Subject    Technique  Order       Time           Errors         Effort \n 1      : 2   Scroll:20   1:20   Min.   : 49.0   Min.   :0.00   Min.   :1  \n 2      : 2   Search:20   2:20   1st Qu.: 94.5   1st Qu.:0.75   1st Qu.:3  \n 3      : 2                      Median :112.5   Median :1.50   Median :4  \n 4      : 2                      Mean   :117.0   Mean   :1.60   Mean   :4  \n 5      : 2                      3rd Qu.:148.2   3rd Qu.:2.25   3rd Qu.:5  \n 6      : 2                      Max.   :192.0   Max.   :4.00   Max.   :7  \n (Other):28                                                                \n\n\nlibrary(xtable)\noptions(xtable.comment=FALSE)\noptions(xtable.booktabs=TRUE)\nxtable(head(srchscrl),caption=\"First rows of data\")\nView descriptive statistics by Technique. There are several ways to do this. The following uses the plyr package. ::: {.cell}\nplyr::ddply(srchscrl, ~ Technique,\n      function(data) summary(data$Time))\n\n  Technique Min. 1st Qu. Median  Mean 3rd Qu. Max.\n1    Scroll   49  123.50  148.5 137.2     161  192\n2    Search   50   86.75   99.5  96.8     106  147\n\nplyr::ddply(srchscrl, ~ Technique,\n      summarise, Time.mean=mean(Time), Time.sd=sd(Time))\n\n  Technique Time.mean  Time.sd\n1    Scroll     137.2 35.80885\n2    Search      96.8 23.23020\n\n:::\nAnother approach is to use the dplyr package. Be aware that it conflicts with plyr so you should try to avoid using both. If you must use both, as I did above, it may make the most sense to call particular functions from the plyr package rather than load the package. This is what I did with plyr::ddply() above.\n\nsrchscrl |&gt;\n  group_by(Technique) |&gt;\n  summarize(mean=mean(Time),sd=sd(Time))\n\n# A tibble: 2 × 3\n  Technique  mean    sd\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Scroll    137.   35.8\n2 Search     96.8  23.2\n\n\nYou can explore the Time response by making histograms or boxplots. One approach is to use the ggplot2 package and put the histograms together in one frame. The ggplot2 package allows for a remarkable variety of options.\n\nggplot(srchscrl,aes(Time,fill=Technique)) +\n  geom_histogram(bins=30,alpha=0.9,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n\n\n\n\nWe can use the same package for boxplots. Boxplots show the median as a bold line in the middle of the box. The box itself ranges from the first quartile (starting at the 25th percentile) to the third quartile (terminating at the 75th percentile). The whiskers run from the minimum to the maximum, where these are defined as the 25th percentile minus 1.5 times the interquartile range and the 75th percentile plus 1.5 times the interquartile range. The interquartile range is the width of the box. Dots outside the whiskers show outliers.\n\nggplot(srchscrl,aes(Technique,Time,fill=Technique)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n\n\n\n\nWe would rather use parametric statistics if ANOVA assumptions are met. Recall that we can test for normality, normality of residuals, and homoscedasticity. In the case of a within-subjects experiment, we can also test for order effects which is one way to test the independence assumption. First test whether these times seem to be drawn from a normal distribution.\n\nshapiro.test(srchscrl[srchscrl$Technique == \"Search\",]$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  srchscrl[srchscrl$Technique == \"Search\", ]$Time\nW = 0.96858, p-value = 0.7247\n\nshapiro.test(srchscrl[srchscrl$Technique == \"Scroll\",]$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  srchscrl[srchscrl$Technique == \"Scroll\", ]$Time\nW = 0.91836, p-value = 0.09213\n\n\nIn both cases we fail to reject the null hypothesis, which is that the Time data are drawn from a normal distribution. Note that we fail to reject at \\(\\alpha=0.05\\) but that in the case of the Scroll technique we would reject at \\(\\alpha=0.1\\).\nFit a model for testing residuals—the Error function is used to indicate within-subject effects, i.e., each Subject was exposed to all levels of Technique. generally, Error(S/(ABC)) means each S was exposed to every level of A, B, C and S is a column encoding subject ids.\n\nm &lt;- aov(Time ~ Technique + Error(Subject/Technique),\n    data=srchscrl)\n\nThe above-specified model has residuals—departures of the observed data from the data that would be expected if the model were accurate.\nNow we can test the residuals of this model for normality and also examine a QQ plot for normality. The QQ plot shows the theoretical line to which the residuals should adhere if they are normally distributed. Deviations from that line are indications of non-normality. First test by Subject.\n\nshapiro.test(residuals(m$Subject))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m$Subject)\nW = 0.9603, p-value = 0.5783\n\nqqnorm(residuals(m$Subject)) \nqqline(residuals(m$Subject))\n\n\n\n\nWe fail to reject the null hypothesis of normality and the QQ plot looks normal. So far, so good.\nNext test by Subject:Technique.\n\nshapiro.test(residuals(m$'Subject:Technique'))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m$\"Subject:Technique\")\nW = 0.97303, p-value = 0.8172\n\nqqnorm(residuals(m$'Subject:Technique'))\nqqline(residuals(m$'Subject:Technique'))\n\n\n\n\nWe fail to reject the null hypothesis of normality and the QQ plot looks normal. We’re getting there.\nWe’re still checking the ANOVA assumptions. Next thing to test is homoscedasticity, the assumption of equal variance. For this we use the Brown-Forsythe test, a variant of Levene’s test that uses the median instead of the mean, providing greater robustness against non-normal data.\n\nleveneTest(Time ~ Technique, data=srchscrl, center=median)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  2.0088 0.1645\n      38               \n\n\nThis experiment used counterbalancing to ward off the possibility of an order effect. An order effect results from learning or fatigue or some other factor based on the order in which the tests were run. We would like to not have that happen and one solution is to have half the subjects do task A first and half the subjects do task B first. This is the simplest form of counterbalancing. It becomes more problematic if there are more than two tasks.\nFor a paired-samples \\(t\\)-test we must use a wide-format table; most R functions do not require a wide-format table, but the dcast() function offers a quick way to translate long-format into wide-format when we need it.\nA wide-format table has one subject in every row. A long-format table has one observation in every row. Most R functions use long-format tables.\n\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\nsrchscrl.wide.order &lt;- dcast(srchscrl, Subject ~ Order,\n                 value.var=\"Time\")\n\nxtable(head(srchscrl.wide.order),\n       caption=\"First rows of wide order\")\nNow conduct a \\(t\\)-test to see if order has an effect. ::: {.cell}\nt.test(srchscrl.wide.order$\"1\", srchscrl.wide.order$\"2\",\n       paired=TRUE, var.equal=TRUE)\n\n\n    Paired t-test\n\ndata:  srchscrl.wide.order$\"1\" and srchscrl.wide.order$\"2\"\nt = -1.3304, df = 19, p-value = 0.1991\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -47.34704  10.54704\nsample estimates:\nmean difference \n          -18.4 \n\n:::\nWe fail to reject the null hypothesis that the responses do not differ according to order. To phrase this in a more readable (!) way, we have evidence that the order does not matter.\n\n13.4.1 Running the paired \\(t\\)-test\nIt now makes sense to use a paired \\(t\\)-test since the ANOVA assumptions have been satisfied. This is a parametric test of Time where we pair subjects by technique. Again, we need the wide-format table to conduct a paired test. The wide-format table has one row for each subject rather than one row for each observation.\n\nsrchscrl.wide.tech = dcast(srchscrl, Subject ~ Technique,\n               value.var=\"Time\")\n\nxtable(head(srchscrl.wide.tech),\n       caption=\"First rows of wide technique\")\n\nt.test(srchscrl.wide.tech$Search, srchscrl.wide.tech$Scroll,\n       paired=TRUE, var.equal=TRUE)\n\n\n    Paired t-test\n\ndata:  srchscrl.wide.tech$Search and srchscrl.wide.tech$Scroll\nt = -3.6399, df = 19, p-value = 0.001743\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -63.63083 -17.16917\nsample estimates:\nmean difference \n          -40.4 \n\n\nThis supports the intuition we developed doing the histogram and boxplots only now we have a valid statistical test to support this intuition.\nSuppose we did not satisfy the ANOVA assumptions. Then we would conduct the nonparametric equivalent of paired-samples t-test.\n\n\n13.4.2 Exploring a Poisson-distributed factor\nExplore the Errors response; error counts are often Poisson-distributed.\n\nplyr::ddply(srchscrl, ~ Technique, function(data)\n      summary(data$Errors))\n\n  Technique Min. 1st Qu. Median Mean 3rd Qu. Max.\n1    Scroll    0       0    0.5  0.7       1    2\n2    Search    1       2    2.5  2.5       3    4\n\nplyr::ddply(srchscrl, ~ Technique, summarise,\n      Errors.mean=mean(Errors), Errors.sd=sd(Errors))\n\n  Technique Errors.mean Errors.sd\n1    Scroll         0.7 0.8013147\n2    Search         2.5 1.0513150\n\n\n\nggplot(srchscrl,aes(Errors,fill=Technique)) +\n  geom_histogram(bins=20,alpha=0.9,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n\n\n\nggplot(srchscrl,aes(Technique,Errors,fill=Technique)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n\n\n\n\nTry to fit a Poisson distribution for count data. Note that ks.test() only works for continuous distributions, but Poisson distributions are discrete, so use fitdist, not fitdistr, and test with gofstat.\n\nlibrary(fitdistrplus)\nfit = fitdist(srchscrl[srchscrl$Technique == \"Search\",]$Errors,\n          \"pois\", discrete=TRUE)\ngofstat(fit) # goodness-of-fit test\n\nChi-squared statistic:  1.522231 \nDegree of freedom of the Chi-squared distribution:  2 \nChi-squared p-value:  0.4671449 \n   the p-value may be wrong with some theoretical counts &lt; 5  \nChi-squared table:\n     obscounts theocounts\n&lt;= 1  4.000000   5.745950\n&lt;= 2  6.000000   5.130312\n&lt;= 3  6.000000   4.275260\n&gt; 3   4.000000   4.848477\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   65.61424\nBayesian Information Criterion   66.60997\n\nfit = fitdist(srchscrl[srchscrl$Technique == \"Scroll\",]$Errors,\n          \"pois\", discrete=TRUE)\ngofstat(fit) # goodness-of-fit test\n\nChi-squared statistic:  0.3816087 \nDegree of freedom of the Chi-squared distribution:  1 \nChi-squared p-value:  0.5367435 \n   the p-value may be wrong with some theoretical counts &lt; 5  \nChi-squared table:\n     obscounts theocounts\n&lt;= 0 10.000000   9.931706\n&lt;= 1  6.000000   6.952194\n&gt; 1   4.000000   3.116100\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   45.53208\nBayesian Information Criterion   46.52781\n\n\nConduct a Wilcoxon signed-rank test on Errors. ::: {.cell}\nwilcoxsign_test(Errors ~ Technique | Subject,\n        data=srchscrl, distribution=\"exact\")\n\n\n    Exact Wilcoxon-Pratt Signed-Rank Test\n\ndata:  y by x (pos, neg) \n     stratified by block\nZ = -3.6701, p-value = 6.104e-05\nalternative hypothesis: true mu is not equal to 0\n\n:::\nNote: the term afer the “|” indicates the within-subjects blocking term for matched pairs.\n\n\n13.4.3 Examining a Likert scale response item\nNow also examine Effort, the ordinal Likert scale response (1-7).\n\nplyr::ddply(srchscrl, ~ Technique, function(data)\n      summary(data$Effort))\n\n  Technique Min. 1st Qu. Median Mean 3rd Qu. Max.\n1    Scroll    1       3      4  4.4    6.00    7\n2    Search    1       3      4  3.6    4.25    5\n\nplyr::ddply(srchscrl, ~ Technique, summarise,\n      Effort.mean=mean(Effort), Effort.sd=sd(Effort))\n\n  Technique Effort.mean Effort.sd\n1    Scroll         4.4  1.698296\n2    Search         3.6  1.187656\n\n\n\nggplot(srchscrl,aes(Effort,fill=Technique)) +\n  geom_histogram(bins=20,alpha=0.9,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n\n\n\nggplot(srchscrl,aes(Technique,Effort,fill=Technique)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Set3\") +\n  geom_dotplot(show.legend=FALSE,binaxis='y',stackdir='center',dotsize=1) +\n  theme_tufte(base_size=8)\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\nOur response is ordinal within-subjects, so use nonparametric Wilcoxon signed-rank.\n\nwilcoxsign_test(Effort ~ Technique | Subject,\n        data=srchscrl, distribution=\"exact\")\n\n\n    Exact Wilcoxon-Pratt Signed-Rank Test\n\ndata:  y by x (pos, neg) \n     stratified by block\nZ = 1.746, p-value = 0.08746\nalternative hypothesis: true mu is not equal to 0"
  },
  {
    "objectID": "week13.html#people-doing-tasks-on-different-phones-in-different-postures-factorial-anova",
    "href": "week13.html#people-doing-tasks-on-different-phones-in-different-postures-factorial-anova",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.5 People doing tasks on different phones in different postures (Factorial ANOVA)",
    "text": "13.5 People doing tasks on different phones in different postures (Factorial ANOVA)\nThe scenario is text entry on smartphone keyboards: iPhone and Galaxy, in different postures: sitting, walking, standing.\nThe statistics employed include Factorial ANOVA, repeated measures ANOVA, main effects, interaction effects, the Aligned Rank Transform for nonparametric ANOVAs.\nThis is a \\(3 \\times 2\\) mixed factorial design. It is mixed in the sense that there is a between-subjects factor (Keyboard) and a within-subjects factor (Posture). It is balanced in the sense that there are twelve persons using each Keyboard and they are each examined for all three levels of Posture.\n\n13.5.1 Read and describe the data\n\nmbltxt &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/mbltxt.csv\"))\nhead(mbltxt)\n\n# A tibble: 6 × 6\n  Subject Keyboard Posture Posture_Order   WPM Error_Rate\n    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1       1 iPhone   Sit                 1  20.2     0.022 \n2       1 iPhone   Stand               2  23.7     0.03  \n3       1 iPhone   Walk                3  20.8     0.0415\n4       2 iPhone   Sit                 1  20.9     0.022 \n5       2 iPhone   Stand               3  23.3     0.0255\n6       2 iPhone   Walk                2  19.1     0.0355\n\nmbltxt &lt;- within(mbltxt, Subject &lt;- as.factor(Subject))\nmbltxt &lt;- within(mbltxt, Keyboard &lt;- as.factor(Keyboard))\nmbltxt &lt;- within(mbltxt, Posture &lt;- as.factor(Posture))\nmbltxt &lt;- within(mbltxt, Posture_Order &lt;- as.factor(Posture_Order))\nsummary(mbltxt)\n\n    Subject     Keyboard   Posture   Posture_Order      WPM        \n 1      : 3   Galaxy:36   Sit  :24   1:24          Min.   : 9.454  \n 2      : 3   iPhone:36   Stand:24   2:24          1st Qu.:19.091  \n 3      : 3               Walk :24   3:24          Median :21.032  \n 4      : 3                                        Mean   :20.213  \n 5      : 3                                        3rd Qu.:23.476  \n 6      : 3                                        Max.   :25.380  \n (Other):54                                                        \n   Error_Rate     \n Min.   :0.01500  \n 1st Qu.:0.02200  \n Median :0.03050  \n Mean   :0.03381  \n 3rd Qu.:0.04000  \n Max.   :0.06950  \n                  \n\n\n\n\n13.5.2 Explore the WPM (words per minute) data\n\ns &lt;- mbltxt |&gt;\n  group_by(Keyboard,Posture) |&gt;\n  summarize(\n    WPM.median=median(WPM),\n    WPM.mean=mean(WPM),\n    WPM.sd=sd(WPM)\n  )\n\n`summarise()` has grouped output by 'Keyboard'. You can override using the\n`.groups` argument.\n\ns\n\n# A tibble: 6 × 5\n# Groups:   Keyboard [2]\n  Keyboard Posture WPM.median WPM.mean WPM.sd\n  &lt;fct&gt;    &lt;fct&gt;        &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Galaxy   Sit           23.8     23.9  0.465\n2 Galaxy   Stand         21.2     21.2  0.810\n3 Galaxy   Walk          12.2     12.1  1.26 \n4 iPhone   Sit           20.9     21.0  0.701\n5 iPhone   Stand         23.8     23.9  0.834\n6 iPhone   Walk          19.1     19.2  1.35 \n\n\n\n\n13.5.3 Histograms for both factors\n\nggplot(mbltxt,aes(WPM,fill=Keyboard)) +\n  geom_histogram(bins=20,alpha=0.9,position=\"dodge\",show.legend=FALSE) +\n  scale_color_brewer() +\n  scale_fill_brewer() +\n  facet_grid(Keyboard~Posture) +\n  theme_tufte(base_size=8)\n\n\n\n\n\n\n13.5.4 Boxplot of both factors\n\nggplot(mbltxt,aes(Keyboard,WPM,fill=Keyboard)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  facet_wrap(~Posture) +\n  theme_tufte(base_size=7)\n\n\n\n\n\n\n13.5.5 An interaction plot\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(mbltxt,\n     interaction.plot(Posture, Keyboard, WPM,\n                      ylim=c(0, max(mbltxt$WPM))))\n\n\n\n\n\n\n13.5.6 Test for a Posture order effect\nThis is to ensure that counterbalancing worked.\n\nlibrary(ez)\nm &lt;- ezANOVA(dv=WPM,\n            between=Keyboard,\n            within=Posture_Order,\n            wid=Subject,\n            data=mbltxt)\nm$Mauchly\n\n                  Effect         W         p p&lt;.05\n3          Posture_Order 0.9912922 0.9122583      \n4 Keyboard:Posture_Order 0.9912922 0.9122583      \n\n\nWikipedia tells us that “Sphericity is an important assumption of a repeated-measures ANOVA. It refers to the condition where the variances of the differences between all possible pairs of within-subject conditions (i.e., levels of the independent variable) are equal. The violation of sphericity occurs when it is not the case that the variances of the differences between all combinations of the conditions are equal. If sphericity is violated, then the variance calculations may be distorted, which would result in an \\(F\\)-ratio that would be inflated.” (from the Wikipedia article on Mauchly’s sphericity test)\nMauchly’s test of sphericity above tells us that there is not a significant departure from sphericity, so we can better rely on the \\(F\\)-statistic in the following ANOVA, the purpose of which is to detect any order effect that would interfere with our later results.\n\nm$ANOVA\n\n                  Effect DFn DFd            F            p p&lt;.05          ges\n2               Keyboard   1  22 1.244151e+02 1.596641e-10     * 0.0794723123\n3          Posture_Order   2  44 5.166254e-02 9.497068e-01       0.0023071128\n4 Keyboard:Posture_Order   2  44 2.830819e-03 9.971734e-01       0.0001266932\n\n\nThe \\(F\\)-statistic for Posture_Order is very small, indicating that there is not an order effect. That gives us the confidence to run the ANOVA test we wanted to run all along."
  },
  {
    "objectID": "week13.html#differences-between-peoples-performance-and-within-a-persons-performance-two-way-mixed-factorial-anova",
    "href": "week13.html#differences-between-peoples-performance-and-within-a-persons-performance-two-way-mixed-factorial-anova",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.6 Differences between people’s performance and within a person’s performance (Two-way mixed factorial ANOVA)",
    "text": "13.6 Differences between people’s performance and within a person’s performance (Two-way mixed factorial ANOVA)\nSince a mixed factorial design by definition has both a between-subjects and a within-subjects factor, we don’t need to also mention that this is a repeated measures test.\n\nm &lt;- ezANOVA(dv=WPM,\n            between=Keyboard,\n            within=Posture,\n            wid=Subject,\n            data=mbltxt)\nm$Mauchly\n\n            Effect         W           p p&lt;.05\n3          Posture 0.6370236 0.008782794     *\n4 Keyboard:Posture 0.6370236 0.008782794     *\n\n\nIn this case, sphericity is violated, so we need to additionally apply the Greenhouse-Geisser correction or the less conservative Huyn-Feldt correction. Nevertheless, let’s look at the uncorrected ANOVA table. Later, we’ll compare it with the uncorrected version provided by the aov() function.\n\nm$ANOVA\n\n            Effect DFn DFd        F            p p&lt;.05       ges\n2         Keyboard   1  22 124.4151 1.596641e-10     * 0.6151917\n3          Posture   2  44 381.4980 1.602465e-28     * 0.9255880\n4 Keyboard:Posture   2  44 157.1600 9.162076e-21     * 0.8367128\n\n\nNote that “ges” in the ANOVA table is the generalized eta-squared measure of effect size, \\(\\eta^2_G\\), preferred to eta-squared or partial eta-squared. See Roger Bakeman (2005) “Recommended effect size statistics for repeated measures designs”, Behavior Research Methods, 37 (3) pages 379–384. There, he points out that the usual \\(\\eta^2\\) is the ratio of effect to total variance:\n\\[\\eta^2=\\frac{SS_{\\text{effect}}}{SS_{\\text{total}}}\\]\nwhere \\(SS\\) is sum of squares. This is similar to the \\(R^2\\) measure typically reported for regression results. The generalized version is alleged to compensate for the deficiencies that \\(\\eta^2\\) shares with \\(R^2\\), mainly that it can be improved by simply adding more predictors. The generalized version looks like this:\n\\[\\eta^2_G=\\frac{SS_{\\text{effect}}}{\\delta \\times SS_{\\text{effect}} + \\sum SS_{\\text{measured}}}\\]\nHere \\(\\delta=0\\) if the effect involves one or more measured factors and \\(\\delta=1\\) if the effect involves only manipulated factors. (Actually it is a little more complicated—here I’m just trying to convey a crude idea that \\(\\eta^2_G\\) ranges between 0 and 1 and that, as it approaches 1, the size of the effect is greater. Oddly enough, it is common to report effect sizes as simply small, medium, or large.)\nNow compute the corrected degrees of freedom for each corrected effect.\n\npos &lt;- match(m$'Sphericity Corrections'$Effect,\n            m$ANOVA$Effect) # positions of within-Ss efx in m$ANOVA\nm$Sphericity$GGe.DFn &lt;- m$Sphericity$GGe * m$ANOVA$DFn[pos] # Greenhouse-Geisser\nm$Sphericity$GGe.DFd &lt;- m$Sphericity$GGe * m$ANOVA$DFd[pos]\nm$Sphericity$HFe.DFn &lt;- m$Sphericity$HFe * m$ANOVA$DFn[pos] # Huynh-Feldt\nm$Sphericity$HFe.DFd &lt;- m$Sphericity$HFe * m$ANOVA$DFd[pos]\nm$Sphericity\n\n            Effect       GGe        p[GG] p[GG]&lt;.05       HFe        p[HF]\n3          Posture 0.7336884 1.558280e-21         * 0.7731517 1.432947e-22\n4 Keyboard:Posture 0.7336884 7.800756e-16         * 0.7731517 1.447657e-16\n  p[HF]&lt;.05  GGe.DFn  GGe.DFd  HFe.DFn  HFe.DFd\n3         * 1.467377 32.28229 1.546303 34.01868\n4         * 1.467377 32.28229 1.546303 34.01868\n\n\nThe above table shows the Greenhouse Geisser correction to the numerator (GGe.DFn) and denominator (GGe.DFd) degrees of freedom and the resulting \\(p\\)-values (p[GG]). The Greenhouse Geiser epsilon statistic (\\(\\epsilon\\)) is shown as GGe. There is an analogous set of measures for the less conservative Huynh-Feldt correction. Note that you could calculate a more conservative \\(F\\)-statistic using the degrees of freedom given even though a corrected \\(F\\)-statistic is not shown for some reason."
  },
  {
    "objectID": "week13.html#anova-results-from-aov",
    "href": "week13.html#anova-results-from-aov",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.7 ANOVA results from aov()",
    "text": "13.7 ANOVA results from aov()\nThe uncorrected results from the ez package are the same as the aov() function in base R, shown below.\n\nm &lt;- aov(WPM ~ Keyboard * Posture + Error(Subject/Posture),\n        data=mbltxt) # fit model\nsummary(m)\n\n\nError: Subject\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nKeyboard   1  96.35   96.35   124.4 1.6e-10 ***\nResiduals 22  17.04    0.77                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Subject:Posture\n                 Df Sum Sq Mean Sq F value Pr(&gt;F)    \nPosture           2  749.6   374.8   381.5 &lt;2e-16 ***\nKeyboard:Posture  2  308.8   154.4   157.2 &lt;2e-16 ***\nResiduals        44   43.2     1.0                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n13.7.1 Manual post hoc pairwise comparisons\nBecause the ANOVA table showed a significant interaction effect and the significance of that interaction effect was borne out by the small p[GG] value, it makes sense to conduct post hoc pairwise comparisons. These require reshaping the data to a wide format because the \\(t\\) test expects data in that format.\n\nmbltxt.wide &lt;- dcast(mbltxt, Subject + Keyboard ~ Posture,\n                    value.var=\"WPM\")\nhead(mbltxt.wide)\n\n  Subject Keyboard     Sit   Stand    Walk\n1       1   iPhone 20.2145 23.7485 20.7960\n2       2   iPhone 20.8805 23.2595 19.1305\n3       3   iPhone 21.2635 23.4945 20.8545\n4       4   iPhone 20.7080 23.9220 18.2575\n5       5   iPhone 21.0075 23.4700 17.7105\n6       6   iPhone 19.9115 24.2975 19.8550\n\nsit &lt;- t.test(mbltxt.wide$Sit ~ Keyboard, data=mbltxt.wide)\nstd &lt;- t.test(mbltxt.wide$Stand ~ Keyboard, data=mbltxt.wide)\nwlk &lt;- t.test(mbltxt.wide$Walk ~ Keyboard, data=mbltxt.wide)\np.adjust(c(sit$p.value, std$p.value, wlk$p.value), method=\"holm\")\n\n[1] 3.842490e-10 4.622384e-08 1.450214e-11\n\n\nThe above \\(p\\)-values indicate significant differences for all three.\n\n\n13.7.2 Compare iPhone ‘sit’ and ‘walk’\n\npar(pin=c(2.75,1.25),cex=0.5)\ntst&lt;-t.test(mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\",]$Sit,\n       mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\",]$Walk,\n       paired=TRUE)\ntst\n\n\n    Paired t-test\n\ndata:  mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\", ]$Sit and mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\", ]$Walk\nt = 3.6259, df = 11, p-value = 0.003985\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.6772808 2.7695525\nsample estimates:\nmean difference \n       1.723417 \n\npar(pin=c(2.75,1.25),cex=0.5)\nboxplot(mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\",]$Sit,\n        mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\",]$Walk,\n        xlab=\"iPhone.Sit vs. iPhone.Walk\", ylab=\"WPM\")"
  },
  {
    "objectID": "week13.html#what-if-anova-assumptions-arent-met-nonparametric-approach-to-factorial-anova",
    "href": "week13.html#what-if-anova-assumptions-arent-met-nonparametric-approach-to-factorial-anova",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.8 What if ANOVA assumptions aren’t met? (Nonparametric approach to factorial ANOVA)",
    "text": "13.8 What if ANOVA assumptions aren’t met? (Nonparametric approach to factorial ANOVA)\nThe rest of this section concerns a nonparametric approach developed at the University of Washington.\n\n13.8.1 The Aligned Rank Transform (ART) procedure\nhttp://depts.washington.edu/aimgroup/proj/art/\n\n\n13.8.2 Explore the Error_Rate data\n\ns &lt;- mbltxt |&gt;\n  group_by(Keyboard,Posture) |&gt;\n  summarize(\n    WPM.median=median(Error_Rate),\n    WPM.mean=mean(Error_Rate),\n    WPM.sd=sd(Error_Rate)\n  )\n\n`summarise()` has grouped output by 'Keyboard'. You can override using the\n`.groups` argument.\n\ns\n\n# A tibble: 6 × 5\n# Groups:   Keyboard [2]\n  Keyboard Posture WPM.median WPM.mean  WPM.sd\n  &lt;fct&gt;    &lt;fct&gt;        &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 Galaxy   Sit         0.019    0.0194 0.00243\n2 Galaxy   Stand       0.0305   0.0307 0.00406\n3 Galaxy   Walk        0.0658   0.0632 0.00575\n4 iPhone   Sit         0.0205   0.0199 0.00248\n5 iPhone   Stand       0.0302   0.0298 0.00258\n6 iPhone   Walk        0.04     0.0399 0.00405\n\n\n\n\n13.8.3 Histograms of Error_Rate\n\nggplot(mbltxt,aes(Error_Rate,fill=Keyboard)) +\n  geom_histogram(bins=20,alpha=0.9,position=\"dodge\",show.legend=FALSE) +\n  scale_color_brewer() +\n  scale_fill_brewer() +\n  facet_grid(Keyboard~Posture) +\n  theme_tufte(base_size=8)\n\n\n\n\n\n\n13.8.4 Box plots of Error_Rate\n\nggplot(mbltxt,aes(Keyboard,Error_Rate,fill=Keyboard)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Greens\") +\n  facet_wrap(~Posture) +\n  theme_tufte(base_size=7)\n\n\n\n\n\n\n13.8.5 Interaction plot of Error_Rate\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(mbltxt,\n     interaction.plot(Posture, Keyboard, Error_Rate,\n                      ylim=c(0, max(mbltxt$Error_Rate))))\n\n\n\n\n\n\n13.8.6 Aligned Rank Transform on Error_Rate\n\nlibrary(ARTool) # for art, artlm\nm &lt;- art(Error_Rate ~ Keyboard * Posture + (1|Subject), data=mbltxt) # uses LMM\nanova(m) # report anova\n\nboundary (singular) fit: see help('isSingular')\n\n\nAnalysis of Variance of Aligned Rank Transformed Data\n\nTable Type: Analysis of Deviance Table (Type III Wald F tests with Kenward-Roger df) \nModel: Mixed Effects (lmer)\nResponse: art(Error_Rate)\n\n                         F Df Df.res     Pr(&gt;F)    \n1 Keyboard          89.450  1     22 3.2959e-09 ***\n2 Posture          274.704  2     44 &lt; 2.22e-16 ***\n3 Keyboard:Posture  78.545  2     44 3.0298e-15 ***\n---\nSignif. codes:   0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\n\n\n\n13.8.7 Examine the normality assumption\n\npar(pin=c(2.75,1.25),cex=0.5)\nshapiro.test(residuals(m)) # normality?\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.98453, p-value = 0.5227\n\nqqnorm(residuals(m)); qqline(residuals(m)) # seems to conform\n\n\n\n\n\n\n13.8.8 Interaction plot\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(mbltxt,\n     interaction.plot(Posture, Keyboard, Error_Rate,\n              ylim=c(0, max(mbltxt$Error_Rate)))) # for convenience\n\n\n\n\n\n\n13.8.9 Conduct post hoc pairwise comparisons within each factor\n\n#. library(emmeans) # instead of lsmeans\n#. for backward compatibility, emmeans provides an lsmeans() function\nlsmeans(artlm(m, \"Keyboard\"), pairwise ~ Keyboard)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n$lsmeans\n Keyboard lsmean   SE df lower.CL upper.CL\n Galaxy     52.3 2.36 22     47.4     57.2\n iPhone     20.7 2.36 22     15.8     25.6\n\nResults are averaged over the levels of: Posture \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n$contrasts\n contrast        estimate   SE df t.ratio p.value\n Galaxy - iPhone     31.6 3.34 22   9.458  &lt;.0001\n\nResults are averaged over the levels of: Posture \nDegrees-of-freedom method: kenward-roger \n\nlsmeans(artlm(m, \"Posture\"), pairwise ~ Posture)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n$lsmeans\n Posture lsmean   SE   df lower.CL upper.CL\n Sit       12.5 1.47 65.9     9.57     15.4\n Stand     36.5 1.47 65.9    33.57     39.4\n Walk      60.5 1.47 65.9    57.57     63.4\n\nResults are averaged over the levels of: Keyboard \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n$contrasts\n contrast     estimate   SE df t.ratio p.value\n Sit - Stand       -24 2.05 44 -11.720  &lt;.0001\n Sit - Walk        -48 2.05 44 -23.439  &lt;.0001\n Stand - Walk      -24 2.05 44 -11.720  &lt;.0001\n\nResults are averaged over the levels of: Keyboard \nDegrees-of-freedom method: kenward-roger \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n#. Warning: don't do the following in ART!\n#lsmeans(artlm(m, \"Keyboard : Posture\"), pairwise ~ Keyboard : Posture)\n\nThe above contrast-testing method is invalid for cross-factor pairwise comparisons in ART. and you can’t just grab aligned-ranks for manual \\(t\\)-tests. instead, use testInteractions() from the phia package to perform “interaction contrasts.” See vignette(\"art-contrasts\").\n\nlibrary(phia)\ntestInteractions(artlm(m, \"Keyboard:Posture\"),\n                 pairwise=c(\"Keyboard\", \"Posture\"), adjustment=\"holm\")\n\nboundary (singular) fit: see help('isSingular')\n\n\nChisq Test: \nP-value adjustment method: holm\n                             Value     SE Df    Chisq Pr(&gt;Chisq)    \nGalaxy-iPhone :  Sit-Stand  -5.083 6.8028  1   0.5584     0.4549    \nGalaxy-iPhone :   Sit-Walk -76.250 6.8028  1 125.6340     &lt;2e-16 ***\nGalaxy-iPhone : Stand-Walk -71.167 6.8028  1 109.4412     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the output, A-B : C-D is interpreted as a difference-of-differences, i.e., the difference between (A-B | C) and (A-B | D). In words, is the difference between A and B significantly different in condition C from condition D?"
  },
  {
    "objectID": "week13.html#experiments-with-interaction-effects",
    "href": "week13.html#experiments-with-interaction-effects",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.9 Experiments with interaction effects",
    "text": "13.9 Experiments with interaction effects\nThis section reports on three experiments with possible interaction effects: Avatars, Notes, and Social media value. To work through the questions, you need the three csv files containing the data: avatars.csv, notes.csv, and socialvalue.csv.\nThese experiments may be between-subjects, within-subjects, or mixed. To be a mixed factorial design, there would have to be at least two independent variables and at least one within-subjects factor and at least one between-subjects factor.\n\n13.9.1 Sentiments about Avatars among males and females (Interaction effects)\nThirty males and thirty females were shown an avatar that was either male or female and asked to write a story about that avatar. The number of positive sentiments in the story were summed. What kind of experimental design is this? [Answer: It is a \\(2\\times 2\\) between-subjects design with factors for Sex (M, F) and Avatar (M, F).]\n\navatars &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/avatars.csv\"))\navatars$Subject &lt;- factor(avatars$Subject)\nsummary(avatars)\n\n    Subject       Sex               Avatar            Positives    \n 1      : 1   Length:60          Length:60          Min.   : 32.0  \n 2      : 1   Class :character   Class :character   1st Qu.: 65.0  \n 3      : 1   Mode  :character   Mode  :character   Median : 84.0  \n 4      : 1                                         Mean   : 85.1  \n 5      : 1                                         3rd Qu.:104.2  \n 6      : 1                                         Max.   :149.0  \n (Other):54                                                        \n\n\nWhat’s the average number of positive sentiments for the most positive combination of Sex and Avatar?\n\nplyr::ddply(avatars,~Sex*Avatar,summarize,\n      Pos.mean=mean(Positives),\n      Pos.sd=sd(Positives))\n\n     Sex Avatar  Pos.mean   Pos.sd\n1 Female Female  63.13333 17.48414\n2 Female   Male  85.20000 25.31008\n3   Male Female 100.73333 18.72152\n4   Male   Male  91.33333 19.66384\n\n\nCreate an interaction plot with Sex on the X-Axis and Avatar as the traces. Do the lines cross? Do the same for reversed axes.\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(avatars,interaction.plot(Sex,Avatar,Positives,\n                  ylim=c(0,max(avatars$Positives))))\n\n\n\nwith(avatars,interaction.plot(Avatar,Sex,Positives,\n                  ylim=c(0,max(avatars$Positives))))\n\n\n\n\nConduct a factorial ANOVA on Positives by Sex and Avatar and report the largest \\(F\\)-statistic. Report which effects are significant.\n\nm&lt;-ezANOVA(dv=Positives,between=c(Sex,Avatar),\n       wid=Subject,data=avatars)\n\nWarning: Converting \"Sex\" to factor for ANOVA.\n\n\nWarning: Converting \"Avatar\" to factor for ANOVA.\n\n\nCoefficient covariances computed by hccm()\n\nm$ANOVA\n\n      Effect DFn DFd         F            p p&lt;.05        ges\n1        Sex   1  56 17.041756 0.0001228287     * 0.23331526\n2     Avatar   1  56  1.429598 0.2368686270       0.02489305\n3 Sex:Avatar   1  56  8.822480 0.0043757511     * 0.13610216\n\n\nConduct planned pairwise comparisons using independent-samples \\(t\\)-tests. Ask whether females produced different numbers of positive sentiments for male vs female avatars. Then ask whether males did the same. Assume equal variances and use Holm’s sequential Bonferroni procedure to correct for multiple comparisons.\n\nf&lt;-t.test(avatars[avatars$Sex==\"Female\" & avatars$Avatar==\"Male\",]$Positives,\n      avatars[avatars$Sex==\"Female\" & avatars$Avatar==\"Female\",]$Positives,\n      var.equal=TRUE)\nf\n\n\n    Two Sample t-test\n\ndata:  avatars[avatars$Sex == \"Female\" & avatars$Avatar == \"Male\", ]$Positives and avatars[avatars$Sex == \"Female\" & avatars$Avatar == \"Female\", ]$Positives\nt = 2.7782, df = 28, p-value = 0.009647\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  5.796801 38.336533\nsample estimates:\nmean of x mean of y \n 85.20000  63.13333 \n\nm&lt;-t.test(avatars[avatars$Sex==\"Male\" & avatars$Avatar==\"Male\",]$Positives,\n      avatars[avatars$Sex==\"Male\" & avatars$Avatar==\"Female\",]$Positives,\n      var.equal=TRUE)\nm\n\n\n    Two Sample t-test\n\ndata:  avatars[avatars$Sex == \"Male\" & avatars$Avatar == \"Male\", ]$Positives and avatars[avatars$Sex == \"Male\" & avatars$Avatar == \"Female\", ]$Positives\nt = -1.3409, df = 28, p-value = 0.1907\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -23.759922   4.959922\nsample estimates:\nmean of x mean of y \n 91.33333 100.73333 \n\np.adjust(c(f$p.value,m$p.value),method=\"holm\")\n\n[1] 0.01929438 0.19073468\n\n\n\n\n13.9.2 Writing notes with builtin or addon apps on two phones (mixed factorial design)\nThe notes.csv file describes a study in which iPhone and Android owners used a built-in note-taking app then a third-party note-taking app or vice versa. What kind of experimental design is this? (Answer: A \\(2 \\times 2\\) mixed factorial design with a between-subjects factor for Phone (iPhone, Android) and a within-subjects factor for Notes (Built-in, Add-on).)\n\nnotes &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/notes.csv\"))\nnotes$Subject&lt;-factor(notes$Subject)\nnotes$Order&lt;-factor(notes$Order)\nsummary(notes)\n\n    Subject      Phone              Notes           Order      Words      \n 1      : 2   Length:40          Length:40          1:20   Min.   :259.0  \n 2      : 2   Class :character   Class :character   2:20   1st Qu.:421.8  \n 3      : 2   Mode  :character   Mode  :character          Median :457.0  \n 4      : 2                                                Mean   :459.2  \n 5      : 2                                                3rd Qu.:518.5  \n 6      : 2                                                Max.   :598.0  \n (Other):28                                                               \n\n\nWhat’s the average number of words recorded for the most heavily used combination of Phone and Notes?\n\nplyr::ddply(notes, ~Phone*Notes,summarize,\n         Words.mean=mean(Words),Words.sd=sd(Words))\n\n    Phone    Notes Words.mean Words.sd\n1 Android   Add-on      388.1 42.38828\n2 Android Built-in      410.9 77.49043\n3  iPhone   Add-on      504.2 47.29529\n4  iPhone Built-in      533.7 48.04176\n\n\nCreate an interaction plot with Phone on the X-Axis and Notes as the traces. Do the lines cross? Do the same for reversed axes.\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(notes,interaction.plot(Phone,Notes,Words,\n                ylim=c(0,max(notes$Words))))\n\n\n\nwith(notes,interaction.plot(Notes,Phone,Words,\n                ylim=c(0,max(notes$Words))))\n\n\n\n\nTest for an order effect in the presentation of order of the Notes factor. Report the \\(p\\)-value.\n\nm&lt;-ezANOVA(dv=Words,between=Phone,within=Order,wid=Subject,data=notes)\n\nWarning: Converting \"Phone\" to factor for ANOVA.\n\nm$ANOVA\n\n       Effect DFn DFd          F            p p&lt;.05        ges\n2       Phone   1  18 43.5625695 3.375888e-06     * 0.56875437\n3       Order   1  18  0.5486763 4.684126e-01       0.01368098\n4 Phone:Order   1  18  3.0643695 9.705122e-02       0.07189858\n\n\nConduct a factorial ANOVA on Words by Phone and Notes. Report the largest \\(F\\)-statistic.\n\nm&lt;-ezANOVA(dv=Words,between=Phone,within=Notes,wid=Subject,data=notes)\n\nWarning: Converting \"Notes\" to factor for ANOVA.\n\n\nWarning: Converting \"Phone\" to factor for ANOVA.\n\nm$ANOVA\n\n       Effect DFn DFd           F            p p&lt;.05         ges\n2       Phone   1  18 43.56256949 3.375888e-06     * 0.562185697\n3       Notes   1  18  2.35976941 1.418921e-01       0.057972811\n4 Phone:Notes   1  18  0.03872717 8.461951e-01       0.001008948\n\n\nConduct paired-samples \\(t\\)-tests to answer two questions. First, did iPhone user enter different numbers of words using the built-in notes app versus the add-on notes app? Second, same for Android. Assume equal variances and use Holm’s sequential Bonferroni procedure to correct for multiple comparisons. Report the lowest adjusted \\(p\\)-value.\n\nnotes.wide&lt;-dcast(notes,Subject+Phone~Notes,value.var=\"Words\")\nhead(notes.wide)\n\n  Subject   Phone Add-on Built-in\n1       1  iPhone    464      561\n2       2 Android    433      428\n3       3  iPhone    598      586\n4       4 Android    347      448\n5       5  iPhone    478      543\n6       6 Android    365      445\n\ni&lt;-t.test(notes.wide[notes.wide$Phone==\"iPhone\",]$'Add-on',\n      notes.wide[notes.wide$Phone==\"iPhone\",]$'Built-in',\n      paired=TRUE,var.equal=TRUE)\ni\n\n\n    Paired t-test\n\ndata:  notes.wide[notes.wide$Phone == \"iPhone\", ]$\"Add-on\" and notes.wide[notes.wide$Phone == \"iPhone\", ]$\"Built-in\"\nt = -1.8456, df = 9, p-value = 0.09804\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -65.658758   6.658758\nsample estimates:\nmean difference \n          -29.5 \n\na&lt;-t.test(notes.wide[notes.wide$Phone==\"Android\",]$'Add-on',\n      notes.wide[notes.wide$Phone==\"Android\",]$'Built-in',\n      paired=TRUE,var.equal=TRUE)\na\n\n\n    Paired t-test\n\ndata:  notes.wide[notes.wide$Phone == \"Android\", ]$\"Add-on\" and notes.wide[notes.wide$Phone == \"Android\", ]$\"Built-in\"\nt = -0.75847, df = 9, p-value = 0.4676\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -90.80181  45.20181\nsample estimates:\nmean difference \n          -22.8 \n\np.adjust(c(i$p.value,a$p.value),method=\"holm\")\n\n[1] 0.1960779 0.4675674\n\n\n\n\n13.9.3 Social media value judged by people after watching clips (two-by-two within subject design)\nThe file socialvalue.csv describes a study of people viewing a pos or neg film clip then going onto social media and judging the value of the first 100 posts they see. The number of valued posts was recorded. What kind of experimental design is this? [Answer: A \\(2\\times 2\\) within-subject design with factors for Clip (positive, negative) and Social (Facebook, Twitter).]\n\nsv &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/socialvalue.csv\"))\nsv$Subject&lt;-factor(sv$Subject)\nsv$Clip&lt;-factor(sv$Clip)\nsv$Social&lt;-factor(sv$Social)\nsv$ClipOrder&lt;-factor(sv$ClipOrder)\nsv$SocialOrder&lt;-factor(sv$SocialOrder)\nsummary(sv)\n\n    Subject         Clip    ClipOrder      Social   SocialOrder     Valued    \n 1      : 4   negative:32   1:32      Facebook:32   1:32        Min.   :13.0  \n 2      : 4   positive:32   2:32      Twitter :32   2:32        1st Qu.:52.0  \n 3      : 4                                                     Median :56.0  \n 4      : 4                                                     Mean   :57.3  \n 5      : 4                                                     3rd Qu.:67.0  \n 6      : 4                                                     Max.   :95.0  \n (Other):40                                                                   \n\n\nWhat’s the average number of valued posts for the most valued combination of Clip and Social?\n\nplyr::ddply(sv, ~Clip*Social,summarize, Valued.mean=mean(Valued),\n      Valued.sd=sd(Valued))\n\n      Clip   Social Valued.mean Valued.sd\n1 negative Facebook     46.3125 22.285178\n2 negative  Twitter     55.5625  5.032809\n3 positive Facebook     68.7500 21.151832\n4 positive  Twitter     58.5625  5.656486\n\n\nCreate an interaction plot with Social on the \\(X\\)-Axis and Clip as the traces. Do the lines cross? Do the same for reversed axes.\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(sv,interaction.plot(Social,Clip,Valued,\n             ylim=c(0,max(sv$Valued))))\n\n\n\nwith(sv,interaction.plot(Clip,Social,Valued,\n             ylim=c(0,max(sv$Valued))))\n\n\n\n\nTest for an order effect in the presentation of order of the ClipOrder or SocialOrder factor. Report the \\(p\\)-values.\n\nm&lt;-ezANOVA(dv=Valued,within=c(ClipOrder,SocialOrder),wid=Subject,data=sv)\nm$ANOVA\n\n                 Effect DFn DFd          F         p p&lt;.05          ges\n2             ClipOrder   1  15 0.93707354 0.3483818       0.0253831509\n3           SocialOrder   1  15 0.81236528 0.3816660       0.0143842018\n4 ClipOrder:SocialOrder   1  15 0.01466581 0.9052172       0.0001913088\n\n\nConduct a factorial ANOVA on Valued by Clip and Social. Report the largest \\(F\\)-statistic.\n\nm&lt;-ezANOVA(dv=Valued,within=c(Clip,Social),wid=Subject,data=sv)\nm$ANOVA\n\n       Effect DFn DFd          F          p p&lt;.05          ges\n2        Clip   1  15 6.99533219 0.01837880     * 0.1469889054\n3      Social   1  15 0.01466581 0.90521722       0.0002340033\n4 Clip:Social   1  15 6.11355984 0.02586779     * 0.0914169000\n\n\nConduct paired-samples \\(t\\)-tests to answer two questions. First, on Facebook, were the number of valued posts different after watching a positive or negative clip. Second, same on Twitter. Assume equal variances and use Holm’s sequential Bonferroni procedure to correct for multiple comparisons. Report the lowest adjusted \\(p\\)-value.\n\nsv.wide&lt;-dcast(sv,Subject+Social~Clip,value.var=\"Valued\")\nhead(sv.wide)\n\n  Subject   Social negative positive\n1       1 Facebook       38       85\n2       1  Twitter       52       53\n3       2 Facebook       73       25\n4       2  Twitter       52       54\n5       3 Facebook       25       95\n6       3  Twitter       53       70\n\nf&lt;-t.test(sv.wide[sv.wide$Social==\"Facebook\",]$positive,\n      sv.wide[sv.wide$Social==\"Facebook\",]$negative,\n      paired=TRUE,var.equal=TRUE)\nf\n\n\n    Paired t-test\n\ndata:  sv.wide[sv.wide$Social == \"Facebook\", ]$positive and sv.wide[sv.wide$Social == \"Facebook\", ]$negative\nt = 2.5929, df = 15, p-value = 0.02039\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n  3.993 40.882\nsample estimates:\nmean difference \n        22.4375 \n\nt&lt;-t.test(sv.wide[sv.wide$Social==\"Twitter\",]$positive,\n      sv.wide[sv.wide$Social==\"Twitter\",]$negative,\n      paired=TRUE,var.equal=TRUE)\nt\n\n\n    Paired t-test\n\ndata:  sv.wide[sv.wide$Social == \"Twitter\", ]$positive and sv.wide[sv.wide$Social == \"Twitter\", ]$negative\nt = 1.9926, df = 15, p-value = 0.06482\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.2089939  6.2089939\nsample estimates:\nmean difference \n              3 \n\np.adjust(c(f$p.value,t$p.value),method=\"holm\")\n\n[1] 0.04077153 0.06482275\n\n\nConduct a nonparametric Aligned Rank Transform Procedure on Valued by Clip and Social.\n\nm&lt;-art(Valued~Clip*Social+(1|Subject),data=sv)\nanova(m)\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nAnalysis of Variance of Aligned Rank Transformed Data\n\nTable Type: Analysis of Deviance Table (Type III Wald F tests with Kenward-Roger df) \nModel: Mixed Effects (lmer)\nResponse: art(Valued)\n\n                     F Df Df.res     Pr(&gt;F)    \n1 Clip        17.13224  1     45 0.00015089 ***\n2 Social       0.49281  1     45 0.48629341    \n3 Clip:Social 11.31751  1     45 0.00157736  **\n---\nSignif. codes:   0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\n\nConduct interaction contrasts to discover whether the difference on Facebook was itself different from the difference on Twitter. Report the \\(\\chi^2\\) statistic.\n\ntestInteractions(artlm(m,\"Clip:Social\"),\n         pairwise=c(\"Clip\",\"Social\"),adjustment=\"holm\")\n\nboundary (singular) fit: see help('isSingular')\n\n\nChisq Test: \nP-value adjustment method: holm\n                                      Value     SE Df  Chisq Pr(&gt;Chisq)    \nnegative-positive : Facebook-Twitter -29.25 8.6946  1 11.318  0.0007678 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "week13.html#what-if-errors-are-not-normally-distributed-generalized-linear-models",
    "href": "week13.html#what-if-errors-are-not-normally-distributed-generalized-linear-models",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.10 What if errors are not normally distributed? (Generalized linear models)",
    "text": "13.10 What if errors are not normally distributed? (Generalized linear models)\nHere are three examples of generalized linear models. The first is analyzed using nominal logistic regression, the second is analyzed via ordinal logistic regression, and the third is analyzed via Poisson regression.\nAs Wikipedia tells us, a generalized linear model or GLM is a flexible generalization of ordinary linear regression that allows for response variables with error distribution models other than a normal distribution. There is also something called a general linear model but it is not the same thing as a generalized linear model. It is just the general form of the ordinary linear regression model: \\(\\mathbfit{Y=X\\beta+\\epsilon}\\).\nGLMs that we examine here are good for between-subjects studies so we’ll actually recode one of our fictitious data sets to be between subjects just to have an example to use.\n\n13.10.1 Preferences among websites by males and females (GLM 1: Nominal logistic regression for preference responses)\n\n\n13.10.2 Multinomial distribution with logit link function\nThe prefsABCsex.csv file records preferences among three websites A, B, and C expressed by males and females. The subject number, preference and sex were recorded.\nThe logit link function is the log odds function, generally \\(\\text{logit}(p)=\\ln \\frac{p}{1-p}\\), where \\(p\\) is the probability of an event such as choosing website A. The form of the link function is \\(\\mathbfit{X\\beta}=\\ln\\frac{\\mu}{1-\\mu}\\). This is just the relationship of a matrix of predictors times a vector of parameters \\(\\mathbfit{\\beta}\\) to the logit of the mean of the distribution.\n\nprefsABCsex &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/prefsABCsex.csv\"))\nhead(prefsABCsex)\n\n# A tibble: 6 × 3\n  Subject Pref  Sex  \n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1       1 C     F    \n2       2 C     M    \n3       3 B     M    \n4       4 C     M    \n5       5 C     M    \n6       6 B     F    \n\nprefsABCsex$Subject&lt;-factor(prefsABCsex$Subject)\nprefsABCsex$Sex&lt;-factor(prefsABCsex$Sex)\nsummary(prefsABCsex)\n\n    Subject       Pref           Sex   \n 1      : 1   Length:60          F:29  \n 2      : 1   Class :character   M:31  \n 3      : 1   Mode  :character         \n 4      : 1                            \n 5      : 1                            \n 6      : 1                            \n (Other):54                            \n\nggplot(prefsABCsex[prefsABCsex$Sex == \"M\",],aes(Pref)) +\n  theme_tufte() +\n  geom_bar(width=0.25,fill=\"gray\") +\n  geom_hline(yintercept=seq(0, 20, 5), col=\"white\", lwd=1) +\n  annotate(\"text\", x = 1.5, y = 18, adj=1,  family=\"serif\",\n    label = c(\"Males prefer\\nwebsite C\"))\n\n\n\nggplot(prefsABCsex[prefsABCsex$Sex == \"F\",],aes(Pref)) +\n  theme_tufte() +\n  geom_bar(width=0.25,fill=\"gray\") +\n  geom_hline(yintercept=seq(0, 20, 5), col=\"white\", lwd=1) +\n  annotate(\"text\", x = 1.5, y = 18, adj=1,  family=\"serif\",\n    label = c(\"Females dislike\\nwebsite A\"))\n\n\n\n\nThese histograms lead us to suspect that C is preferred by males and that A is disliked by females, but we should still run tests to be convinced that the variability observed is not due to chance.\nAnalyze Pref by Sex using multinomial logistic regression, aka nominal logistic regression. Here we are testing for whether there is a difference between the sexes regarding their preferences.\nThe annotation type=3 is borrowed from SAS and refers to one of three ways of handling an unbalanced design. This experimental design is unbalanced because there are more males than females being tested. This way of handling the unbalanced design is only valid if there are significant interactions, as hinted by the gross differences between the preceding histograms.\n\nlibrary(nnet) # provides multinom()\n#. library(car) # provides Anova()\n#. set sum-to-zero contrasts for the Anova call\ncontrasts(prefsABCsex$Sex) &lt;- \"contr.sum\"\nm&lt;-multinom(Pref~Sex, data=prefsABCsex)\n\n# weights:  9 (4 variable)\ninitial  value 65.916737 \niter  10 value 55.099353\niter  10 value 55.099353\nfinal  value 55.099353 \nconverged\n\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Pref\n    LR Chisq Df Pr(&gt;Chisq)  \nSex   7.0744  2    0.02909 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Analysis of Deviance table tells us that there is a significant main effect for Sex. It does not tell us more detail but motivates pairwise tests to get more detail. If there were no significant effect, pairwise tests would not be warranted.\nPairwise tests tell which of the bins are over or under populated based on the assumption that each bin should contain one third of the observations (hence p=1/3). When making multiple comparisons we would overstate the significance of the differences so we use Holm’s sequential Bonferroni procedure to correct this.\n\nma&lt;-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"A\"),\n           nrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\nmb&lt;-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"B\"),\n           nrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\nmc&lt;-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"C\"),\n           nrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\n#. correct for multiple comparisons\np.adjust(c(ma$p.value, mb$p.value, mc$p.value), method=\"holm\")\n\n[1] 0.109473564 0.126622172 0.001296754\n\nfa&lt;-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"A\"),\n           nrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\nfb&lt;-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"B\"),\n           nrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\nfc&lt;-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"C\"),\n           nrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\n#. correct for multiple comparisons\np.adjust(c(fa$p.value, fb$p.value, fc$p.value), method=\"holm\")\n\n[1] 0.02703274 0.09447821 0.69396951\n\n\nThe preceding tests confirm what we suspected from looking at histograms: males prefer C and females dislike A. We see this by looking at the adjusted \\(p\\)-values, where the first row, third value is significant and the second row, first value is significant.\nHow would we write this up in a report? We could make the following claim. We tested the main effect for sex and found a significant result, \\(\\chi^2_2=7.1, p&lt;0.05\\). An exact binomial test found the preference among males for website C greater than chance, \\(p&lt;0.01\\). An exact binomial test found the preference among females against website A greater than chance, \\(p&lt;0.05\\). No other significant differences were found.\n\n\n13.10.3 Judgments of perceived effort (GLM 2: Ordinal logistic regression for Likert responses)\n\n\n13.10.4 Multinomial distribution with cumulative logit link function\nIn this example, users are either searching, scrolling or using voice to find contacts in a smartphone address book. The time it takes to find a certain number of contacts, the perceived effort, and the number of errors are all recorded. Of interest now is the perceived effort, recorded on a Likert scale. A Likert scale can not be normally distributed because of the restrictions on the ends and is not likely to even look vaguely normal.\nThe cumulative logit link function is like the logit link function:\n\\[\\text{logit}(P(Y\\leqslant j|x))=\\ln\\frac{P(Y\\leqslant j|x)}{1-P(Y\\leqslant j|x)} \\text{ where }Y=1,2,\\ldots,J\\]\nIn this case \\(J\\) ranges from 1 to 7.\nRead in the data and examine it. We see that it is a within-subjects study but it is a fictitious study anyway so we will recode it as if it were a between-subjects study. Then we will be able to apply the following techniques, which we would have to modify for a within-subjects study.\n\nsrchscrlvce &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/srchscrlvce.csv\"))\nhead(srchscrlvce)\n\n# A tibble: 6 × 6\n  Subject Technique Order  Time Errors Effort\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1       1 Search        1    98      4      5\n2       1 Scroll        2   152      0      6\n3       2 Search        2    57      2      2\n4       2 Scroll        1   148      0      3\n5       3 Search        1    86      3      2\n6       3 Scroll        2   160      0      4\n\nsrchscrlvce$Subject&lt;-(1:nrow(srchscrlvce)) # recode as between-subjects\nsrchscrlvce$Subject&lt;-factor(srchscrlvce$Subject)\nsrchscrlvce$Technique&lt;-factor(srchscrlvce$Technique)\nsrchscrlvce$Order&lt;-NULL # drop order, n/a for between-subjects\nhead(srchscrlvce) # verify\n\n# A tibble: 6 × 5\n  Subject Technique  Time Errors Effort\n  &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1       Search       98      4      5\n2 2       Scroll      152      0      6\n3 3       Search       57      2      2\n4 4       Scroll      148      0      3\n5 5       Search       86      3      2\n6 6       Scroll      160      0      4\n\nsummary(srchscrlvce)\n\n    Subject    Technique       Time           Errors         Effort    \n 1      : 1   Scroll:20   Min.   : 49.0   Min.   :0.00   Min.   :1.00  \n 2      : 1   Search:20   1st Qu.: 86.0   1st Qu.:1.00   1st Qu.:3.00  \n 3      : 1   Voice :20   Median : 97.0   Median :2.00   Median :4.00  \n 4      : 1               Mean   :106.2   Mean   :2.75   Mean   :4.15  \n 5      : 1               3rd Qu.:128.0   3rd Qu.:4.00   3rd Qu.:5.00  \n 6      : 1               Max.   :192.0   Max.   :9.00   Max.   :7.00  \n (Other):54                                                            \n\n\nA good description of Effort is the median and quantiles. Another good description is the mean and standard deviation.\n\nplyr::ddply(srchscrlvce, ~ Technique,\n       function(data) summary(data$Effort))\n\n  Technique Min. 1st Qu. Median Mean 3rd Qu. Max.\n1    Scroll    1    3.00      4 4.40    6.00    7\n2    Search    1    3.00      4 3.60    4.25    5\n3     Voice    1    3.75      5 4.45    5.25    6\n\nplyr::ddply(srchscrlvce, ~ Technique,\n       summarize, Effort.mean=mean(Effort), Effort.sd=sd(Effort))\n\n  Technique Effort.mean Effort.sd\n1    Scroll        4.40  1.698296\n2    Search        3.60  1.187656\n3     Voice        4.45  1.356272\n\npar(cex=0.6)\nggplot(srchscrlvce,aes(Effort,fill=Technique)) +\n  geom_histogram(bins=7,alpha=0.8,position=\"dodge\") +\n  scale_color_grey() +\n  scale_fill_grey() +\n  theme_tufte()\n\n\n\nggplot(srchscrlvce,aes(Technique,Effort,fill=Technique)) +\n  geom_tufteboxplot(show.legend=FALSE) +\n  theme_tufte()\n\nWarning: The following aesthetics were dropped during statistical transformation: y\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\nWarning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\nThe boxplots (these are Tufte-style boxplots) are not encouraging. We may not find a significant difference among these three techniques but let us try anyway. We analyze Effort Likert ratings by Technique using ordinal logistic regression.\n\n#. library(MASS) # provides polr()\n#. library(car) # provides Anova()\nsrchscrlvce$Effort &lt;- ordered(srchscrlvce$Effort)\n#. set sum-to-zero contrasts for the Anova call\ncontrasts(srchscrlvce$Technique) &lt;- \"contr.sum\"\nm &lt;- polr(Effort ~ Technique, data=srchscrlvce, Hess=TRUE) # ordinal logistic\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Effort\n          LR Chisq Df Pr(&gt;Chisq)\nTechnique   4.5246  2     0.1041\n\n\nPost hoc pairwise comparisons are NOT justified due to lack of significance but here’s how we would do them, just for completeness. Tukey means to compare all pairs and holm is the adjustment due to the double-counting that overstates the significance.\n\nsummary(glht(m, mcp(Technique=\"Tukey\")), test=adjusted(type=\"holm\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: polr(formula = Effort ~ Technique, data = srchscrlvce, Hess = TRUE)\n\nLinear Hypotheses:\n                      Estimate Std. Error z value Pr(&gt;|z|)\nSearch - Scroll == 0 -1.016610   0.584614  -1.739    0.191\nVoice - Scroll == 0   0.007397   0.587700   0.013    0.990\nVoice - Search == 0   1.024007   0.552298   1.854    0.191\n(Adjusted p values reported -- holm method)\n\n\nHow would we express this in a report? We would simply say that we found no significant differences between the three techniques.\n\n\n13.10.5 Counting errors in a task (GLM 3: Poisson regression for count responses)\n\n\n13.10.6 Poisson distribution with log link function\nUsing the same data but now focus on the Errors column instead of effort. Errors likely have a Poisson distribution. The log link function is just \\(\\mathbfit{X\\beta}=\\ln(\\mu)\\) rather than the more elaborate logit link function we saw before.\n\nplyr::ddply(srchscrlvce, ~ Technique,\n             function(data) summary(data$Errors))\n\n  Technique Min. 1st Qu. Median Mean 3rd Qu. Max.\n1    Scroll    0    0.00    0.5 0.70    1.00    2\n2    Search    1    2.00    2.5 2.50    3.00    4\n3     Voice    2    3.75    5.0 5.05    6.25    9\n\nplyr::ddply(srchscrlvce, ~ Technique, summarize,\n             Errors.mean=mean(Errors), Errors.sd=sd(Errors))\n\n  Technique Errors.mean Errors.sd\n1    Scroll        0.70 0.8013147\n2    Search        2.50 1.0513150\n3     Voice        5.05 1.9049796\n\npar(cex=0.6)\nggplot(srchscrlvce,aes(Errors,fill=Technique)) +\n  geom_histogram(bins=9,alpha=0.8,position=\"dodge\") +\n  scale_color_grey() +\n  scale_fill_grey() +\n  theme_tufte()\n\n\n\nggplot(srchscrlvce,aes(Technique,Errors,fill=Technique)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Greens\") +\n  theme_tufte()\n\n\n\n\nThese boxplots are very encouraging. There appears to be a clear difference between all three of these techniques. Notice that you could draw horizontal lines across the plot without intersecting the boxes. That represents a high degree of separation.\nNow verify that these data are Poisson-distributed with a goodness-of-fit test for each technique. If the results are not significant, we expect that the data do not deviate significantly from what we would expect of a Poisson distribution.\n\n#. library(fitdistrplus)\nfit&lt;-fitdist(srchscrlvce[srchscrlvce$Technique == \"Search\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n\nChi-squared statistic:  1.522231 \nDegree of freedom of the Chi-squared distribution:  2 \nChi-squared p-value:  0.4671449 \n   the p-value may be wrong with some theoretical counts &lt; 5  \nChi-squared table:\n     obscounts theocounts\n&lt;= 1  4.000000   5.745950\n&lt;= 2  6.000000   5.130312\n&lt;= 3  6.000000   4.275260\n&gt; 3   4.000000   4.848477\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   65.61424\nBayesian Information Criterion   66.60997\n\nfit&lt;-fitdist(srchscrlvce[srchscrlvce$Technique == \"Scroll\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n\nChi-squared statistic:  0.3816087 \nDegree of freedom of the Chi-squared distribution:  1 \nChi-squared p-value:  0.5367435 \n   the p-value may be wrong with some theoretical counts &lt; 5  \nChi-squared table:\n     obscounts theocounts\n&lt;= 0 10.000000   9.931706\n&lt;= 1  6.000000   6.952194\n&gt; 1   4.000000   3.116100\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   45.53208\nBayesian Information Criterion   46.52781\n\nfit&lt;-fitdist(srchscrlvce[srchscrlvce$Technique == \"Voice\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n\nChi-squared statistic:  0.1611327 \nDegree of freedom of the Chi-squared distribution:  3 \nChi-squared p-value:  0.9836055 \n   the p-value may be wrong with some theoretical counts &lt; 5  \nChi-squared table:\n     obscounts theocounts\n&lt;= 3  5.000000   5.161546\n&lt;= 4  4.000000   3.473739\n&lt;= 5  3.000000   3.508476\n&lt;= 6  3.000000   2.952967\n&gt; 6   5.000000   4.903272\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   84.19266\nBayesian Information Criterion   85.18839\n\n\nAll three of the above goodness of fit tests tell us that there is no evidence of deviation from a Poisson distribution. Since we are now convinced of the Poisson distribution for each of the three techniques, analyze the errors using Poisson regression.\nWe’ve been saying “set sum-to-zero contrasts for the Anova call” but what does that mean? Contrasts are linear combinations used in ANOVA. As Wikipedia defines it, a contrast is a linear combination \\(\\sum^t_{i=1}a_i\\theta_i\\), where each \\(\\theta_i\\) is a statistic and the \\(a_i\\) values sum to zero. Typically, the \\(a_i\\) values are \\(1\\) and \\(-1\\). A simple contrast represents a difference between means and is used in ANOVA. In R, they are invisible if you use Type I ANOVA, but have to be specified as follows if using a Type III ANOVA. The default anova() function is Type I but we’re using Type III, available from the Anova() function in the car package.\nA minor detail is that we don’t really need to use Anova() here instead of anova() because the study is balanced, meaning that it has the same number of observations in each condition. The only reason for using Anova() on this data is that it gives a better-looking output. The anova() function would just display the \\(\\chi^2\\) statistic without the associated \\(p\\)-value.\n\ncontrasts(srchscrlvce$Technique) &lt;- \"contr.sum\"\n#. family parameter identifies both distribution and link fn\nm &lt;- glm(Errors ~ Technique, data=srchscrlvce, family=poisson)\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Errors\n          LR Chisq Df Pr(&gt;Chisq)    \nTechnique    74.93  2  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBecause the Analysis of Deviance table shows a significant \\(\\chi^2\\) value and corresponding \\(p\\)-value, we are justified to conduct pairwise comparisons among levels of Technique.\n\n#. library(multcomp)\nsummary(glht(m, mcp(Technique=\"Tukey\")), test=adjusted(type=\"holm\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: glm(formula = Errors ~ Technique, family = poisson, data = srchscrlvce)\n\nLinear Hypotheses:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \nSearch - Scroll == 0   1.2730     0.3024   4.210 5.11e-05 ***\nVoice - Scroll == 0    1.9761     0.2852   6.929 1.27e-11 ***\nVoice - Search == 0    0.7031     0.1729   4.066 5.11e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\nWe see from the table that all three differences are significant. We could have guessed this result from glancing at the boxplot above, but it is valuable to have statistical evidence that this is not a chance difference."
  },
  {
    "objectID": "week13.html#more-experiments-without-normally-distributed-errors-more-generalized-linear-models",
    "href": "week13.html#more-experiments-without-normally-distributed-errors-more-generalized-linear-models",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.11 More experiments without normally distributed errors (More generalized linear models)",
    "text": "13.11 More experiments without normally distributed errors (More generalized linear models)\n\n13.11.1 Preference between touchpads vs trackballs by non / disabled people and males / females\nThis study examines whether participants of either sex with or without a disability prefer touchpads or trackballs. Start by examining the data and determining how many participants are involved.\n\ndps &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/deviceprefssex.csv\"))\ndps$Subject&lt;-as.factor(dps$Subject)\ndps$Disability&lt;-as.factor(dps$Disability)\ndps$Sex&lt;-as.factor(dps$Sex)\ndps$Pref&lt;-as.factor(dps$Pref)\nsummary(dps)\n\n    Subject   Disability Sex           Pref   \n 1      : 1   0:18       F:15   touchpad :21  \n 2      : 1   1:12       M:15   trackball: 9  \n 3      : 1                                   \n 4      : 1                                   \n 5      : 1                                   \n 6      : 1                                   \n (Other):24                                   \n\n\nUse binomial regression to examine Pref by Disability and Sex. Report the \\(p\\)-value of the interaction of Disability\\(\\times\\)Sex.\n\ncontrasts(dps$Disability) &lt;- \"contr.sum\"\ncontrasts(dps$Sex) &lt;- \"contr.sum\"\nm&lt;-glm(Pref ~ Disability*Sex, data=dps, family=binomial)\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Pref\n               LR Chisq Df Pr(&gt;Chisq)   \nDisability      10.4437  1   0.001231 **\nSex              2.8269  1   0.092695 . \nDisability:Sex   0.6964  1   0.403997   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow use multinomial regression for the same task and report the corresponding \\(p\\)-value.\n\n#. library(nnet)\ncontrasts(dps$Disability) &lt;- \"contr.sum\"\ncontrasts(dps$Sex) &lt;- \"contr.sum\"\nm&lt;-multinom(Pref~Disability*Sex, data=dps)\n\n# weights:  5 (4 variable)\ninitial  value 20.794415 \niter  10 value 13.023239\niter  20 value 13.010200\nfinal  value 13.010184 \nconverged\n\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Pref\n               LR Chisq Df Pr(&gt;Chisq)   \nDisability      10.4434  1   0.001231 **\nSex              2.8267  1   0.092710 . \nDisability:Sex   0.6961  1   0.404087   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow conduct post-hoc binomial tests for each Disability\\(\\times\\)Sex combination.\n\nm0&lt;-binom.test(sum(dps[dps$Sex == \"M\" & dps$Disability == \"0\",]$Pref == \"touchpad\"),\n               nrow(dps[dps$Sex == \"M\" & dps$Disability == \"0\",]),p=1/2)\nm1&lt;-binom.test(sum(dps[dps$Sex == \"M\" & dps$Disability == \"1\",]$Pref == \"touchpad\"),\n               nrow(dps[dps$Sex == \"M\" & dps$Disability == \"1\",]),p=1/2)\n\nf0&lt;-binom.test(sum(dps[dps$Sex == \"F\" & dps$Disability == \"0\",]$Pref == \"touchpad\"),\n               nrow(dps[dps$Sex == \"F\" & dps$Disability == \"0\",]),p=1/2)\nf1&lt;-binom.test(sum(dps[dps$Sex == \"F\" & dps$Disability == \"1\",]$Pref == \"touchpad\"),\n               nrow(dps[dps$Sex == \"F\" & dps$Disability == \"1\",]),p=1/2)\n\np.adjust(c(m0$p.value, m1$p.value, f0$p.value,f1$p.value), method=\"holm\")\n\n[1] 0.0625000 1.0000000 0.1962891 1.0000000\n\n\n\n\n13.11.2 Handwriting recognition speed between different tools and right-handed vs left-handed people\nThis study examined three handwriting recognizers, A, B, and C and participants who are either right-handed or left-handed. The response is the number of incorrectly recognized handwritten words out of every 100 handwritten words. Examine the data and tell how many participants were involved.\n\nhw &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/hwreco.csv\"))\nhw$Subject&lt;-factor(hw$Subject)\nhw$Recognizer&lt;-factor(hw$Recognizer)\nhw$Hand&lt;-factor(hw$Hand)\nsummary(hw)\n\n    Subject   Recognizer    Hand        Errors     \n 1      : 1   A:17       Left :25   Min.   : 1.00  \n 2      : 1   B:17       Right:25   1st Qu.: 3.00  \n 3      : 1   C:16                  Median : 4.00  \n 4      : 1                         Mean   : 4.38  \n 5      : 1                         3rd Qu.: 6.00  \n 6      : 1                         Max.   :11.00  \n (Other):44                                        \n\n\nCreate an interaction plot of Recognizer on the \\(x\\)-axis and Hand as the traces and tell how many times the traces cross.\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(hw,interaction.plot(Recognizer,Hand,Errors,\n                  ylim=c(0,max(hw$Errors))))\n\n\n\n\nTest whether the Errors of each Recognizer fit a Poisson distribution. First fit the Poisson distribution using fitdist(), then test the fit using gofstat(). The null hypothesis of this test is that the data do not deviate from a Poisson distribution.\n\n#. library(fitdistrplus)\nfit&lt;-fitdist(hw[hw$Recognizer == \"A\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n\nChi-squared statistic:  1.807852 \nDegree of freedom of the Chi-squared distribution:  2 \nChi-squared p-value:  0.4049767 \n   the p-value may be wrong with some theoretical counts &lt; 5  \nChi-squared table:\n     obscounts theocounts\n&lt;= 2  4.000000   3.627277\n&lt;= 3  5.000000   3.168895\n&lt;= 5  4.000000   6.072436\n&gt; 5   4.000000   4.131392\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   75.86792\nBayesian Information Criterion   76.70113\n\nfit&lt;-fitdist(hw[hw$Recognizer == \"B\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n\nChi-squared statistic:  0.9192556 \nDegree of freedom of the Chi-squared distribution:  2 \nChi-squared p-value:  0.6315187 \n   the p-value may be wrong with some theoretical counts &lt; 5  \nChi-squared table:\n     obscounts theocounts\n&lt;= 3  5.000000   3.970124\n&lt;= 5  4.000000   5.800601\n&lt;= 6  3.000000   2.588830\n&gt; 6   5.000000   4.640444\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   78.75600\nBayesian Information Criterion   79.58921\n\nfit&lt;-fitdist(hw[hw$Recognizer == \"C\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n\nChi-squared statistic:  0.3521272 \nDegree of freedom of the Chi-squared distribution:  2 \nChi-squared p-value:  0.8385647 \n   the p-value may be wrong with some theoretical counts &lt; 5  \nChi-squared table:\n     obscounts theocounts\n&lt;= 2  5.000000   4.600874\n&lt;= 3  4.000000   3.347372\n&lt;= 4  3.000000   3.085858\n&gt; 4   4.000000   4.965897\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   70.89042\nBayesian Information Criterion   71.66301\n\n\nNow use Poisson regression to examine Errors by Recommender and Hand. Report the \\(p\\)-value for the Recognizer\\(\\times\\)Hand interaction.\n\n#. library(car)\ncontrasts(hw$Recognizer) &lt;- \"contr.sum\"\ncontrasts(hw$Hand) &lt;- \"contr.sum\"\nm&lt;-glm(Errors ~ Recognizer*Hand, data=hw, family=poisson)\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Errors\n                LR Chisq Df Pr(&gt;Chisq)   \nRecognizer        4.8768  2   0.087299 . \nHand              3.1591  1   0.075504 . \nRecognizer:Hand  12.9682  2   0.001528 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConduct planned comparisons between left and right errors for each recognizer. Using glht() and lsm() will give all comparisons and we only want three so don’t correct for multiple comparisons automatically. That would overcorrect. Instead, extract the three relevant \\(p\\)-values manually and and use p.adjust() to correct for those.\n\n#. library(multcomp) # for glht\n#. library(lsmeans) # for lsm\nsummary(glht(m, lsm(pairwise ~ Recognizer * Hand)),\n        test=adjusted(type=\"none\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: glm(formula = Errors ~ Recognizer * Hand, family = poisson, data = hw)\n\nLinear Hypotheses:\n                         Estimate Std. Error z value Pr(&gt;|z|)    \nA Left - B Left == 0   -8.938e-01  2.611e-01  -3.423 0.000619 ***\nA Left - C Left == 0   -2.231e-01  3.000e-01  -0.744 0.456990    \nA Left - A Right == 0  -8.183e-01  2.638e-01  -3.102 0.001925 ** \nA Left - B Right == 0  -5.306e-01  2.818e-01  -1.883 0.059702 .  \nA Left - C Right == 0  -5.306e-01  2.818e-01  -1.883 0.059702 .  \nB Left - C Left == 0    6.707e-01  2.412e-01   2.780 0.005428 ** \nB Left - A Right == 0   7.551e-02  1.944e-01   0.388 0.697704    \nB Left - B Right == 0   3.632e-01  2.182e-01   1.665 0.095955 .  \nB Left - C Right == 0   3.632e-01  2.182e-01   1.665 0.095955 .  \nC Left - A Right == 0  -5.952e-01  2.441e-01  -2.438 0.014779 *  \nC Left - B Right == 0  -3.075e-01  2.635e-01  -1.167 0.243171    \nC Left - C Right == 0  -3.075e-01  2.635e-01  -1.167 0.243171    \nA Right - B Right == 0  2.877e-01  2.214e-01   1.299 0.193822    \nA Right - C Right == 0  2.877e-01  2.214e-01   1.299 0.193822    \nB Right - C Right == 0  3.331e-16  2.425e-01   0.000 1.000000    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- none method)\n\np.adjust(c(0.001925,0.095955,0.243171),method=\"holm\")\n\n[1] 0.005775 0.191910 0.243171\n\n\nThe above analyses suggest that the error counts were Poisson-distributed. The above analyses suggest that there was a significant Recognizer\\(\\times\\)Hand interaction. The above analyses suggest that for recognizer A, there were significantly more errors for right-handed participants than for left-handed participants.\n\n\n13.11.3 Ease of booking international or domestic flights on three different services\nThis study describes flight bookings using one of three services, Expedia, Orbitz, or Priceline. Each booking was either International or Domestic and the Ease of each interaction was recorded on a 7 point Likert scale where 7 was easiest. Examine the data and determine the number of participants in the study.\n\nbf &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/bookflights.csv\"))\nbf$Subject&lt;-factor(bf$Subject)\nbf$International&lt;-factor(bf$International)\nbf$Website&lt;-factor(bf$Website)\nbf$International&lt;-factor(bf$International)\nbf$Ease&lt;-as.ordered(bf$Ease)\nsummary(bf)\n\n    Subject         Website    International Ease   \n 1      :  1   Expedia  :200   0:300         1: 87  \n 2      :  1   Orbitz   :200   1:300         2: 58  \n 3      :  1   Priceline:200                 3:108  \n 4      :  1                                 4:107  \n 5      :  1                                 5: 95  \n 6      :  1                                 6: 71  \n (Other):594                                 7: 74  \n\n\nDraw an interaction plot with Website on the x-axis and International as the traces. Determine how many times the traces cross.\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(bf,interaction.plot(Website,International,as.numeric(Ease),\n                  ylim=c(0,max(as.numeric(bf$Ease)))))\n\n\n\n\nUse ordinal logistic regression to examine Ease by Website and International. Report the \\(p\\)-value of the Website main effect.\n\n#. library(MASS) # provides polr()\n#. library(car) # provides Anova()\n#. set sum-to-zero contrasts for the Anova call\ncontrasts(bf$Website) &lt;- \"contr.sum\"\ncontrasts(bf$International) &lt;- \"contr.sum\"\nm &lt;- polr(Ease ~ Website*International, data=bf, Hess=TRUE)\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Ease\n                      LR Chisq Df Pr(&gt;Chisq)    \nWebsite                  6.811  2    0.03319 *  \nInternational            0.668  1    0.41383    \nWebsite:International   90.590  2    &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConduct three pairwise comparisons of Ease between domestic and international for each service. Report the largest adjusted \\(p\\)-value. Use the same technique as above where you extracted the relevant unadjusted \\(p\\)-values manually and used p.adjust() to adjust them.\n\nsummary(as.glht(pairs(lsmeans(m, pairwise ~ Website * International))),\n        test=adjusted(type=\"none\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nLinear Hypotheses:\n                                                         Estimate Std. Error\nExpedia International0 - Orbitz International0 == 0       -2.1442     0.2619\nExpedia International0 - Priceline International0 == 0    -0.9351     0.2537\nExpedia International0 - Expedia International1 == 0      -1.6477     0.2570\nExpedia International0 - Orbitz International1 == 0       -0.3217     0.2490\nExpedia International0 - Priceline International1 == 0    -0.7563     0.2517\nOrbitz International0 - Priceline International0 == 0      1.2091     0.2555\nOrbitz International0 - Expedia International1 == 0        0.4965     0.2505\nOrbitz International0 - Orbitz International1 == 0         1.8225     0.2571\nOrbitz International0 - Priceline International1 == 0      1.3879     0.2546\nPriceline International0 - Expedia International1 == 0    -0.7126     0.2518\nPriceline International0 - Orbitz International1 == 0      0.6134     0.2497\nPriceline International0 - Priceline International1 == 0   0.1789     0.2501\nExpedia International1 - Orbitz International1 == 0        1.3260     0.2524\nExpedia International1 - Priceline International1 == 0     0.8914     0.2506\nOrbitz International1 - Priceline International1 == 0     -0.4345     0.2477\n                                                         z value Pr(&gt;|z|)    \nExpedia International0 - Orbitz International0 == 0       -8.189 2.22e-16 ***\nExpedia International0 - Priceline International0 == 0    -3.686 0.000228 ***\nExpedia International0 - Expedia International1 == 0      -6.411 1.44e-10 ***\nExpedia International0 - Orbitz International1 == 0       -1.292 0.196380    \nExpedia International0 - Priceline International1 == 0    -3.004 0.002663 ** \nOrbitz International0 - Priceline International0 == 0      4.732 2.22e-06 ***\nOrbitz International0 - Expedia International1 == 0        1.982 0.047498 *  \nOrbitz International0 - Orbitz International1 == 0         7.089 1.35e-12 ***\nOrbitz International0 - Priceline International1 == 0      5.452 4.99e-08 ***\nPriceline International0 - Expedia International1 == 0    -2.830 0.004659 ** \nPriceline International0 - Orbitz International1 == 0      2.457 0.014023 *  \nPriceline International0 - Priceline International1 == 0   0.715 0.474476    \nExpedia International1 - Orbitz International1 == 0        5.254 1.49e-07 ***\nExpedia International1 - Priceline International1 == 0     3.557 0.000375 ***\nOrbitz International1 - Priceline International1 == 0     -1.754 0.079408 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- none method)\n\np.adjust(c(1.44e-10,1.35e-12,0.474476),method=\"holm\")\n\n[1] 2.88000e-10 4.05000e-12 4.74476e-01\n\n\nThe above analyses indicate a significant main effect of Website on Ease. The above analyses indicate a significant interaction between Website and International. Expedia was perceived as significantly easier for booking international flights than domestic flights. Orbitz, on the other hand, was perceived as significantly easier for booking domestic flights than international flights."
  },
  {
    "objectID": "week13.html#same-person-using-different-tools-within-subjects-studies",
    "href": "week13.html#same-person-using-different-tools-within-subjects-studies",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.12 Same person using different tools (Within subjects studies)",
    "text": "13.12 Same person using different tools (Within subjects studies)\n\n13.12.1 Two search engines compared\n\nws &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/websearch2.csv\"))\n\nHow many subjects took part in this study?\n\nws$Subject&lt;-factor(ws$Subject)\nws$Engine&lt;-factor(ws$Engine)\nsummary(ws)\n\n    Subject      Engine       Order        Searches         Effort    \n 1      : 2   Bing  :30   Min.   :1.0   Min.   : 89.0   Min.   :1.00  \n 2      : 2   Google:30   1st Qu.:1.0   1st Qu.:135.8   1st Qu.:2.00  \n 3      : 2               Median :1.5   Median :156.5   Median :4.00  \n 4      : 2               Mean   :1.5   Mean   :156.9   Mean   :3.90  \n 5      : 2               3rd Qu.:2.0   3rd Qu.:175.2   3rd Qu.:5.25  \n 6      : 2               Max.   :2.0   Max.   :241.0   Max.   :7.00  \n (Other):48                                                           \n\ntail(ws)\n\n# A tibble: 6 × 5\n  Subject Engine Order Searches Effort\n  &lt;fct&gt;   &lt;fct&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 28      Google     2      131      4\n2 28      Bing       1      192      4\n3 29      Google     1      162      5\n4 29      Bing       2      163      3\n5 30      Google     2      146      5\n6 30      Bing       1      137      2\n\n\nThirty subjects participated.\nWhat is the average number of searches for the engine with the largest average number of searches?\n\nws |&gt;\n  group_by(Engine) |&gt;\n  summarize(avg=mean(Searches))\n\n# A tibble: 2 × 2\n  Engine   avg\n  &lt;fct&gt;  &lt;dbl&gt;\n1 Bing    166.\n2 Google  148.\n\n\nBing had 166 searches on average.\nWhat is the \\(p\\)-value (four digits) from a paired-samples \\(t\\)-test of order effect?\n\n#. library(reshape2)\nws.wide.order &lt;- dcast(ws,Subject ~ Order, value.var=\"Searches\")\ntst&lt;-t.test(ws.wide.order$\"1\",ws.wide.order$\"2\",paired=TRUE,var.equal=TRUE)\ntst\n\n\n    Paired t-test\n\ndata:  ws.wide.order$\"1\" and ws.wide.order$\"2\"\nt = 0.34273, df = 29, p-value = 0.7343\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -13.57786  19.04453\nsample estimates:\nmean difference \n       2.733333 \n\n\nThe \\(p\\)-value is 0.7343\nWhat is the \\(t\\)-statistic (two digits) for a paired-samples \\(t\\)-test of Searches by Engine?\n\n#. library(reshape2)\nws.wide.engine &lt;- dcast(ws,Subject ~ Engine, value.var=\"Searches\")\ntst&lt;-t.test(ws.wide.engine$\"Bing\",ws.wide.engine$\"Google\",paired=TRUE,var.equal=TRUE)\ntst\n\n\n    Paired t-test\n\ndata:  ws.wide.engine$Bing and ws.wide.engine$Google\nt = 2.5021, df = 29, p-value = 0.01824\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n  3.310917 32.955750\nsample estimates:\nmean difference \n       18.13333 \n\n\nThe \\(t\\)-statistic is 2.50.\nWhat is the \\(p\\)-value (four digits) from a Wilcoxon signed-rank test on Effort?\n\n#. library(coin)\nwilcoxsign_test(Effort~Engine|Subject,data=ws,distribution=\"exact\")\n\n\n    Exact Wilcoxon-Pratt Signed-Rank Test\n\ndata:  y by x (pos, neg) \n     stratified by block\nZ = 0.68343, p-value = 0.5016\nalternative hypothesis: true mu is not equal to 0\n\n\nThe \\(p\\)-value is 0.5016.\n\n\n13.12.2 Same but with three search engines\n\nws3 &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/websearch3.csv\"))\n\nHow many subjects took part in this study? ::: {.cell}\nsummary(ws3)\n\n    Subject        Engine              Order      Searches         Effort     \n Min.   : 1.0   Length:90          Min.   :1   Min.   : 92.0   Min.   :1.000  \n 1st Qu.: 8.0   Class :character   1st Qu.:1   1st Qu.:139.0   1st Qu.:3.000  \n Median :15.5   Mode  :character   Median :2   Median :161.0   Median :4.000  \n Mean   :15.5                      Mean   :2   Mean   :161.6   Mean   :4.256  \n 3rd Qu.:23.0                      3rd Qu.:3   3rd Qu.:181.8   3rd Qu.:6.000  \n Max.   :30.0                      Max.   :3   Max.   :236.0   Max.   :7.000  \n\nws3$Subject&lt;-as.factor(ws3$Subject)\nws3$Order&lt;-as.factor(ws3$Order)\nws3$Engine&lt;-as.factor(ws3$Engine)\ntail(ws3)\n\n# A tibble: 6 × 5\n  Subject Engine Order Searches Effort\n  &lt;fct&gt;   &lt;fct&gt;  &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 29      Google 3          157      5\n2 29      Bing   1          195      4\n3 29      Yahoo  2          182      5\n4 30      Google 3          152      1\n5 30      Bing   2          188      7\n6 30      Yahoo  1          131      3\n\n:::\nAgain, thirty subjects participated.\nWhat is the average number of searches for the engine with the largest average number of searches?\n\nplyr::ddply(ws3,~ Engine,summarize, Mean=mean(Searches))\n\n  Engine     Mean\n1   Bing 159.8333\n2 Google 152.6667\n3  Yahoo 172.4000\n\n\nYahoo required 172.40 searches on average.\nFind Mauchly’s \\(W\\) criterion (four digits) as a value of violation of sphericity.\n\nlibrary(ez)\nm = ezANOVA(dv=Searches, within=Order, wid=Subject, data=ws3)\nm$Mauchly\n\n  Effect         W         p p&lt;.05\n2  Order 0.9416469 0.4309561      \n\n\nMauchly’s \\(W = 0.9416\\), indicating that there is no violation of sphericity.\nConduct the appropriate ANOVA and give the \\(p\\)-value of the \\(F\\)-test (four digits).\n\nm$ANOVA\n\n  Effect DFn DFd        F        p p&lt;.05       ges\n2  Order   2  58 1.159359 0.320849       0.0278629\n\n\nThe relevant \\(p\\)-value is 0.3208.\nConduct a repeated measures ANOVA on Searches by Engine and give the Mauchly’s \\(W\\) criterion (four digits).\n\n#. library(ez)\nm &lt;- ezANOVA(dv=Searches, within=Engine, wid=Subject, data=ws3)\nm$Mauchly\n\n  Effect         W         p p&lt;.05\n2 Engine 0.9420316 0.4334278      \n\n\nMauchly’s \\(W = 0.9420\\), indicating that there is no violation of sphericity.\nConduct the appropriate ANOVA and give the \\(p\\)-value of the \\(F\\)-test (four digits).\n\nm$ANOVA\n\n  Effect DFn DFd        F          p p&lt;.05        ges\n2 Engine   2  58 2.856182 0.06560302       0.06498641\n\n\nThe relevant \\(p\\)-value is 0.0656.\nConduct post-hoc paired sample \\(t\\)-tests among levels of Engine, assuming equal variances and using “holm” to correct for multiple comparisons. What is the smallest \\(p\\)-value (four digits)? ::: {.cell}\n#. library(reshape2)\nws3.wide.engine &lt;- dcast(ws3,Subject~Engine,value.var=\"Searches\")\nbi.go&lt;-t.test(ws3.wide.engine$Bing,ws3.wide.engine$Google,paired=TRUE,var.equal=TRUE)\nbi.ya&lt;-t.test(ws3.wide.engine$Bing,ws3.wide.engine$Yahoo,paired=TRUE,var.equal=TRUE)\ngo.ya&lt;-t.test(ws3.wide.engine$Google,ws3.wide.engine$Yahoo,paired=TRUE,var.equal=TRUE)\np.adjust(c(bi.go$p.value,bi.ya$p.value,go.ya$p.value),method=\"holm\")\n\n[1] 0.37497103 0.37497103 0.05066714\n\n::: The smallest \\(p\\)-value is 0.0507.\nConduct a Friedman (nonparametric) test on Effort. Find the \\(\\chi^2\\) statistic (four digits).\n\n#. library(coin)\nfriedman_test(Effort~Engine|Subject,data=ws3,distribution=\"asymptotic\")\n\n\n    Asymptotic Friedman Test\n\ndata:  Effort by\n     Engine (Bing, Google, Yahoo) \n     stratified by Subject\nchi-squared = 8.0182, df = 2, p-value = 0.01815\n\n\n\\(\\chi^2=8.0182\\)\nConduct post hoc pairwise Wilcoxon signed-rank tests on Effort by Engine with “holm” for multiple comparison correction. Give the smallest \\(p\\)-value (four digits).\n\n#. library(reshape2)\nws3.wide.effort &lt;- dcast(ws3,Subject~Engine,value.var=\"Effort\")\nbi.go&lt;-wilcox.test(ws3.wide.effort$Bing,ws3.wide.effort$Google,paired=TRUE,exact=FALSE)\nbi.ya&lt;-wilcox.test(ws3.wide.effort$Bing,ws3.wide.effort$Yahoo,paired=TRUE,exact=FALSE)\ngo.ya&lt;-wilcox.test(ws3.wide.effort$Google,ws3.wide.effort$Yahoo,paired=TRUE,exact=FALSE)\np.adjust(c(bi.go$p.value,bi.ya$p.value,go.ya$p.value),method=\"holm\")\n\n[1] 0.69319567 0.03085190 0.04533852\n\n\nThe smallest \\(p\\)-value is 0.0309."
  },
  {
    "objectID": "week13.html#experiments-with-people-in-groups-doing-tasks-with-different-tools-mixed-models",
    "href": "week13.html#experiments-with-people-in-groups-doing-tasks-with-different-tools-mixed-models",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.13 Experiments with people in groups doing tasks with different tools (Mixed models)",
    "text": "13.13 Experiments with people in groups doing tasks with different tools (Mixed models)\nMixed models contain both fixed effects and random effects. Following are linear mixed models and generalized linear mixed models examples. Recall that linear models have normally distributed residuals while generalized linear models may have residuals following other distributions.\n\n13.13.1 Searching to find facts and effort of searching (A linear mixed model)\nLoad websearch3.csv. It describes a test of the number of searches required to find out a hundred facts and the perceived effort of searching. How many subjects participated?\n\nws &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/websearch3.csv\"))\nws&lt;-within(ws,Subject&lt;-factor(Subject))\nws&lt;-within(ws,Order&lt;-factor(Order))\nws&lt;-within(ws,Engine&lt;-factor(Engine))\ntail(ws)\n\n# A tibble: 6 × 5\n  Subject Engine Order Searches Effort\n  &lt;fct&gt;   &lt;fct&gt;  &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 29      Google 3          157      5\n2 29      Bing   1          195      4\n3 29      Yahoo  2          182      5\n4 30      Google 3          152      1\n5 30      Bing   2          188      7\n6 30      Yahoo  1          131      3\n\nsummary(ws)\n\n    Subject      Engine   Order     Searches         Effort     \n 1      : 3   Bing  :30   1:30   Min.   : 92.0   Min.   :1.000  \n 2      : 3   Google:30   2:30   1st Qu.:139.0   1st Qu.:3.000  \n 3      : 3   Yahoo :30   3:30   Median :161.0   Median :4.000  \n 4      : 3                      Mean   :161.6   Mean   :4.256  \n 5      : 3                      3rd Qu.:181.8   3rd Qu.:6.000  \n 6      : 3                      Max.   :236.0   Max.   :7.000  \n (Other):72                                                     \n\n\nWhat was the average number of Search instances for each Engine?\n\nws |&gt;\n  group_by(Engine) |&gt;\n  summarize(median=median(Searches),\n        avg=mean(Searches),\n        sd=sd(Searches))\n\n# A tibble: 3 × 4\n  Engine median   avg    sd\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bing     162.  160.  30.6\n2 Google   152   153.  24.6\n3 Yahoo    170.  172.  37.8\n\nplyr::ddply(ws,~Engine,summarize,avg=mean(Searches))\n\n  Engine      avg\n1   Bing 159.8333\n2 Google 152.6667\n3  Yahoo 172.4000\n\n\nConduct a linear mixed model analysis of variance on Search by Engine and report the \\(p\\)-value.\n\nlibrary(lme4) # for lmer\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\n\nAttaching package: 'lme4'\n\n\nThe following object is masked from 'package:RVAideMemoire':\n\n    dummy\n\nlibrary(lmerTest)\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\n#. library(car) # for Anova\ncontrasts(ws$Engine) &lt;- \"contr.sum\"\nm &lt;- lmer(Searches ~ Engine + (1|Subject), data=ws)\n\nboundary (singular) fit: see help('isSingular')\n\nAnova(m, type=3, test.statistic=\"F\")\n\nAnalysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)\n\nResponse: Searches\n                    F Df Df.res  Pr(&gt;F)    \n(Intercept) 2374.8089  1     29 &lt; 2e-16 ***\nEngine         3.0234  2     58 0.05636 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConduct simultaneous pairwise comparisons among all levels of Engine, despite the previous \\(p\\)-value. Report the adjusted(by Holm’s sequential Bonferroni procedure) \\(p\\)-values.\n\n#. library(multcomp)\nsummary(glht(m, mcp(Engine=\"Tukey\")),\n    test=adjusted(type=\"holm\")) # Tukey means compare all pairs\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lmer(formula = Searches ~ Engine + (1 | Subject), data = ws)\n\nLinear Hypotheses:\n                    Estimate Std. Error z value Pr(&gt;|z|)  \nGoogle - Bing == 0    -7.167      8.124  -0.882   0.3777  \nYahoo - Bing == 0     12.567      8.124   1.547   0.2438  \nYahoo - Google == 0   19.733      8.124   2.429   0.0454 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\n\n\n13.13.2 People judging social media posts after viewing clips (Another linear mixed model)\nThe file socialvalue.csv describes a study of people viewing a positive or negative film clip then going onto social media and judging the value (1 or 0) of the first hundred posts they see. The number of valued posts was recorded. Load the file and tell how many participated.\n\nsv &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/socialvalue.csv\"))\nsv&lt;-within(sv,Subject&lt;-factor(Subject))\nsv&lt;-within(sv,Clip&lt;-factor(Clip))\nsv&lt;-within(sv,Social&lt;-factor(Social))\nsv&lt;-within(sv,ClipOrder&lt;-factor(ClipOrder))\nsv&lt;-within(sv,SocialOrder&lt;-factor(SocialOrder))\nsummary(sv)\n\n    Subject         Clip    ClipOrder      Social   SocialOrder     Valued    \n 1      : 4   negative:32   1:32      Facebook:32   1:32        Min.   :13.0  \n 2      : 4   positive:32   2:32      Twitter :32   2:32        1st Qu.:52.0  \n 3      : 4                                                     Median :56.0  \n 4      : 4                                                     Mean   :57.3  \n 5      : 4                                                     3rd Qu.:67.0  \n 6      : 4                                                     Max.   :95.0  \n (Other):40                                                                   \n\ntail(sv)\n\n# A tibble: 6 × 6\n  Subject Clip     ClipOrder Social   SocialOrder Valued\n  &lt;fct&gt;   &lt;fct&gt;    &lt;fct&gt;     &lt;fct&gt;    &lt;fct&gt;        &lt;dbl&gt;\n1 15      negative 2         Facebook 2               60\n2 15      negative 2         Twitter  1               62\n3 16      positive 2         Facebook 2               34\n4 16      positive 2         Twitter  1               61\n5 16      negative 1         Facebook 1               59\n6 16      negative 1         Twitter  2               70\n\n\nHow many more posts were valued on Facebook than on Twitter after seeing a positive clip?\n\nout&lt;-plyr::ddply(sv,~Clip*Social,summarize,ValuedAvg=mean(Valued))\nout\n\n      Clip   Social ValuedAvg\n1 negative Facebook   46.3125\n2 negative  Twitter   55.5625\n3 positive Facebook   68.7500\n4 positive  Twitter   58.5625\n\n68.75-58.5625\n\n[1] 10.1875\n\n\nConduct a linear mixed model analysis of variance on Valued by Social and Clip. Report the \\(p\\)-value of the interaction effect.\n\n#. library(lme4) # for lmer\n#. library(lmerTest)\n#. library(car) # for Anova\ncontrasts(sv$Social) &lt;- \"contr.sum\"\ncontrasts(sv$Clip) &lt;- \"contr.sum\"\nm &lt;- lmer(Valued ~ (Social * Clip) + (1|Subject), data=sv)\n\nboundary (singular) fit: see help('isSingular')\n\nAnova(m, type=3, test.statistic=\"F\")\n\nAnalysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)\n\nResponse: Valued\n                   F Df Df.res    Pr(&gt;F)    \n(Intercept) 839.2940  1     15 1.392e-14 ***\nSocial        0.0140  1     45  0.906195    \nClip         10.3391  1     45  0.002413 ** \nSocial:Clip   6.0369  1     45  0.017930 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConduct planned pairwise comparisons of how the clips may have influenced judgments about the value of social media. Report whether the number of valued posts differed after seeing a positive versus negative clip.\n\n#. library(multcomp) # for glht\n#. library(emmeans) # for lsm\nsummary(glht(m, lsm(pairwise ~ Social * Clip)),test=adjusted(type=\"none\"))\n\nNote: df set to 45\n\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: lmer(formula = Valued ~ (Social * Clip) + (1 | Subject), data = sv)\n\nLinear Hypotheses:\n                                           Estimate Std. Error t value Pr(&gt;|t|)\nFacebook negative - Twitter negative == 0    -9.250      5.594  -1.654 0.105175\nFacebook negative - Facebook positive == 0  -22.438      5.594  -4.011 0.000225\nFacebook negative - Twitter positive == 0   -12.250      5.594  -2.190 0.033759\nTwitter negative - Facebook positive == 0   -13.188      5.594  -2.357 0.022810\nTwitter negative - Twitter positive == 0     -3.000      5.594  -0.536 0.594397\nFacebook positive - Twitter positive == 0    10.188      5.594   1.821 0.075234\n                                              \nFacebook negative - Twitter negative == 0     \nFacebook negative - Facebook positive == 0 ***\nFacebook negative - Twitter positive == 0  *  \nTwitter negative - Facebook positive == 0  *  \nTwitter negative - Twitter positive == 0      \nFacebook positive - Twitter positive == 0  .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- none method)\n\np.adjust(c(0.00017,0.59374),method=\"holm\")\n\n[1] 0.00034 0.59374\n\n\n\n\n13.13.3 People watching teasers in different orders and judging (Yet another linear mixed model)\nThe file teaser.csv describes a study in which people watched teasers for different genres and reported whether they liked them. Load the file and tell the number of participants.\n\nte &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/teaser.csv\"))\nte&lt;-within(te,Subject&lt;-factor(Subject))\nte&lt;-within(te,Order&lt;-factor(Order))\nte&lt;-within(te,Teaser&lt;-factor(Teaser))\ntail(te)\n\n# A tibble: 6 × 4\n  Subject Teaser   Order Liked\n  &lt;fct&gt;   &lt;fct&gt;    &lt;fct&gt; &lt;dbl&gt;\n1 19      thriller 4         1\n2 20      action   3         1\n3 20      comedy   2         0\n4 20      horror   4         0\n5 20      romance  1         0\n6 20      thriller 5         1\n\npar(pin=c(2.75,1.25),cex=0.5)\nboxplot(Liked~Teaser,data=te)\n\n\n\n\nInvestigate order effects.\n\ncontrasts(te$Order) &lt;- \"contr.sum\"\nm &lt;- glmer(Liked ~ Order + (1|Subject), data=te, family=binomial, nAGQ=0)\n\nboundary (singular) fit: see help('isSingular')\n\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: Liked\n             Chisq Df Pr(&gt;Chisq)\n(Intercept) 1.0392  1     0.3080\nOrder       3.9205  4     0.4169\n\n\nConduct a linear mixed model analysis of variance.\n\ncontrasts(te$Teaser) &lt;- \"contr.sum\"\nm &lt;- glmer(Liked ~ Teaser + (1|Subject), data=te, family=binomial, nAGQ=0)\n\nboundary (singular) fit: see help('isSingular')\n\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: Liked\n             Chisq Df Pr(&gt;Chisq)    \n(Intercept)  1.209  1     0.2715    \nTeaser      26.695  4  2.291e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n#. library(multcomp)\nsummary(glht(m, mcp(Teaser=\"Tukey\")),\n    test=adjusted(type=\"holm\")) # Tukey means compare all pairs\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: glmer(formula = Liked ~ Teaser + (1 | Subject), data = te, family = binomial, \n    nAGQ = 0)\n\nLinear Hypotheses:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \ncomedy - action == 0     -3.7917     1.1361  -3.337 0.006763 ** \nhorror - action == 0     -5.1417     1.2681  -4.054 0.000502 ***\nromance - action == 0    -2.5390     1.1229  -2.261 0.118786    \nthriller - action == 0   -1.5581     1.1684  -1.334 0.389097    \nhorror - comedy == 0     -1.3499     0.8909  -1.515 0.389097    \nromance - comedy == 0     1.2528     0.6682   1.875 0.243191    \nthriller - comedy == 0    2.2336     0.7420   3.010 0.018279 *  \nromance - horror == 0     2.6027     0.8740   2.978 0.018279 *  \nthriller - horror == 0    3.5835     0.9317   3.846 0.001080 ** \nthriller - romance == 0   0.9808     0.7217   1.359 0.389097    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\n\n\n13.13.4 Finding number of unique words used in posts by males and females (A generalized linear mixed model)\nThe file vocab.csv describes a study in which 50 posts by males and females were analyzed for the number of unique words used. Load the file and tell the number of participants.\n\nvo &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vocab.csv\"))\nvo&lt;-within(vo,Subject&lt;-factor(Subject))\nvo&lt;-within(vo,Sex&lt;-factor(Sex))\nvo&lt;-within(vo,Order&lt;-factor(Order))\nvo&lt;-within(vo,Social&lt;-factor(Social))\ntail(vo)\n\n# A tibble: 6 × 5\n  Subject Sex   Social   Order Vocab\n  &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;dbl&gt;\n1 29      M     Facebook 3        46\n2 29      M     Twitter  1        38\n3 29      M     Gplus    2        22\n4 30      F     Facebook 3       103\n5 30      F     Twitter  2        97\n6 30      F     Gplus    1        92\n\n\nCreate an interaction plot and see how often the lines cross.\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(vo,interaction.plot(Social,Sex,Vocab,ylim=c(0,max(vo$Vocab))))\n\n\n\n\nPerform Kolmogorov-Smirnov goodness-of-fit tests on Vocab for each level of Social using exponential distributions.\n\n#. library(MASS)\nfit = fitdistr(vo[vo$Social == \"Facebook\",]$Vocab, \"exponential\")$estimate\nks.test(vo[vo$Social == \"Facebook\",]$Vocab, \"pexp\", rate=fit[1], exact=TRUE)\n\nWarning in ks.test.default(vo[vo$Social == \"Facebook\", ]$Vocab, \"pexp\", : ties\nshould not be present for the Kolmogorov-Smirnov test\n\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  vo[vo$Social == \"Facebook\", ]$Vocab\nD = 0.17655, p-value = 0.2734\nalternative hypothesis: two-sided\n\nfit = fitdistr(vo[vo$Social == \"Twitter\",]$Vocab, \"exponential\")$estimate\nks.test(vo[vo$Social == \"Twitter\",]$Vocab, \"pexp\", rate=fit[1], exact=TRUE)\n\nWarning in ks.test.default(vo[vo$Social == \"Twitter\", ]$Vocab, \"pexp\", rate =\nfit[1], : ties should not be present for the Kolmogorov-Smirnov test\n\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  vo[vo$Social == \"Twitter\", ]$Vocab\nD = 0.099912, p-value = 0.8966\nalternative hypothesis: two-sided\n\nfit = fitdistr(vo[vo$Social == \"Gplus\",]$Vocab, \"exponential\")$estimate\nks.test(vo[vo$Social == \"Gplus\",]$Vocab, \"pexp\", rate=fit[1], exact=TRUE)\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  vo[vo$Social == \"Gplus\", ]$Vocab\nD = 0.14461, p-value = 0.5111\nalternative hypothesis: two-sided\n\n\nUse a generallized linear mixed model to conduct a test of order effects on Vocab.\n\ncontrasts(vo$Sex) &lt;- \"contr.sum\"\ncontrasts(vo$Order) &lt;- \"contr.sum\"\nm &lt;- glmer(Vocab ~ Sex*Order + (1|Subject), data=vo, family=Gamma(link=\"log\"))\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: Vocab\n                Chisq Df Pr(&gt;Chisq)    \n(Intercept) 1179.0839  1     &lt;2e-16 ***\nSex            1.3687  1     0.2420    \nOrder          0.7001  2     0.7047    \nSex:Order      2.0655  2     0.3560    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConduct a test of Vocab by Sex and Social using a generalized linear mixed model.\n\ncontrasts(vo$Sex) &lt;- \"contr.sum\"\ncontrasts(vo$Social) &lt;- \"contr.sum\"\nm = glmer(Vocab ~ Sex*Social + (1|Subject), data=vo, family=Gamma(link=\"log\"))\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: Vocab\n                Chisq Df Pr(&gt;Chisq)    \n(Intercept) 1172.6022  1  &lt; 2.2e-16 ***\nSex            0.7925  1     0.3733    \nSocial        26.2075  2  2.038e-06 ***\nSex:Social     0.3470  2     0.8407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPerform post hoc pairwise comparisons among levels of Social adjusted with Holm’s sequential Bonferroni procedure.\n\n#. library(multcomp)\nsummary(glht(m, mcp(Social=\"Tukey\")),\n    test=adjusted(type=\"holm\")) # Tukey means compare all pairs\n\nWarning in mcp2matrix(model, linfct = linfct): covariate interactions found --\ndefault contrast might be inappropriate\n\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: glmer(formula = Vocab ~ Sex * Social + (1 | Subject), data = vo, \n    family = Gamma(link = \"log\"))\n\nLinear Hypotheses:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \nGplus - Facebook == 0     0.8676     0.2092   4.148 6.72e-05 ***\nTwitter - Facebook == 0  -0.1128     0.2026  -0.556    0.578    \nTwitter - Gplus == 0     -0.9803     0.2071  -4.734 6.60e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\n\n\n13.13.5 Judging search effort among different search engines (Another generalized linear mixed model)\nRecode Effort from websearch3.csv as an ordinal response.\n\nws&lt;-within(ws,Effort&lt;-factor(Effort))\nws&lt;-within(ws,Effort&lt;-ordered(Effort))\nsummary(ws)\n\n    Subject      Engine   Order     Searches     Effort\n 1      : 3   Bing  :30   1:30   Min.   : 92.0   1: 6  \n 2      : 3   Google:30   2:30   1st Qu.:139.0   2: 8  \n 3      : 3   Yahoo :30   3:30   Median :161.0   3:19  \n 4      : 3                      Mean   :161.6   4:16  \n 5      : 3                      3rd Qu.:181.8   5:16  \n 6      : 3                      Max.   :236.0   6:15  \n (Other):72                                      7:10  \n\n\nConduct an ordinal logistic regression to determine Effort by Engine, using a generalized linear mixed model.\n\nlibrary(ordinal) # provides clmm\n\n\nAttaching package: 'ordinal'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n#. library(RVAideMemoire) # provides Anova.clmm\nws2&lt;-data.frame(ws)\nm&lt;-clmm(Effort~Engine + (1|Subject),data=ws2)\nAnova.clmm(m,type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Effort\n       LR Chisq Df Pr(&gt;Chisq)  \nEngine    8.102  2     0.0174 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPerform pairwise comparisons of Engine on Effort.\n\npar(pin=c(2.75,1.25),cex=0.5)\nplot(as.numeric(Effort) ~ Engine,data=ws2)\n\n\n\n#. library(lme4)\n#. library(multcomp)\nm &lt;- lmer(as.numeric(Effort)~Engine + (1|Subject), data=ws2)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(glht(m,mcp(Engine=\"Tukey\")),test=adjusted(type=\"holm\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lmer(formula = as.numeric(Effort) ~ Engine + (1 | Subject), data = ws2)\n\nLinear Hypotheses:\n                    Estimate Std. Error z value Pr(&gt;|z|)  \nGoogle - Bing == 0  -0.03333    0.42899  -0.078   0.9381  \nYahoo - Bing == 0    1.10000    0.42899   2.564   0.0247 *\nYahoo - Google == 0  1.13333    0.42899   2.642   0.0247 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)"
  },
  {
    "objectID": "week14.html",
    "href": "week14.html",
    "title": "14  Data Science Intro",
    "section": "",
    "text": "For this week, we review two chapters of James et al. (2021), chapters 2 and 5. To review them, we will use the slideshows Ch2_Statistical_Learning.pdf and Ch5_Resampling_Methods.pdf from the book’s authors. We will then do the lab from chapter 5, saved on Canvas as Ch5-resample-lab.html in the folder week14material. We will complete exercises 2.4.1, 5.4.5, 5.4.6, and 5.4.8 from the same book. These materials are all available on the book’s website, as well as on Canvas. This book, now in its second edition, is a widely used introduction to statistical machine learning.\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning, 2nd Edition. Springer New York."
  },
  {
    "objectID": "nextSteps.html",
    "href": "nextSteps.html",
    "title": "15  Next Steps",
    "section": "",
    "text": "What should you do next? You’ve learned some statistics, specifically\n\nnumerical description\nvisual description\nlinear regression\nlinear regression diagnostics\n\nYou’ve also been introduced to hypothesis testing, confidence intervals, and classification (through logistic regression).\nHere are my ideas about topics you should learn next.\n\nBasic calculus through partial derivatives (the Calculus Tutoring Book is a good start)\nProbability (the Probability Tutoring Book is a good start)\nLinear algebra (there are dozens of free linear algebra books online) (although I’m partial to the nonfree linear algebra introduction in the Goodfellow Deep Learning book and the nonfree Howard Anton linear algebra textbook)\nStatistical learning (read the free ISLR2 book!)\nBias-Variance Tradeoff (covered in the ISLR2 book)\nTransformer neural networks (for an intro on neural networks in general, see https://youtu.be/aircAruvnKk?si=zjHr3lJJ4vY91BaJ and the following videos) (covering MLPs multi layer perceptrons, backpropagation, stochastic gradient descent, RELU, and more)\nRead the paper “Attention is all you need”\nRead the paper “Textbooks are all you need”\nRead about retrieval augmented generation (RAG)\nRead about generative adversarial networks (GAN)\n\nThen, after you’ve learned at least some of the above topics, fine-tune a small generative AI model using one of the pre-trained models you can find in the Hugging Face Hub repository. This will teach you much more about generative AI than you will from listening to others talk about it. But still, you won’t understand what you are doing unless you get some grounding in probability and linear algebra.\nI realize that the above is a long list and that you probably won’t do everything on it! I’ve tried to put the most important things at the top in the sense that you won’t understand the items lower down on the list unless you understand probability and linear algebra, for instance. Those are foundational. The rest is an adventure!"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Anderson, David. 2008. Model Based Inference in the Life Sciences: A\nPrimer on Evidence. New York, NY: Springer.\n\n\nAsh, Carol. 1993. The Probability Tutoring Book. New York, NY:\nIEEE Press.\n\n\nDiez, David, Mine Çetinkaya-Rundel, and Cristopher D Barr. 2019.\nOpenIntro Statistics, Fourth Edition. self-published. https://openintro.org/os.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning, 2nd Edition.\nSpringer New York.\n\n\nMendenhall, William, and Terry Sincich. 2012. A Second Course in\nStatistics, Regression Analysis, Seventh Edition. Boston, MA, USA:\nPrentice Hall.\n\n\nvan Buuren, Stef. 2018. Flexible Imputation of Missing Data.\nBoca Raton, FL: CRC Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualize, and Model\nData. 2nd ed. O’Reilly Media, Inc. https://r4ds.hadley.nz/."
  }
]