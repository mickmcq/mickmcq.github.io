[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Study Guide: Statistics for Informatics",
    "section": "",
    "text": "Preface\nThis is the study guide for a University of Texas at Austin, School of Information course in the undergraduate Informatics major: I306 Statistics for Informatics. It was developed during the Spring 2023 semester, so some of the material is already dated and the instructor will point out that material during class sessions.\nThis study guide supplements our main textbook, Diez, Çetinkaya-Rundel, and Barr (2019) and our secondary textbook, Wickham, Çetinkaya-Rundel, and Grolemund (2023). We will go over the study guide in class instead of slideshows, which will only be used on the first and last day of class.\nThis book was created using the Quarto document publishing system, which is the same system you will use to complete all homework and the take-home final exam in this course. Consequently, the guide describes many details of Quarto, some of which you will need to complete homework and some of which are optional. Quarto is an example of tools for reproducible research and literate programming, two important concepts in science that will be discussed in class sessions. Quarto can be used to produce books, websites, presentations, documents, and more. This is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.\nNote that the display of a Quarto book has three parts: (1) a left sidebar, containing a list of book chapters and a search box that can return results from any chapter, (2) the body, containing the text of the current chapter, and (3) a right sidebar, containing the table of contents for just the current chapter."
  },
  {
    "objectID": "index.html#why-quarto",
    "href": "index.html#why-quarto",
    "title": "Study Guide: Statistics for Informatics",
    "section": "Why Quarto?",
    "text": "Why Quarto?\nQuarto is free and is being actively developed by Posit, the same company developing RStudio. It provides us with an audit trail of our work, and a platform that allows you to think aloud in a sense, adding code and thoughts and pictures to a document that slowly takes shape as you explore data. You gradually sharpen your ideas and the document along with them and, when you are finished, the document contains all the code, thoughts, pictures, equations, and whatever else you wish to share with your audience."
  },
  {
    "objectID": "index.html#why-r-and-rstudio",
    "href": "index.html#why-r-and-rstudio",
    "title": "Study Guide: Statistics for Informatics",
    "section": "Why R and RStudio?",
    "text": "Why R and RStudio?\nR is free and is the leading platform for statisticians in the world today. It is both a language and software that implements that language and is where most statisticians implement their ideas in code. Other statistics platforms such as SPSS are more consumer oriented and don’t receive the amount of debugging and new features that R offers.\nRStudio is free (for our use, there is also a paid version that allows you to collaborate, kind of like Google docs but for data analysis). It is the most prominent IDE for the use of R. It is also an IDE for Python, by the way, although we won’t use Python in this course. It is produced, and the paid version is marketed, by Posit, a company based in Austin that employs as its chief scientist, Hadley Wickham, the leading developer of R software in the world today."
  },
  {
    "objectID": "index.html#why-use-our-textbooks",
    "href": "index.html#why-use-our-textbooks",
    "title": "Study Guide: Statistics for Informatics",
    "section": "Why use our textbooks?",
    "text": "Why use our textbooks?\nBoth our main and secondary textbooks are free, open educational resources. The main textbook covers introductory statistics and the secondary textbook covers R in the context of data science, which is closely related to statistics. I will also mention other valuable texts in the course of the study guide. These are listed in the References section at the end of the book.\n\n\n\n\nDiez, David, Mine Çetinkaya-Rundel, and Cristopher D Barr. 2019. OpenIntro Statistics, Fourth Edition. self-published. https://openintro.org/os.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd ed. O’Reilly Media, Inc. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "week01.html#introducing-r-and-rstudio",
    "href": "week01.html#introducing-r-and-rstudio",
    "title": "1  Introduction to the Course",
    "section": "1.1 Introducing R and RStudio",
    "text": "1.1 Introducing R and RStudio\nFirst we introduce R and RStudio and, finally, Quarto. First, either use the RStudio server at https://rstudio.ischool.utexas.edu or install R and RStudio, in that order. Some people have trouble installing, especially Windows or Mac. Some Mac users were opening the .dmg file for RStudio as a readonly volume, then open the app on that volume. Instead, you have to drag the RStudio icon to the Applications folder and open it from there. The telltale sign of this problem is that you can’t save any files. Windows users have a different problem. Some Windows users try to install RStudio and R on OneDrive. RStudio won’t run from OneDrive and some Windows users can’t tell the difference between installing on a local hard drive and installing in the cloud on OneDrive."
  },
  {
    "objectID": "week01.html#console",
    "href": "week01.html#console",
    "title": "1  Introduction to the Course",
    "section": "1.2 Console",
    "text": "1.2 Console\nThe first thing we can try (after installing if you chose to have it on your machine) is to use the console. By default, that is in the lower left of the RStudio window (you can move everything around, though) and it has a command prompt that looks like &gt;. There enter the following function to verify that you can download R packages, which are collections of functions.\n\nlibrary(MASS)\n\nIf this works, you won’t get any output from the library() function but the command prompt will reappear. That function loads the package from the library into our environment so we can use it in the current session. If you screw around with RStudio and particularly if you follow a lot of hints on Stack Overflow, you may end up with several libraries of packages, all out of sync with each other. If you have trouble loading packages from the library, you may want to call the following function to see how many libraries are on your computer and where they are. This function will return the list of library folders on the server if you call it there.\n\n.libPaths()\n\n[1] \"/Users/mcq/Library/R/x86_64/4.3/library\"                              \n[2] \"/Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library\"\n\n\nThis function returns a list of folders containing libraries, one library per folder. You can then use the terminal or a file explorer outside of R to delete some duplicate packages. The important library for this class is /opt/R/4.3.1/lib/R/library. Only I can install packages there. If you need a package that is not installed, it is best to ask me to install it there. You also probably have a personal library where you can install packages, but on this machine, it is better not to do so.\nAfter loading the relative small package known as MASS, go on to install a package that is actually a large set of packages, collectively known as the Tidyverse. This is a set of packages we will use in our homework.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::select() masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThis takes a while because there are so many packages involved."
  },
  {
    "objectID": "week01.html#the-mtcars-data-set",
    "href": "week01.html#the-mtcars-data-set",
    "title": "1  Introduction to the Course",
    "section": "1.3 The mtcars data set",
    "text": "1.3 The mtcars data set\nThere is a lot of data built into R by default. We look at one such data set, called mtcars. Run a function that looks at the first few lines of the data set, head(mtcars), then checked the help screen for the data set, saying help(mtcars), then produce a linear model of the mpg column being predicted by the disp column, saying summary(lm(mpg~disp,data=mtcars)). This linear model is the heart of regression analysis and one of the main things we’ll learn in this course is how to read the summary.\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nhelp(mtcars)\n\nstarting httpd help server ... done\n\n\nDescriptive statistics deals with numerical analysis of data, such as finding the mean, median etc of values in the dataset. We will find mean, median and mode using the weight column in mtcars with the functions mean() and median(). R doesn’t have a built-in function for mode so we calculate it explicitly.\n\nmean_wt &lt;- mean(mtcars$wt)\nprint(paste0(\"Mean = \", mean_wt))\n\n[1] \"Mean = 3.21725\"\n\n\n\nmedian_wt &lt;- median(mtcars$wt)\nprint(paste0(\"Median = \", median_wt))\n\n[1] \"Median = 3.325\"\n\n\n\nmode_wt &lt;- as.numeric(names(sort(table(mtcars$wt), decreasing = TRUE)[1]))\nprint(paste0(\"Mode = \", mode_wt))\n\n[1] \"Mode = 3.44\"\n\n\nHistograms show distribution of data. Let’s create a histogram using the data in mtcars. First load the dataset using data(mtcars). Then we use the hist() function in R to create a histogram for the mpg variable.\n\ndata(mtcars)\nhist(mtcars$mpg, main = \"Miles per Gallon Distribution\", xlab = \"Miles per Gallon\", ylab = \"Frequency\")\n\n\n\n\nThen we calculate the range of miles per gallon from the histogram using the range() function.\n\nmpg_range &lt;- range(mtcars$mpg)\nmpg_range\n\n[1] 10.4 33.9\n\n\nNow, we will produce a linear model of the mpg column being predicted by the disp column, saying summary(lm(mpg~disp,data=mtcars)). This linear model is the heart of regression analysis and one of the main things we’ll learn in this course is how to read the summary.\n\nsummary(lm(mpg~disp,data=mtcars))\n\n\nCall:\nlm(formula = mpg ~ disp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8922 -2.2022 -0.9631  1.6272  7.2305 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 29.599855   1.229720  24.070  &lt; 2e-16 ***\ndisp        -0.041215   0.004712  -8.747 9.38e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.251 on 30 degrees of freedom\nMultiple R-squared:  0.7183,    Adjusted R-squared:  0.709 \nF-statistic: 76.51 on 1 and 30 DF,  p-value: 9.38e-10"
  },
  {
    "objectID": "week01.html#textbook-data-sets",
    "href": "week01.html#textbook-data-sets",
    "title": "1  Introduction to the Course",
    "section": "1.4 Textbook data sets",
    "text": "1.4 Textbook data sets\nOur textbook, Openintro Stats, contains references to a lot of data sets, many of which I’ve downloaded and put into the folder /home/mm223266/data/. You can just use them from this location or go to the URL https://openintro.org/data, but if you can’t remember that, you can just google openintro stats and navigate to the data sets. Look at the metadata for the loan50 data set, which is used in Chapter 1 of the textbook. You can download it in four different formats, the best of which is the .rda file or R Data file. It’s the best because it preserves the data types, in this case dbl, int, and fctr. If we instead import the .csv file, we have to then specify the data types in R, which is an extra step we’d like to avoid when possible.\nWhen we download a file, R doesn’t know where it is automatically. We do one of three things.\n\nChange R to address the folder where we downloaded it\nMove it to the folder R is currently addressing\nKeep R addressing your homework folder, but reach out for the data sets where I’ve downloaded them (only works if you’re using the RStudio Server).\n\nHow you do this depends on the operating system but, in any operating system we use the following three functions.\n\ngetwd()\n\n[1] \"/Users/mcq/courses/stats/i306studyGuide\"\n\n# setwd(\"/home/mm223266/i306/\")\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/loan50.rda\"))\n\nThe first function tells us which folder (or directory if you prefer) R is addressing. The second one changes to the folder / directory we would like to use. (I’ve commented it out for this study guide.) The third one addresses the data where I’ve put it but leaves your working directory where it is. This is very convenient because it means that (1) you don’t have to download data sets, and (2) I don’t have to modify your homework file in order to check it.\nMy suggestion is that you create a folder for this class and use the third option. You can make your folder your default in RStudio, using Tools &gt; Global Options &gt; General &gt; Default working directory.\nOnce we load loan50.rda, look at it and try to predict total_credit_limit using annual_income. Keep in mind that, if the file is in the current working directory / folder, R will autocomplete its name when you say lo and then press the tab key (assuming there are no other files starting with the letters lo in the same folder). You just have to enter enough letters to make the name unique before you press the tab key. If nothing happens when you press the tab key, you are either in the wrong folder or you have other files starting with the same letters.\n\nhead(loan50)\n\n  state emp_length term homeownership annual_income verified_income\n1    NJ          3   60          rent         59000    Not Verified\n2    CA         10   36          rent         60000    Not Verified\n3    SC         NA   36      mortgage         75000        Verified\n4    CA          0   36          rent         75000    Not Verified\n5    OH          4   60      mortgage        254000    Not Verified\n6    IN          6   36      mortgage         67000 Source Verified\n  debt_to_income total_credit_limit total_credit_utilized\n1      0.5575254              95131                 32894\n2      1.3056833              51929                 78341\n3      1.0562800             301373                 79221\n4      0.5743467              59890                 43076\n5      0.2381496             422619                 60490\n6      1.0770448             349825                 72162\n  num_cc_carrying_balance       loan_purpose loan_amount grade interest_rate\n1                       8 debt_consolidation       22000     B         10.90\n2                       2        credit_card        6000     B          9.92\n3                      14 debt_consolidation       25000     E         26.30\n4                      10        credit_card        6000     B          9.92\n5                       2   home_improvement       25000     B          9.43\n6                       4   home_improvement        6400     B          9.92\n  public_record_bankrupt loan_status has_second_income total_income\n1                      0     Current             FALSE        59000\n2                      1     Current             FALSE        60000\n3                      0     Current             FALSE        75000\n4                      0     Current             FALSE        75000\n5                      0     Current             FALSE       254000\n6                      0     Current             FALSE        67000\n\nsummary(lm(total_credit_limit~annual_income,data=loan50))\n\n\nCall:\nlm(formula = total_credit_limit ~ annual_income, data = loan50)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-303384  -91959  -38226   89869  239503 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.649e+04  3.156e+04   1.156    0.253    \nannual_income 1.997e+00  3.055e-01   6.535  3.8e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 123100 on 48 degrees of freedom\nMultiple R-squared:  0.4708,    Adjusted R-squared:  0.4598 \nF-statistic: 42.71 on 1 and 48 DF,  p-value: 3.796e-08"
  },
  {
    "objectID": "week01.html#the-migraine-data-set",
    "href": "week01.html#the-migraine-data-set",
    "title": "1  Introduction to the Course",
    "section": "1.5 The migraine data set",
    "text": "1.5 The migraine data set\nNext we load the migraine.rda file from the same place as above and reproduce a figure from the textbook by using the table() function.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/migraine.rda\"))\nhead(migraine)\n\n# A tibble: 6 × 2\n  group     pain_free\n  &lt;fct&gt;     &lt;fct&gt;    \n1 treatment yes      \n2 treatment yes      \n3 treatment yes      \n4 treatment yes      \n5 treatment yes      \n6 treatment yes      \n\nwith(migraine,table(pain_free,group))\n\n         group\npain_free control treatment\n      no       44        33\n      yes       2        10\n\n\nWe could have done this graphically.\n\ntbl &lt;- with(migraine,table(pain_free,group))\nmosaicplot(tbl)\n\n\n\n\nExamine the mosaic plot and the table to see how the sizes of the rectangles compare to the numbers.\nWe could also more precisely reproduce the figure from the textbook by adding row and column sums.\n\naddmargins(tbl)\n\n         group\npain_free control treatment Sum\n      no       44        33  77\n      yes       2        10  12\n      Sum      46        43  89\n\n\nWe could make it prettier by using the pander package.\n\nlibrary(pander)\npander(addmargins(tbl))\n\n\n\n\n\n\n\n\n\n\n \ncontrol\ntreatment\nSum\n\n\n\n\nno\n44\n33\n77\n\n\nyes\n2\n10\n12\n\n\nSum\n46\n43\n89\n\n\n\n\n\npander has a lot of options we could use to make it even prettier, but we’ll skip that for now. There are also a lot of other packages similar to pander for prettifying R output.\nWe could display proportions instead of the raw numbers, but it looks ugly, so we’ll then use the options() function to make it look better.\n\nprop.table(tbl)\n\n         group\npain_free    control  treatment\n      no  0.49438202 0.37078652\n      yes 0.02247191 0.11235955\n\noptions(digits=1)\nprop.table(tbl)\n\n         group\npain_free control treatment\n      no     0.49      0.37\n      yes    0.02      0.11\n\n\nBear in mind that digits=1 is a suggestion to R and that R will determine the exact number of digits on its own, depending on the value of the variables to be displayed."
  },
  {
    "objectID": "week02.html#recap-week-01",
    "href": "week02.html#recap-week-01",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.1 Recap Week 01",
    "text": "2.1 Recap Week 01\nWe looked at four things:\n\nAn example experiment (stents)\ndata basics\nsampling\nexperiments\n\nThe best way to read the textbook is to do some of the exercises. I expect you to spend 6 to 9 hours doing so outside of class each week. This is based on the popular rule of thumb that three class hours implies six to nine study hours.\nLet’s look at some of the Chapter 1 exercises briefly."
  },
  {
    "objectID": "week02.html#textbook-section-1.1-an-example-experiment-stents",
    "href": "week02.html#textbook-section-1.1-an-example-experiment-stents",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.2 Textbook Section 1.1, An Example Experiment (stents)",
    "text": "2.2 Textbook Section 1.1, An Example Experiment (stents)\nHere we looked at treatment groups and control groups.\n\n2.2.1 Textbook Exercise 1.1, the migraine data set\nIn class we loaded the data set and made a contingency table, with which we can answer the four questions in Exercise 1.1.\n\noptions(digits=1)\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/migraine.rda\"))\nhead(migraine)\n\n      group pain_free\n1 treatment       yes\n2 treatment       yes\n3 treatment       yes\n4 treatment       yes\n5 treatment       yes\n6 treatment       yes\n\ntbl &lt;- with(migraine,table(pain_free,group))\ntbl\n\n         group\npain_free control treatment\n      no       44        33\n      yes       2        10\n\n\nThe four questions are:\n\nWhat percent of patients in the treatment group were pain free 24 hours after receiving acupuncture?\nWhat percent were pain free in the control group?\nIn which group did a higher percent of patients become pain free 24 hours after receiving acupuncture?\nYour findings so far might suggest that acupuncture is an effective treatment for migraines for all people who suffer from migraines. However, this is not the only possible conclusion that can be drawn based on your findings so far. What is one other possible explanation for the observed difference between the percentages of patients that are pain free 24 hours after receiving acupuncture in the two groups?\n\nWe can use the tbl object we created to answer these questions.\n\nThe percentage of patients in the treatment group who were pain free was 23.3%.\nThe percentage of patients in the control group who were pain free was 4.3%.\nA higher percent of the treatment group became pain free.\nNot all migraine headaches are alike. It is possible that, due to chance, the patients in the treatment group had less severe migraine headaches that were easier to cure.\n\nExamine the .qmd file that renders into this .html file. You’ll see that, rather than specify the numbers in the answers above, we actually did the calculations inline. What is good about doing this? (Hint: what happens if you add patients to the data set and rerender the document?) What is bad about doing this? (Hint: it looks clumsy and we could just as easily run the calculations in an r chunk, assign the results to object, and name the objects inline.)\nAnother issue is the code for part c. I only accounted for the case where the treatment group has the greater percent. It would be much better to add an else clause to account for the possibility that the answer should be control and an else to account for the two to be equal. The best way to do that is to create a non-echoing chunk and use the results inline. For example, the following chunk only appears in the .qmd file, not the .html file, but its results can be used inline.\nNow we can say that the treatment group had the higher percentage. Only one problem remains and it is a software engineering problem. We haven’t tested the above code on a case where the control group or neither group had the higher percentages. We’ll leave that for now as a more advanced topic."
  },
  {
    "objectID": "week02.html#textbook-section-1.2-data-basics",
    "href": "week02.html#textbook-section-1.2-data-basics",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.3 Textbook Section 1.2, Data Basics",
    "text": "2.3 Textbook Section 1.2, Data Basics\nHere we looked at\n\nobservations (rows), variables (columns), and data matrices (data frames)\ntypes of variables (dbl or continuous, int or discrete, fctr or nominal, and ordered fctr or ordinal)\nrelationships between variables\nexplanatory (x or features or input) and response (y or targets or output) variables\nobservational studies and experiments (and we mentioned an in-between activity called quasi-experiments)\n\n\nData Type terminology: R vs the Textbook\n\n\nR\nOpenintro Stats textbook\n\n\n\n\ndbl, as.numeric()\nnumerical, continuous\n\n\nint, as.integer()\nnumerical, discrete\n\n\nfctr, factor()\ncategorical, nominal\n\n\nord, ordered()\ncategorical, ordinal\n\n\n\nOn the left of the above table, you see how R refers to data types. On the right is how the OpenIntro Stats textbook refers to data types. When you display a tibble (a tibble is a data frame with some extra information) using R, each variable column will be headed with dbl, int, fctr, or ord to indicate the four kinds of numbers. If a variable is not interpreted as a number, R will display chr as an abbreviation of character.\n\n2.3.1 Textbook Exercise 1.7\nWhat were the explanatory and response variables in the migraine study? The group was explanatory and pain_free was the response variable.\n\n\n2.3.2 Textbook Exercise 1.12\nThis is a hard question in two parts.\n\nList the variables used in creating this visualization.\nIndicate whether each variable in the study is numerical or categorical. If numerical, identify as contin- uous or discrete. If categorical, indicate if the variable is ordinal.\n\nThere is actually an r package underlying this question. If you visit https://github.com/dgrtwo/unvotes you will see the data represented as a tibble. If you recall, during week one we said that a tibble is a data frame that behaves well. Among its features is a list of the data types, so you can answer parts a and b by looking at a tibble of the data, where you’ll see that\n\nyear is stored as dbl although it is really discrete and could be stored as int\ncountry is stored as chr which means characters although it is really a nominal factor\npercent_yes is stored as a dbl which is appropriate\nissue is stored as chr although it is really a nominal factor\n\nLater we’ll learn how to produce a visualization like this, although you are welcome to try based on the code at the unvotes website mentioned above. If you want the actual code itself, you can slightly modify the code at https://rpubs.com/minebocek/unvotes to include Mexico."
  },
  {
    "objectID": "week02.html#textbook-section-1.3-sampling",
    "href": "week02.html#textbook-section-1.3-sampling",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.4 Textbook Section 1.3, Sampling",
    "text": "2.4 Textbook Section 1.3, Sampling\nHere we talked about random sampling, stratified sampling, cluster sampling, and observational studies.\n\n2.4.1 Textbook Exercise 1.15, Asthma\n\nWhat is the population of interest and the sample? Note that the population is NOT all asthma sufferers. The population of interest is all asthma patients aged 18-69 who rely on medication for asthma treatment. The sample consists of 600 such patients.\nIs the study generalizable? Can we establish cause and effect? The patients are probably not randomly sampled, so we need to know more to say whether they represent all asthma patients 18–69 who rely on medication. For example, they could all be from a high-pollution city. We would need to know that. The cause and effect determination is easier. An experiment can determine cause and effect, while an observational study only determines association."
  },
  {
    "objectID": "week02.html#textbook-section-1.4-experiments",
    "href": "week02.html#textbook-section-1.4-experiments",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.5 Textbook Section 1.4, Experiments",
    "text": "2.5 Textbook Section 1.4, Experiments\nHere we discussed four issues:\n\ncontrol\nrandomization\nreplication\nblocking\n\n\n2.5.1 Textbook Exercise 1.34, Exercise and mental health\nA researcher is interested in the effects of exercise on mental health and he proposes the following study: Use stratified random sampling to ensure representative proportions of 18-30, 31-40 and 41-55 year olds from the population. Next, randomly assign half the subjects from each age group to exercise twice a week, and instruct the rest not to exercise. Conduct a mental health exam at the beginning and at the end of the study, and compare the results.\n\nWhat type of study is this?\nWhat are the treatment and control groups in this study?\nDoes this study make use of blocking? If so, what is the blocking variable?\nDoes this study make use of blinding?\nComment on whether or not the results of the study can be used to establish a causal relationship between exercise and mental health, and indicate whether or not the conclusions can be generalized to the population at large.\nSuppose you are given the task of determining if this proposed study should get funding. Would you have any reservations about the study proposal?\n\n\n\n2.5.2 Answers\n\nThis is an experiment.\nThe treatment is exercise twice a week and control is no exercise.\nYes, the blocking variable is age.\nNo, the study is not blinded since the patients will know whether or not they are exercising.\nSince this is an experiment, we can make a causal statement. Since the sample is random, the causal statement can be generalized to the population at large. However, we should be cautious about making a causal statement because of a possible placebo effect.\nIt would be very difficult, if not impossible, to successfully conduct this study since randomly sampled people cannot be required to participate in a clinical trial"
  },
  {
    "objectID": "week02.html#textbook-chapter-2-summarizing-data",
    "href": "week02.html#textbook-chapter-2-summarizing-data",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.6 Textbook Chapter 2: Summarizing data",
    "text": "2.6 Textbook Chapter 2: Summarizing data\nThis week, we’ll look at numerical data, categorical data, and a case study.\n\n2.6.1 Numerical data\nThere are graphical and numerical methods for summarizing numerical data, including\n\nscatterplots\ndot plots\nmean\nhistograms\nvariance and standard deviation\nbox plots, quartiles, and the median\nrobust statistics\ncartographic maps and cartograms\n\nWe can draw a scatterplot of two variables of the loan50 data as follows.\n\noptions(repos=structure(c(CRAN=\"https://mirrors.nics.utk.edu/cran/\")))\nlibrary(tidyverse)\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/loan50.rda\"))\nloan50 |&gt;\n  ggplot(aes(annual_income,loan_amount)) +\n  geom_point()\n\n\n\n\nThe above is a very basic scatterplot. Later, we’ll learn to change colors, background, labels, legends, and more. Bear in mind that the scatterplot is meant to compare two numeric variables. You can’t use it for a numeric variable and a categorical variable.\nWe can create a basic dotplot as follows.\n\nloan50 |&gt;\n  ggplot(aes(homeownership)) +\n  geom_dotplot()\n\n\n\n\nA dotplot may be helpful to illustrate a categorical variable, but I seldom use them. Using them in concert with a boxplot may make more sense. We’ll look at boxplots later.\nThe mean is part of a good summary of data. We can find the mean of a variable by saying mean(variable_name) or as part of a summary. For instance\n\nwith(loan50,mean(annual_income))\n\n[1] 86170\n\nwith(loan50,summary(annual_income))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  28800   55750   74000   86170   99500  325000 \n\n\nNotice that the summary() function also gives us the minimum, the first quartile, the median, the 3rd quartile, and the maximum value of the variable, in addition to the mean. We’ll discuss all these statistics in the context of other ways to extract them. The problem with the mean that is demonstrated in the textbook is that two variables may have very different shapes but the same mean. So the textbook then describes histograms, which are a good way to identify the shape of a variable.\n\nloan50 |&gt;\n  ggplot(aes(annual_income)) +\n  geom_histogram()\n\n\n\n\nNotice that the x-axis labels are shown in scientific notation. We can fix this using the scales package. By the way, in case I haven’t mentioned it before, we always refer to the horizontal axis as the x-axis and the vertical axis as the y-axis. This is the default for r and many other languages that create graphical displays.\n\nlibrary(scales)\nloan50 |&gt;\n  ggplot(aes(annual_income)) +\n  geom_histogram() +\n  scale_x_continuous(labels = comma_format())\n\n\n\n\nYou may have noticed that Hadley Wickham, the inventor of the Tidyverse, dislikes the default number of bins in a histogram. So he programmed ggplot() to always show a warning message saying to pick a better number, depending on your data. We can fix that easily with a parameter to geom_histogram(). Then each bin (vertical stripe) will represent a thousand dollars.\n\nloan50 |&gt;\n  ggplot(aes(annual_income)) +\n  geom_histogram(binwidth=1000) +\n  scale_x_continuous(labels = comma_format())\n\n\n\n\nYou may notice that warning messages aren’t dealbreakers. An error message on the other hand, will often stop output dead in its tracks.\nThe next concepts covered in the textbook are variance and standard deviation. We can calculate them as follows. When you do this, you may notice that sd() is the square root of var(). Why would you prefer one over the other? Usually you use sd() because it’s in the same units as the data, dollars in the following case, unlike var(), which is in squared units, squared dollars in the following case.\n\nwith(loan50,var(annual_income))\n\n[1] 3e+09\n\nwith(loan50,sd(annual_income))\n\n[1] 57566\n\n\nTogether, the mean and standard deviation are often a good, yet compact, description of a data set.\nYou may want to find the means of all the columns in a data set. If you try to do that with the colMeans() function, you’ll get an error message as follows. (Actually, I’ve disabled the following code chunk by saying #| eval: false in the .qmd file because otherwise the rendering would halt.)\n\ncolMeans(loan50)\n\nThe remedy is to use a logical function to identify only the numeric columns.\n\ncolMeans(loan50[sapply(loan50, is.numeric)])\n\n             emp_length                    term           annual_income \n                     NA                   4e+01                   9e+04 \n         debt_to_income      total_credit_limit   total_credit_utilized \n                  7e-01                   2e+05                   6e+04 \nnum_cc_carrying_balance             loan_amount           interest_rate \n                  5e+00                   2e+04                   1e+01 \n public_record_bankrupt            total_income \n                  8e-02                   1e+05 \n\n\nThis is okay, but the results are in scientific notation. You can use the format() function to supress scientific notation as follows.\n\nformat(colMeans(loan50[sapply(loan50, is.numeric)]),scientific=FALSE)\n\n             emp_length                    term           annual_income \n            \"       NA\"             \"    42.72\"             \" 86170.00\" \n         debt_to_income      total_credit_limit   total_credit_utilized \n            \"     0.72\"             \"208546.64\"             \" 61546.54\" \nnum_cc_carrying_balance             loan_amount           interest_rate \n            \"     5.06\"             \" 17083.00\"             \"    11.57\" \n public_record_bankrupt            total_income \n            \"     0.08\"             \"105220.56\" \n\n\nNotice that you often wrap a function inside another function. The only problem is that it’s easy to lose track of all the parentheses.\nThe next topic in the textbook is the box plot. This also gives an opportunity to talk about the quartiles and the median. We can display a box plot as follows.\n\nloan50 |&gt;\n  ggplot(aes(annual_income)) +\n  geom_boxplot() +\n  scale_x_continuous(labels = comma_format())\n\n\n\n\nThe thick line in the middle of the box is the median, the middle value of the data set. The box itself is bound by the first and third quartiles, known as hinges. The full name of this construct is actually a box and whiskers plot and the lines extending horizontally from the box are called whiskers. The upper whisker extends from the hinge to the largest value no further than 1.5 * IQR from the hinge (where IQR is the inter-quartile range, or distance between the first and third quartiles). The lower whisker extends from the hinge to the smallest value at most 1.5 * IQR of the hinge. Data beyond the end of the whiskers are called “outlying” points and are plotted individually.\nWe often want to create several box plots and compare them. This is easy to do as follows.\n\nloan50 |&gt;\n  ggplot(aes(annual_income,homeownership)) +\n  geom_boxplot() +\n  scale_x_continuous(labels = comma_format())\n\n\n\n\nThe textbook’s next topic is Robust Statistics and we’re going to pretty much skip that for now, except to say that outliers, as shown in the textbook, can affect the value of some statistics more than others. The mean and median are a good example. The median is much more robust to outliers than is the mean, which can be dragged way up or down by the presence of just one or a few outliers, whereas the median can not. As a kind of thought experiment, consider the following data set and the addition of an outlier and the effect of the outlier on the mean and median.\n\nx&lt;-c(2,3,3,3,4,4,4,5,5,6,6)\nmean(x)\n\n[1] 4\n\nmedian(x)\n\n[1] 4\n\ny&lt;-c(2,3,3,3,4,4,4,5,5,6,6,800)\nmean(y)\n\n[1] 70\n\nmedian(y)\n\n[1] 4\n\n\nFinally, the chapter considers maps. There is a kind of map called a choropleth map that shows the value of a numeric variable in a geographic region. An example of density of French restaurants follows. First, here’s an open source map of Sagarmatha, aka Mount Everest, followed by one of London, followed by one of Austin. To get the bounding box for Austin, I used https://norbertrenner.de/osm/bbox.html.\n\nlibrary(ggmap)\nbbox_everest &lt;- c(left = 86.05, bottom = 27.21, right = 87.81, top = 28.76)\nggmap(get_stamenmap(bbox_everest, zoom = 9))\n\n\n\nbbox_london&lt;-c(-0.489,51.28,0.236,51.686)\nggmap(get_stamenmap(bbox_london))\n\n\n\nbbox_austin &lt;- c(-97.818,30.179,-97.604,30.336)\nggmap(get_stamenmap(bbox_austin,zoom=12))"
  },
  {
    "objectID": "week02.html#categorical-data",
    "href": "week02.html#categorical-data",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.7 Categorical Data",
    "text": "2.7 Categorical Data\nWe’ve already seen contingency tables and how to manipulate them, which are introduced in more detail in this section. We’ve also seen a mosaic plot. Another kind of plot introduced in this section is the bar plot. We’ll examine each of these.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/migraine.rda\"))\ntbl &lt;- with(migraine,table(pain_free,group))\ntbl &lt;- addmargins(tbl)\nlibrary(kableExtra)\ntbl |&gt;\n  kbl() |&gt;\n  kable_classic(full_width=F) |&gt;\n  row_spec(3, color = \"white\", background = \"#AAAAAA\") |&gt;\n  column_spec(4, color = \"white\", background = \"#AAAAAA\") |&gt;\n  column_spec(1, color = \"black\", background = \"white\")\n\n\n\n\n\ncontrol\ntreatment\nSum\n\n\n\n\nno\n44\n33\n77\n\n\nyes\n2\n10\n12\n\n\nSum\n46\n43\n89\n\n\n\n\n\n\n\nAs before, we can create a mosaic plot.\n\ntbl &lt;- with(migraine,table(pain_free,group))\nmosaicplot(tbl)\n\n\n\n\nWe can also create bar plots.\n\nloan50 |&gt;\n  ggplot(aes(homeownership)) +\n    geom_bar()\n\n\n\n\n\nloan50 |&gt;\n  ggplot(aes(homeownership,fill=loan_purpose)) +\n    geom_bar() +\n    scale_fill_brewer()\n\n\n\n\nThe above looks better but is misleading because it implies an ordinal relationship between the loan purposes and there is no such relationship. We would be better of specifying that the loan_purpose variable is qualitative.\n\nloan50 |&gt;\n  ggplot(aes(homeownership,fill=loan_purpose)) +\n    geom_bar() +\n    scale_fill_brewer(type=\"qual\",palette=\"Set1\")\n\n\n\n\nI find the above palette to be ugly but several others are available. Google colorbrewer for more info. By the way, these colors have been extensively psychologically tested to verify that people can easily distinguish between them. I’m uncertain about colorblind people because the most prevalent form of color blindness is red-green."
  },
  {
    "objectID": "week02.html#thursdays-class",
    "href": "week02.html#thursdays-class",
    "title": "2  Numerical and Visual Data Summaries",
    "section": "2.8 Thursday’s class",
    "text": "2.8 Thursday’s class\n\n2.8.1 Statistical summary tools\n\nlibrary(ISLR2)\ndata(Auto)\nAuto &lt;- na.omit(Auto)\nwith(Auto,cylinders&lt;-as.factor(cylinders))\n\nHere are some statistical summary questions we can answer about the Auto data set.\n\nWhat is the range of each quantitative predictor? You can answer this using the range() function.\n\n\nwith(Auto,range(mpg))\n\n[1]  9 47\n\nwith(Auto,range(displacement))\n\n[1]  68 455\n\nsapply(Auto[,c(1,3:7)],range)\n\n     mpg displacement horsepower weight acceleration year\n[1,]   9           68         46   1613            8   70\n[2,]  47          455        230   5140           25   82\n\nsapply(Auto[,sapply(Auto,is.numeric)],range)\n\n     mpg cylinders displacement horsepower weight acceleration year origin\n[1,]   9         3           68         46   1613            8   70      1\n[2,]  47         8          455        230   5140           25   82      3\n\n\n\nWhat is the mean and standard deviation of each quantitative predictor?\n\n\nsapply(Auto[,c(1,3:7)],mean)\n\n         mpg displacement   horsepower       weight acceleration         year \n          23          194          104         2978           16           76 \n\nsapply(Auto[,c(1,3:7)],sd)\n\n         mpg displacement   horsepower       weight acceleration         year \n           8          105           38          849            3            4 \n\nas.data.frame(t(sapply(Auto[,c(1,3:7)],function(bla) list(means=mean(bla),sds=sd(bla),ranges=range(bla)))))\n\n             means sds     ranges\nmpg             23   8      9, 47\ndisplacement   194 105    68, 455\nhorsepower     104  38    46, 230\nweight        2978 849 1613, 5140\nacceleration    16   3      8, 25\nyear            76   4     70, 82\n\n\n\nNow remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\n\n\nas.data.frame(t(sapply(Auto[-10:-85,c(1,3:7)],function(bla) list(means=mean(bla),sds=sd(bla),ranges=range(bla)))))\n\n             means sds     ranges\nmpg             24   8     11, 47\ndisplacement   187 100    68, 455\nhorsepower     101  36    46, 230\nweight        2936 811 1649, 4997\nacceleration    16   3      8, 25\nyear            77   3     70, 82\n\n\n\n\n2.8.2 Exercises\nCreate a Quarto document called week02exercises.qmd. Use your name as the author name and the date as the current date. Make the title within the document “Week 2 Exercises”.\nAnswer the following questions in the document, using a combination of narration and R chunks.\n\nUse the loan50 data set. Find the mean and median of annual_income using R. Tell why they differ in words.\nUse the loan50 data set. Make a contingency table of loan_purpose and grade. Tell the most frequently occurring grade and most frequently occurring loan purpose in words.\nUse the loan50 data set. Provide a statistical summary of total_credit_limit.\nUse the loan50 data set. Show the column means for all numeric columns.\nUse the loan50 data set. Make a contingency table of state and homeownership. Tell which state has the most mortgages in words.\n\nNow render the document and submit both the .qmd file and the .html file to Canvas under “week02exercises”.\n\n\n2.8.3 Solutions to exercises\n\n\nUse the loan50 data set. Find the mean and median of annual_income using R. Tell why they differ in words.\n\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/loan50.rda\"))\nwith(loan50,mean(annual_income))\n\n[1] 86170\n\nwith(loan50,median(annual_income))\n\n[1] 74000\n\n\nThey differ because the mean is susceptible to outliers. There are about four outliers in this data set (high annual incomes) and they drag the mean upward but not the median. The median is a more reliable measure of centrality when there are influential outliers.\n\nUse the loan50 data set. Make a contingency table of loan_purpose and grade. Tell the most frequently occurring grade and most frequently occurring loan purpose in words.\n\n\nwith(loan50,addmargins(table(loan_purpose,grade)))\n\n                    grade\nloan_purpose             A  B  C  D  E  F  G Sum\n                      0  0  0  0  0  0  0  0   0\n  car                 0  0  1  1  0  0  0  0   2\n  credit_card         0  6  4  1  1  1  0  0  13\n  debt_consolidation  0  2  9  4  7  1  0  0  23\n  home_improvement    0  1  4  0  0  0  0  0   5\n  house               0  0  1  0  0  0  0  0   1\n  major_purchase      0  0  0  0  0  0  0  0   0\n  medical             0  0  0  0  0  0  0  0   0\n  moving              0  0  0  0  0  0  0  0   0\n  other               0  4  0  0  0  0  0  0   4\n  renewable_energy    0  1  0  0  0  0  0  0   1\n  small_business      0  1  0  0  0  0  0  0   1\n  vacation            0  0  0  0  0  0  0  0   0\n  wedding             0  0  0  0  0  0  0  0   0\n  Sum                 0 15 19  6  8  2  0  0  50\n\n\nThe most frequently occurring purpose is debt consolidation, while the most frequently occurring grade is B.\n\nUse the loan50 data set. Provide a statistical summary of total_credit_limit.\n\n\nwith(loan50,summary(total_credit_limit))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15980   70526  147364  208547  299766  793009 \n\nwith(loan50,scales::comma_format(summary(total_credit_limit)))\n\nfunction (x) \n{\n    number(x, accuracy = accuracy, scale = scale, prefix = prefix, \n        suffix = suffix, big.mark = big.mark, decimal.mark = decimal.mark, \n        style_positive = style_positive, style_negative = style_negative, \n        scale_cut = scale_cut, trim = trim, ...)\n}\n&lt;bytecode: 0x7ff2e7f196d0&gt;\n&lt;environment: 0x7ff2d9ce55b8&gt;\n\n#. same info with commas in numbers\nloan50 |&gt;\n    summarise(Min=comma(min(total_credit_limit)),\n              firstq=comma(quantile(total_credit_limit,0.25)),\n              Median=comma(median(total_credit_limit)),\n              Mean=comma(mean(total_credit_limit)),\n              thirdq=comma(quantile(total_credit_limit,0.75)),\n              Max=comma(max(total_credit_limit)))\n\n     Min firstq  Median    Mean  thirdq     Max\n1 15,980 70,526 147,364 208,547 299,766 793,009\n\n\n\nUse the loan50 data set. Show the column means for all numeric columns.\n\n\noptions(digits=1)\nformat(colMeans(loan50[sapply(loan50, is.numeric)]),scientific=FALSE,big.mark=\",\")\n\n             emp_length                    term           annual_income \n           \"        NA\"            \"     42.72\"            \" 86,170.00\" \n         debt_to_income      total_credit_limit   total_credit_utilized \n           \"      0.72\"            \"208,546.64\"            \" 61,546.54\" \nnum_cc_carrying_balance             loan_amount           interest_rate \n           \"      5.06\"            \" 17,083.00\"            \"     11.57\" \n public_record_bankrupt            total_income \n           \"      0.08\"            \"105,220.56\" \n\n\n\nUse the loan50 data set. Make a contingency table of state and homeownership. Tell which state has the most mortgages in words.\n\n\nwith(loan50,table(state,homeownership))\n\n     homeownership\nstate rent mortgage own\n         0        0   0\n   AK    0        0   0\n   AL    0        0   0\n   AR    0        0   0\n   AZ    0        0   1\n   CA    7        2   0\n   CO    0        0   0\n   CT    1        0   0\n   DC    0        0   0\n   DE    0        0   0\n   FL    1        2   0\n   GA    0        0   0\n   HI    1        1   0\n   ID    0        0   0\n   IL    3        0   1\n   IN    0        1   1\n   KS    0        0   0\n   KY    0        0   0\n   LA    0        0   0\n   MA    1        1   0\n   MD    2        1   0\n   ME    0        0   0\n   MI    0        1   0\n   MN    0        1   0\n   MO    0        1   0\n   MS    0        1   0\n   MT    0        0   0\n   NC    0        0   0\n   ND    0        0   0\n   NE    0        1   0\n   NH    1        0   0\n   NJ    2        1   0\n   NM    0        0   0\n   NV    0        2   0\n   NY    1        0   0\n   OH    0        1   0\n   OK    0        0   0\n   OR    0        0   0\n   PA    0        0   0\n   RI    0        1   0\n   SC    0        1   0\n   SD    0        0   0\n   TN    0        0   0\n   TX    0        5   0\n   UT    0        0   0\n   VA    1        0   0\n   VT    0        0   0\n   WA    0        0   0\n   WI    0        1   0\n   WV    0        1   0\n   WY    0        0   0\n\n\nTexas has five mortgages, more than any other state.\n\n\n2.8.4 Exercise Notes\n\nMany students did not follow instructions on file naming. I will take off a lot of points if this happens when you turn in a graded assignment. I expect all files to be uniformly named.\nSeveral students left the boilerplate verbiage in their .qmd file. I will take off a lot of points if this happens when you turn in a graded assignment.\nOne student put their narrative inside the code chunks as R comments. Don’t do this. It undercuts the purpose of mixing narrative and code in a Quarto document.\nSome students didn’t try to answer the second part of question 1. One way to understand this is to draw a boxplot of the data, showing that there are four outliers at the top end, dragging the mean upward but leaving the median pretty much alone.\n\n\nloan50 |&gt; ggplot(aes(annual_income)) + geom_boxplot()\n\n\n\n\n\nSome students included graphics, which don’t show up in the copy on Canvas. One way to make these graphics show up is to add the following code to the front matter (the front matter is the stuff between two sets of three dashes at the beginning of the file):\n\nformat:\n  html:\n    embed-resources: true\nThe indentation shown above is essential for it to work.\n\nSome students highlighted relevant rows and columns as shown below. This was a really great addition.\n\n\ntbl &lt;- with(loan50,table(loan_purpose,grade))\ntbl &lt;- addmargins(tbl)\nlibrary(kableExtra)\ntbl |&gt;\n  kbl() |&gt;\n  kable_classic(full_width=F) |&gt;\n  row_spec(4, color = \"white\", background = \"#AAAAAA\") |&gt;\n  column_spec(4, color = \"white\", background = \"#AAAAAA\") |&gt;\n  column_spec(1, color = \"black\", background = \"white\")\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nSum\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\ncar\n0\n0\n1\n1\n0\n0\n0\n0\n2\n\n\ncredit_card\n0\n6\n4\n1\n1\n1\n0\n0\n13\n\n\ndebt_consolidation\n0\n2\n9\n4\n7\n1\n0\n0\n23\n\n\nhome_improvement\n0\n1\n4\n0\n0\n0\n0\n0\n5\n\n\nhouse\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\nmajor_purchase\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nmedical\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nmoving\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nother\n0\n4\n0\n0\n0\n0\n0\n0\n4\n\n\nrenewable_energy\n0\n1\n0\n0\n0\n0\n0\n0\n1\n\n\nsmall_business\n0\n1\n0\n0\n0\n0\n0\n0\n1\n\n\nvacation\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nwedding\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nSum\n0\n15\n19\n6\n8\n2\n0\n0\n50\n\n\n\n\n\n\n\nAnother way to do this is to say\n\nmaxrow&lt;-max(tbl[1:length(levels(loan50$loan_purpose))-1,\"Sum\"])\nmaxcol&lt;-max(tbl[\"Sum\",1:length(levels(loan50$grade))-1])\n\nand\n\nmaxrownum &lt;- which.max(tbl[1:length(levels(loan50$loan_purpose))-1,\"Sum\"])\nmaxcolnum &lt;- which.max(tbl[\"Sum\",1:length(levels(loan50$grade))-1])+1\n\nNow you can plug maxrownum and maxcolnum into the formula without having to know which row and column you’re talking about.\n\ntbl |&gt;\n  kbl() |&gt;\n  kable_classic(full_width=F) |&gt;\n  row_spec(maxrownum, color = \"white\", background = \"#AAAAAA\") |&gt;\n  column_spec(maxcolnum, color = \"white\", background = \"#AAAAAA\") |&gt;\n  column_spec(1, color = \"black\", background = \"white\")\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nSum\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\ncar\n0\n0\n1\n1\n0\n0\n0\n0\n2\n\n\ncredit_card\n0\n6\n4\n1\n1\n1\n0\n0\n13\n\n\ndebt_consolidation\n0\n2\n9\n4\n7\n1\n0\n0\n23\n\n\nhome_improvement\n0\n1\n4\n0\n0\n0\n0\n0\n5\n\n\nhouse\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\nmajor_purchase\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nmedical\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nmoving\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nother\n0\n4\n0\n0\n0\n0\n0\n0\n4\n\n\nrenewable_energy\n0\n1\n0\n0\n0\n0\n0\n0\n1\n\n\nsmall_business\n0\n1\n0\n0\n0\n0\n0\n0\n1\n\n\nvacation\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nwedding\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nSum\n0\n15\n19\n6\n8\n2\n0\n0\n50\n\n\n\n\n\n\n\nAnd, in the narrative you can say that the maximum frequency of loan_purpose is 23. In the narrative you can alo say that the maximum frequency of grade is 19.\n\nOne student stipulated that the mean and median could not ever be the same except in two unusual circumstances. Actually it is quite easy for the mean to equal the mean as you can see from this simple example.\n\n\nx &lt;- c(1,2,3,4,5,6,7,8,9)\nmean(x)\n\n[1] 5\n\nmedian(x)\n\n[1] 5"
  },
  {
    "objectID": "week03.html#recap-week-02",
    "href": "week03.html#recap-week-02",
    "title": "3  More about R and Quarto",
    "section": "3.1 Recap Week 02",
    "text": "3.1 Recap Week 02\nWe did some exercises, for which there are now solutions in the file week02exercises-soln.qmd and week02exercises-soln.html. You should examine and compare these two files, especially the exercise parts."
  },
  {
    "objectID": "week03.html#week-03-more-on-r-and-quarto",
    "href": "week03.html#week-03-more-on-r-and-quarto",
    "title": "3  More about R and Quarto",
    "section": "3.2 Week 03: More on R and Quarto",
    "text": "3.2 Week 03: More on R and Quarto\nWe will establish groups for the milestones. I’m open to moving you around if needed, subject to the constraint that we have no more than five members in a group. I expect to have four groups of four and two groups of five.\n\n3.2.1 The template files\n\n\n3.2.2 Categorical Data\nThe template files show how you can begin the milestones. Notice that I have named the data frame as df in the template.qmd file. As a result, I can write functions like the following.\n\nlibrary(tidyverse)\ndf &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.csv\"))\ndf |&gt; count(title_status,sort=TRUE)\n\n# A tibble: 7 × 2\n  title_status      n\n  &lt;chr&gt;         &lt;int&gt;\n1 clean        405117\n2 &lt;NA&gt;           8242\n3 rebuilt        7219\n4 salvage        3868\n5 lien           1422\n6 missing         814\n7 parts only      198\n\n\nThe above is a good way to investigate categorical data. Notice that I’ve used the pipe character, so that the data frame df is sent to the count() function. Then a particular categorical column of df is counted and sorted in descending numerical order. The count() function is part of the dplyr package, which is one of the tidyverse packages. You only need to load the tidyverse set of packages once in a document, preferably near the beginning.\nAnother point about the above result is that the count() function behaves different for large numbers of values. For example, region has 404 values. The count() function will just display the first and last 5 by default. I can make it work by adding the print fuction:\n\ndf |&gt; count(region,sort=TRUE) |&gt; print(n=404)\n\n# A tibble: 404 × 2\n    region                         n\n    &lt;chr&gt;                      &lt;int&gt;\n  1 columbus                    3608\n  2 jacksonville                3562\n  3 spokane / coeur d'alene     2988\n  4 eugene                      2985\n  5 fresno / madera             2983\n  6 orlando                     2983\n  7 bend                        2982\n  8 omaha / council bluffs      2982\n  9 kennewick-pasco-richland    2981\n 10 new hampshire               2981\n 11 nashville                   2980\n 12 salem                       2980\n 13 oklahoma city               2979\n 14 reno / tahoe                2979\n 15 boston                      2978\n 16 rochester                   2978\n 17 sarasota-bradenton          2977\n 18 stockton                    2977\n 19 boise                       2976\n 20 portland                    2976\n 21 houston                     2975\n 22 south jersey                2974\n 23 minneapolis / st paul       2973\n 24 modesto                     2973\n 25 philadelphia                2973\n 26 seattle-tacoma              2973\n 27 grand rapids                2972\n 28 baltimore                   2971\n 29 las vegas                   2971\n 30 milwaukee                   2971\n 31 pittsburgh                  2971\n 32 cincinnati                  2970\n 33 sacramento                  2970\n 34 tulsa                       2970\n 35 washington, DC              2970\n 36 austin                      2969\n 37 north jersey                2969\n 38 tucson                      2969\n 39 charlotte                   2968\n 40 maine                       2966\n 41 atlanta                     2965\n 42 hawaii                      2964\n 43 long island                 2964\n 44 central NJ                  2961\n 45 phoenix                     2960\n 46 detroit metro               2959\n 47 dallas / fort worth         2957\n 48 kansas city, MO             2957\n 49 ft myers / SW florida       2955\n 50 cleveland                   2953\n 51 san diego                   2953\n 52 albuquerque                 2952\n 53 denver                      2952\n 54 orange county               2952\n 55 inland empire               2950\n 56 new york city               2950\n 57 norfolk / hampton roads     2946\n 58 tampa bay area              2945\n 59 st louis, MO                2940\n 60 los angeles                 2937\n 61 SF bay area                 2936\n 62 chicago                     2929\n 63 des moines                  2923\n 64 south florida               2920\n 65 raleigh / durham / CH       2918\n 66 colorado springs            2914\n 67 san antonio                 2891\n 68 knoxville                   2763\n 69 anchorage / mat-su          2742\n 70 hartford                    2564\n 71 albany                      2537\n 72 bakersfield                 2528\n 73 redding                     2526\n 74 springfield                 2520\n 75 ventura county              2518\n 76 vermont                     2513\n 77 fayetteville                2415\n 78 madison                     2387\n 79 fort collins / north CO     2385\n 80 richmond                    2346\n 81 greenville / upstate        2327\n 82 rhode island                2320\n 83 bellingham                  2313\n 84 indianapolis                2303\n 85 akron / canton              2211\n 86 greensboro                  2078\n 87 western massachusetts       2039\n 88 buffalo                     2006\n 89 palm springs                1978\n 90 el paso                     1977\n 91 medford-ashland             1962\n 92 louisville                  1938\n 93 worcester / central MA      1914\n 94 little rock                 1841\n 95 ocala                       1828\n 96 dayton / springfield        1787\n 97 yuba-sutter                 1747\n 98 memphis                     1724\n 99 yakima                      1704\n100 billings                    1701\n101 wichita                     1694\n102 hudson valley               1692\n103 birmingham                  1647\n104 new haven                   1644\n105 daytona beach               1633\n106 charleston                  1509\n107 chico                       1486\n108 san luis obispo             1455\n109 monterey bay                1451\n110 asheville                   1423\n111 toledo                      1406\n112 columbia                    1388\n113 missoula                    1378\n114 lansing                     1367\n115 jackson                     1351\n116 space coast                 1346\n117 syracuse                    1338\n118 fredericksburg              1328\n119 wenatchee                   1324\n120 new orleans                 1321\n121 huntsville / decatur        1273\n122 appleton-oshkosh-FDL        1261\n123 lehigh valley               1261\n124 flint                       1258\n125 columbia / jeff city        1257\n126 bozeman                     1249\n127 kalamazoo                   1236\n128 western slope               1188\n129 corpus christi              1176\n130 treasure coast              1173\n131 corvallis/albany            1168\n132 roanoke                     1121\n133 lexington                   1115\n134 tri-cities                  1110\n135 ann arbor                   1085\n136 rockford                    1059\n137 east idaho                  1052\n138 santa barbara               1052\n139 lakeland                    1048\n140 myrtle beach                1043\n141 chattanooga                 1038\n142 green bay                   1033\n143 winston-salem               1027\n144 mcallen / edinburg          1019\n145 scranton / wilkes-barre     1008\n146 moses lake                   994\n147 tyler / east TX              992\n148 lancaster                    988\n149 kalispell                    978\n150 harrisburg                   976\n151 wilmington                   975\n152 fargo / moorhead             969\n153 south bend / michiana        969\n154 st cloud                     954\n155 delaware                     949\n156 gainesville                  926\n157 visalia-tulare               922\n158 eastern NC                   895\n159 flagstaff / sedona           881\n160 saginaw-midland-baycity      876\n161 hickory / lenoir             853\n162 eau claire                   846\n163 jersey shore                 838\n164 erie                         805\n165 santa fe / taos              801\n166 twin falls                   794\n167 fort wayne                   788\n168 clarksville                  785\n169 york                         777\n170 tallahassee                  771\n171 gold country                 765\n172 duluth / superior            763\n173 kenosha-racine               757\n174 prescott                     746\n175 pueblo                       746\n176 santa maria                  740\n177 baton rouge                  733\n178 wausau                       726\n179 east oregon                  720\n180 olympic peninsula            717\n181 lewiston / clarkston         710\n182 skagit / island / SJI        701\n183 boulder                      694\n184 oregon coast                 694\n185 macon / warner robins        687\n186 quad cities, IA/IL           687\n187 waco                         681\n188 winchester                   672\n189 youngstown                   664\n190 killeen / temple / ft hood   662\n191 merced                       654\n192 cedar rapids                 648\n193 south coast                  645\n194 charlottesville              642\n195 battle creek                 639\n196 mobile                       626\n197 pensacola                    622\n198 sioux falls / SE SD          621\n199 northern michigan            612\n200 wyoming                      610\n201 athens                       607\n202 utica-rome-oneida            607\n203 lincoln                      604\n204 cape cod / islands           598\n205 pullman / moscow             595\n206 wichita falls                589\n207 eastern CT                   583\n208 topeka                       582\n209 amarillo                     564\n210 southern illinois            555\n211 waterloo / cedar falls       555\n212 holland                      534\n213 brainerd                     533\n214 monroe                       530\n215 great falls                  524\n216 la crosse                    521\n217 savannah / hinesville        516\n218 rapid city / west SD         502\n219 lynchburg                    496\n220 lubbock                      492\n221 lima / findlay               485\n222 salt lake city               485\n223 augusta                      480\n224 southwest michigan           476\n225 binghamton                   474\n226 muskegon                     473\n227 poconos                      464\n228 north mississippi            462\n229 central michigan             461\n230 mankato                      445\n231 finger lakes                 439\n232 mohave county                435\n233 odessa / midland             433\n234 peoria                       432\n235 danville                     427\n236 fairbanks                    427\n237 shreveport                   422\n238 reading                      416\n239 bowling green                415\n240 northwest GA                 415\n241 watertown                    414\n242 evansville                   413\n243 montgomery                   408\n244 southern maryland            405\n245 northwest CT                 397\n246 sioux city                   393\n247 humboldt county              385\n248 elmira-corning               379\n249 harrisonburg                 375\n250 eastern shore                368\n251 bemidji                      365\n252 altoona-johnstown            364\n253 st george                    362\n254 williamsport                 362\n255 brownsville                  361\n256 mansfield                    359\n257 las cruces                   358\n258 port huron                   358\n259 upper peninsula              355\n260 janesville                   343\n261 klamath falls                343\n262 boone                        338\n263 jonesboro                    337\n264 laredo                       337\n265 yuma                         335\n266 joplin                       328\n267 st augustine                 328\n268 fort smith                   327\n269 dothan                       325\n270 florence                     324\n271 st joseph                    323\n272 college station              313\n273 zanesville / cambridge       313\n274 morgantown                   307\n275 ithaca                       303\n276 panama city                  298\n277 roseburg                     294\n278 frederick                    288\n279 beaumont / port arthur       287\n280 the thumb                    286\n281 annapolis                    285\n282 western maryland             285\n283 texoma                       284\n284 imperial county              282\n285 champaign urbana             281\n286 sheboygan                    281\n287 new river valley             275\n288 glens falls                  274\n289 ashtabula                    270\n290 northern panhandle           268\n291 parkersburg-marietta         264\n292 northern WI                  262\n293 valdosta                     262\n294 lafayette / west lafayette   260\n295 chillicothe                  257\n296 plattsburgh-adirondacks      256\n297 helena                       255\n298 southwest VA                 254\n299 iowa city                    253\n300 eastern panhandle            251\n301 manhattan                    251\n302 muncie / anderson            248\n303 southeast missouri           245\n304 sandusky                     244\n305 victoria                     243\n306 sierra vista                 241\n307 mendocino county             238\n308 lake of the ozarks           236\n309 abilene                      235\n310 north central FL             230\n311 dubuque                      228\n312 galveston                    225\n313 chautauqua                   221\n314 kenai peninsula              221\n315 bloomington                  217\n316 bloomington-normal           217\n317 mason city                   217\n318 eastern kentucky             213\n319 grand island                 212\n320 western KY                   212\n321 texarkana                    211\n322 state college                210\n323 gadsden-anniston             207\n324 lawton                       207\n325 florida keys                 203\n326 lawrence                     199\n327 stillwater                   198\n328 terre haute                  198\n329 ames                         194\n330 okaloosa / walton            194\n331 grand forks                  192\n332 cookeville                   189\n333 brunswick                    187\n334 hilton head                  186\n335 cumberland valley            179\n336 lafayette                    179\n337 san marcos                   179\n338 hanford-corcoran             178\n339 meadville                    177\n340 southeast IA                 175\n341 decatur                      171\n342 heartland florida            170\n343 high rockies                 169\n344 salina                       168\n345 florence / muscle shoals     165\n346 tuscaloosa                   162\n347 san angelo                   161\n348 kokomo                       156\n349 lake charles                 156\n350 mattoon-charleston           156\n351 tuscarawas co                154\n352 logan                        152\n353 catskills                    147\n354 northwest OK                 145\n355 northwest KS                 143\n356 auburn                       142\n357 owensboro                    140\n358 statesboro                   140\n359 farmington                   136\n360 twin tiers NY/PA             136\n361 butte                        134\n362 central louisiana            132\n363 gulfport / biloxi            127\n364 north dakota                 126\n365 elko                         120\n366 kirksville                   119\n367 southeast KS                 119\n368 susanville                   119\n369 huntington-ashland           118\n370 scottsbluff / panhandle      117\n371 la salle co                  116\n372 show low                     112\n373 outer banks                  109\n374 del rio / eagle pass         108\n375 potsdam-canton-massena       102\n376 roswell / carlsbad           100\n377 hattiesburg                   99\n378 southwest KS                  96\n379 bismarck                      92\n380 south dakota                  90\n381 deep east texas               88\n382 houma                         88\n383 fort dodge                    87\n384 southeast alaska              84\n385 siskiyou county               83\n386 north platte                  80\n387 clovis / portales             78\n388 ogden-clearfield              76\n389 eastern montana               75\n390 provo / orem                  75\n391 western IL                    63\n392 oneonta                       62\n393 southwest MN                  56\n394 pierre / central SD           55\n395 eastern CO                    40\n396 southern WV                   36\n397 st louis                      35\n398 northeast SD                  34\n399 southwest TX                  30\n400 meridian                      28\n401 southwest MS                  14\n402 kansas city                   11\n403 fort smith, AR                 9\n404 west virginia (old)            8\n\n\nThe number 404 is because there are 404 unique elements in the list, which you can find out by looking at the output of the count() function.\nAnother solution (more cumbersome) follows.\n\ndf %&gt;%\n  group_by(region) %&gt;%\n  do(data.frame(nrow=nrow(.))) %&gt;%\n  arrange(desc(nrow)) %&gt;%\n  print(n=40)\n\n# A tibble: 404 × 2\n# Groups:   region [404]\n   region                    nrow\n   &lt;chr&gt;                    &lt;int&gt;\n 1 columbus                  3608\n 2 jacksonville              3562\n 3 spokane / coeur d'alene   2988\n 4 eugene                    2985\n 5 fresno / madera           2983\n 6 orlando                   2983\n 7 bend                      2982\n 8 omaha / council bluffs    2982\n 9 kennewick-pasco-richland  2981\n10 new hampshire             2981\n11 nashville                 2980\n12 salem                     2980\n13 oklahoma city             2979\n14 reno / tahoe              2979\n15 boston                    2978\n16 rochester                 2978\n17 sarasota-bradenton        2977\n18 stockton                  2977\n19 boise                     2976\n20 portland                  2976\n21 houston                   2975\n22 south jersey              2974\n23 minneapolis / st paul     2973\n24 modesto                   2973\n25 philadelphia              2973\n26 seattle-tacoma            2973\n27 grand rapids              2972\n28 baltimore                 2971\n29 las vegas                 2971\n30 milwaukee                 2971\n31 pittsburgh                2971\n32 cincinnati                2970\n33 sacramento                2970\n34 tulsa                     2970\n35 washington, DC            2970\n36 austin                    2969\n37 north jersey              2969\n38 tucson                    2969\n39 charlotte                 2968\n40 maine                     2966\n# ℹ 364 more rows\n\n\nAnother way to investigate categorical data is through contingency tables. You have already made some of these using the table() function and some associated functions that are mentioned in the week02exercises-soln.qmd file. It will take some intuition to figure out which pairs of categorical columns should be tabulated for Milestone 1.\n\n\n3.2.3 Numerical Data\nFor numerical data, you can do what we did last week to investigate a single column.\n\nsummary(df$price)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.000e+00 5.900e+03 1.395e+04 7.520e+04 2.649e+04 3.737e+09 \n\n\nor\n\nlibrary(scales)\ndf$price &lt;- as.double(df$price)\ndf %&gt;%\n    summarize(Min=comma(min(price)),\n              firstq=comma(quantile(price,0.25)),\n              Median=comma(median(price)),\n              Mean=comma(mean(price)),\n              thirdq=comma(quantile(price,0.75)),\n              Max=comma(max(price)))\n\n# A tibble: 1 × 6\n  Min   firstq Median Mean   thirdq Max          \n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;        \n1 0     5,900  13,950 75,199 26,486 3,736,928,711\n\ndf$price &lt;- as.integer(df$price)\n\nWarning: NAs introduced by coercion to integer range\n\n\nYou’ll notice that one of the prices is 3.7 billion dollars for a Toyota truck! This is obviously a misprint! You should probably remove this row from the data frame and save the data frame without it. A more sophisticated alternative would be to impute a value for this truck. There are many advanced statistical ways to do this, but they are beyond the scope of this course. van Buuren (2018) describes several excellent ways to do so, particularly in that book’s Section 5.1.1, Recommended Workflows. It is usually a mistake to use an average for missing (NA) values because to do so compresses the variance unnaturally. You may remove that particular row by saying df &lt;- df[-(which.max(df$price)),]. Unfortunately, you will find that not to be very useful because there are several prices of over a billion dollars for generic cars! In fact, there are several with prices listed as 1234567890 and some listed at 987654321. Many other ridiculous patterns can be found for price. So what can you do? Personally, I might add the following line right after reading in the file:\n\ndf &lt;- df[df$price&lt;100000&df$price&gt;0,]\n\nor, better yet,\n\ndf &lt;- df[df$price &lt; quantile(df$price,.9,na.rm=TRUE) & df$price &gt; quantile(df$price,.1,na.rm=TRUE),]\n\nThe first one will rid the data frame of cars priced at greater than 100,000 dollars and still leave you with over 300,000 automobiles to analyze. Of course, there are will still be spurious entries, but at least it’s a start. The first code will also get rid of cars priced at exactly zero dollars. Examination of the data frame will show that many of the zero dollar entries are just ads for used car dealers. The expression price&lt;100000&price&gt;0 is called a compound Boolean expression. It is compound because of the ampersand, which stands for the word and. It means that the row has to contain a price less than 100,000 AND it also has to be a price greater than zero.\nThe second code gets rid of the 90th percentile and above and the 10th percentile and below, which will still leave plenty.\nBy the way, this is a good reminder that a data frame has a row and a column index. You can refer to rows by saying df[expression,] and to columns by saying df[,expression]. The expression can be any mathematical expression that resolves to TRUE or FALSE. The first above expression resolves to TRUE for cars priced at greater than zero but less than 100,000 dollars, and FALSE for cars priced at any integer greater than or equal to 100,000. You can tell that price is a 64 bit integer by saying str(df) which will tell the structure of the df data frame. You should be able to see that most of the columns are classified as chr or character. This is not desirable. Most of the columns clasified as chr should more properly be classified as factors. Factors take up less space on your computer, are faster to process, and allow more types of processing than chr. Unfortunately, if you store your intermediate work as a .csv file, you will lose the factor designation. Therefore, I recommend that you do the following.\nStep 1. Get rid of rows you don’t want, such as those with prices over or under some threshold value you choose.\nStep 2. Get rid of columns you don’t want to analyze, such as url or VIN.\nStep 3. Convert some of the chr columns to factor. For instance, you can say df$state &lt;- as.factor(df$state).\nStep 4. Save your file by saying something like save(df,file=\"df.RData\")\nStep 5. Quit using this file and open a file called intermediate.qmd\nStep 6. At the beginning of that file, say load(\"df.RData\").\nStep 7. Do all your work in that file, then paste the work back into your template.qmd file so you can run it as required. (Remember, you are not turning in a .RData file. Your m1.qmd file must start with reading in the vehicles.csv file and do processing on the resulting data frame.)\nStep 8. Merge your template.qmd file with those of your group members into one m1.qmd file. For example, you could name all your individual template files with your names and one group member could merge them together. This should be easy for Milestone 1 since an obvious way to divide up your work is to assign different columns to different group members.\n\n\n3.2.4 Combining numerical and categorical selection\n\ndfX &lt;- subset(df,state %in% c(\"ca\",\"ny\") & type %in% c(\"sedan\",\"SUV\") & price&lt;99999 & price&gt;0)\ntbl &lt;- table(dfX$state,dfX$type)\naddmargins(tbl)\n\n     \n      sedan   SUV   Sum\n  ca  10313  6537 16850\n  ny   3487  2953  6440\n  Sum 13800  9490 23290\n\n\nAbove is an example of getting a small contingency table with only the data you want. The first line selects only cars offered in ca or ny, only sedans or SUVs, and only with prices below 99,999 dollars and more than zero dollars. Then we can make a compact contingency table of that new data frame and add the margins to it.\n\n\n3.2.5 Getting the data displayed as you wish\nSomeone asked me how to display price ranges by manufacturer. Here’s one way to do that:\n\ndf |&gt;\n  group_by(manufacturer) |&gt;\n  reframe(min = min(price),max=max(price)) |&gt;\n  print(n=43)\n\n# A tibble: 43 × 3\n   manufacturer      min   max\n   &lt;chr&gt;           &lt;int&gt; &lt;int&gt;\n 1 acura             574 37500\n 2 alfa-romeo       1000 37500\n 3 aston-martin     1947 37000\n 4 audi              504 37509\n 5 bmw               501 37500\n 6 buick             521 37500\n 7 cadillac          507 37500\n 8 chevrolet         502 37587\n 9 chrysler          539 37500\n10 datsun           2200 30000\n11 dodge             513 37500\n12 ferrari          2034 35000\n13 fiat              800 32500\n14 ford              502 37513\n15 gmc               501 37550\n16 harley-davidson  2500 27995\n17 honda             502 37500\n18 hyundai           543 36900\n19 infiniti          517 37500\n20 jaguar            514 37388\n21 jeep              501 37587\n22 kia               529 37500\n23 land rover       3199 10999\n24 lexus             521 37500\n25 lincoln           541 37495\n26 mazda             515 36995\n27 mercedes-benz     502 37509\n28 mercury           600 37495\n29 mini              600 35990\n30 mitsubishi        538 37500\n31 morgan           1000 36500\n32 nissan            522 37575\n33 pontiac           600 37500\n34 porsche           560 37500\n35 ram               502 37556\n36 rover             507 37500\n37 saturn            600 22000\n38 subaru            501 37200\n39 tesla             563 36999\n40 toyota            505 37583\n41 volkswagen        507 37000\n42 volvo             502 37000\n43 &lt;NA&gt;               NA    NA\n\n\nThe number 43 is because there are 43 manufacturers in the data frame. Note that, using the reframe() function, I could add a few more comma-separated statistics to the output.\n\n\n3.2.6 Investigating words\nTo make a word cloud, I first exported the model column from the data frame to a file called bla, using the write_csv() function. Next I used Vim to convert all spaces to newlines, so that the file would have one word on each line. To do so I said :%s/ /\\r/g in Vim.\nNext I said sort bla | uniq -c | sort -n &gt;blabla to get the following output. This is just the last few lines of the file. Note that the most frequently occuring word in the file is 1500, which occurs 24,014 times.\n3383    fusion\n3475    tundra\n3479    xl\n3528    corolla\n3585    unlimited\n3985    altima\n3985    explorer\n4072    3500\n4105    f-250\n4256    mustang\n4277    escape\n5162    camry\n5208    series\n5244    pickup\n5277    NA\n5479    accord\n5585    xlt\n5659    awd\n5660    cherokee\n5667    f150\n5680    tacoma\n5734    civic\n5744    s\n5865    2d\n6185    limited\n6213    lt\n6295    coupe\n6519    crew\n6869    premium\n7190    duty\n7319    se\n7531    2500\n7564    utility\n8093    wrangler\n8327    super\n8642    sierra\n8733    grand\n9578    4x4\n10283   f-150\n14877   sedan\n15152   cab\n17181   silverado\n17488   4d\n23130   sport\n24014   1500\nNext I opened the blabla file in Vim and converted all sequences of spaces to a tab character, saying :%s/^  *// to get rid of leading spaces, then :%s/  */\\t/ to convert the intercolumn spaces to tabs. I saved this file as wordfreq.tsv and opened it in R, using the following code to convert it to a word cloud.\n\nlibrary(tidyverse)\ndf&lt;-read_tsv(\"wordfreq.tsv\")\ndf&lt;-df|&gt;relocate(freq,.after=word)\nhead(df)\n\n# A tibble: 6 × 2\n  word         freq\n  &lt;chr&gt;       &lt;dbl&gt;\n1 \"!\"           102\n2 \"!!\"            4\n3 \"!!!\"           2\n4 \"!!!\\\"\"         1\n5 \"!!clean!!\"     2\n6 \"!!leveled\"     2\n\nlibrary(wordcloud2)\nwordcloud2(df)\n\n\n\n\n\n\nI was not able to reproduce the error message I kept getting in class. This simply worked the first time through after class. I also can not explain why several of the most frequently occurring words do not appear in the word cloud. I suspect it is because the word cloud is truncated in this display. When I have constructed this word cloud previously, it was ellipse-shaped and zoomed out. I have forgotten whatever I did to make that happen!\n\n\n3.2.7 Filtering two specific trucks\nTwo of the most common words I found above were f-150 and silverado. Since these are two popular truck models, I thought to compare them by making a data frame for each. To do so I used the filter() function of the dplyr package. This is described in great detail in Chapter 4 of Wickham, Çetinkaya-Rundel, and Grolemund (2023). The special construction (?i) makes whatever follows case-insensitive. Thus, in this case, it picks up Silverado and SILVERADO, as well as silverado. It would also pick up sIlVeRaDo if such a strange mixed case version presented itself. This is called a regular expression or regex and is commonly used in finding and replacing text patterns.\nThe regexes for f-150 are much more complicated. First, I used the alternating selector [Ff]. This stands for either a capital F or small f but not both. I can put any number of characters in the brackets and this will select any of them occurring one time. Next, I used the zero-or one character selector, which consists of a dot followed by a question mark. That selector attaches itself to whatever precedes it, in this case [Ff]. So the whole construct [Ff].? can be read as “exactly one F or f followed by exactly one character.” Next comes a literal 150, so the only time that an F or f plus at most one character will be matched is if it is immediately followed by 150. The last construct in this regular expression is the negating alternator, [^] with a zero in it. The negating alternator matches anything except the characters following the ^ in brackets. In this case it means that any character except a zero is okay. It’s actually a vestige of an earlier attempt. I had previously written the expression as [Ff].*150 and run it and it erroneously picked up a model string that said “pacifica $1500”. That was because I used a * instead of a ?. The * symbol means zero or more characters. As a result, the words “pacifica $1500” matched because there is an f followed by some characters, followed by 150! So I stuck the negating alternator [^0] in to get rid of the 15000 before I realized that the real problem was the *. I report this so you can see that it is a potentially long iterative process to find the right regex. This regex picks up f150, F 150, f-150, and so on. I could have refined it further. Can you see how?\n\nlibrary(tidyverse)\n#df &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.csv\"))\n#load(\"/home/mm223266/data/vehicles.Rdata\")\n#df &lt;- read_csv(\"/home/mm223266/data/vehicles.csv\")\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.Rdata\"))\n#names(df)\ndf2 &lt;- df |&gt; filter(str_detect(df$model,regex(\"(?i)silverado\")))\nhead(table(df2$model),12)\n\n\n      1500 sierra silverado              1500 silverado \n                          1                          23 \n         1500 silverado 4x4      1500 silverado ltz z71 \n                          5                           2 \n      1500 silverado sierra        1500 silverado titan \n                          1                           2 \n         1500 silverado z71          1980 Silverado K10 \n                          1                           1 \n             2000 Silverado              2006 Silverado \n                          1                           1 \n     2006 SILVERADO 2500 HD 2007 Silverado crew cab 4x4 \n                          1                           1 \n\nprint(\"::::::::::::::::::::::::::::::::::\")\n\n[1] \"::::::::::::::::::::::::::::::::::\"\n\ndf3 &lt;- df |&gt; filter(str_detect(df$model,regex(\"[Ff].?150[^0]\")))\nhead(table(df3$model),12)\n\n\n              1999 F150 4X4               1999 f150 xlt \n                          1                           2 \n      2000 F150 4x4 5 speed      2000 F150 XLT SUPERCAB \n                          1                           1 \n             2002 F150 4 WD               2004 f150 4x4 \n                          1                           1 \n              2004 F150 XLT               2005 f150 fx4 \n                          1                           1 \n           2006 F150 Lariat     2006 f150 Supercrew Cab \n                          1                           1 \n2012 F-150 Lariat SuperCrew        2013 F150 KING RANCH \n                          1                           1 \n\n\n\n\n3.2.8 A function between numeric and visual\nMilestone 1 is supposed to be entirely about numeric descriptions of the data, not visual descriptions, which will be covered in Milestone 2. Yet there is one function that exists in a gray area between numeric and graphical. That is the stem and leaf plot. Consider the following output.\n\nstem(df$odometer)\n\n\n  The decimal point is 6 digit(s) to the right of the |\n\n  0 | 00000000000000000000000000000000000000000000000000000000000000000000+421502\n  1 | 00000000000000000000000000000000000000000000000000000000000000000000+562\n  2 | 000000000000000000111122223333333335555566666677789\n  3 | 02333357\n  4 | 0577777777\n  5 | 005556666666669\n  6 | 15\n  7 | 555678888888888888\n  8 | 04789999\n  9 | 000189\n  10 | 00000000000000000000000000000000000000000000000000000000000000000000+58\n\n\nThe output does not need any graphical processor. It is only characters that can be included in text. Yet it is a kind of graphic because you can see, for instance, that of the cars have either very little mileage on the odometer or very much. Read it like this:\n\nThe stem is the vertical line.\nThe numbers to the left of the stem are, in this case, numbers in the sixth place to the left of the decimal point. In other words the first row represents zero to 999999.\nEach character to the right of the stem represents one car. There are probably 80 zeros in the first row. The +390151 indicates that there are 390,151 cars in that category that are not represented. The numbers in these cases represent the next significant digit after the one on the stem.\nIt is probably easier to read a stem and leaf plot for a smaller data frame, in the following case for the first 100 cars in the above data frame.\n\n\nstem(df$odometer[1:100])\n\n\n  The decimal point is 4 digit(s) to the right of the |\n\n   0 | 0222788900012223447789999\n   2 | 11267799000004555678\n   4 | 01123335568\n   6 | 39117\n   8 | 0804567\n  10 | 0\n  12 | 8\n  14 | 5\n  16 | 6\n  18 | 2\n\n\nThese entries come from a reduced data frame where I first ran the above code, getting rid of the high-priced and free cars. It may make it easier to understand to look at the entries themselves.\n\nhead(df$odometer,n=100L) |&gt; sort(decreasing=TRUE)\n\n [1] 192000 176144 144700 128000  99615  97000  96003  95000  94020  90000\n[11]  88000  80318  77087  71229  70760  68696  63129  57923  55783  55251\n[21]  55068  43182  43000  42755  41568  41124  40784  39508  37725  37332\n[31]  35835  35320  35290  34940  34152  30237  30176  30047  30041  29652\n[41]  29499  28942  26978  26685  26129  22120  20856  20581  19179  19160\n[51]  18650  18531  17805  17302  16594  14230  14169  13035  12302  12231\n[61]  12102  10688   9954   9859   9704   8663   8141   7885   6897   2195\n[71]   1834   1722     21\n\n\nThe very first row in the stem and leaf plot above counts cars priced at less than 20,000 dollars. There are 28 of them. They all have 0 or 1 in the fifth position to the left of the decimal. Only one of those, which is offered at 21 dollars, has zeros in both of the first two positions. It is the very first entry after the stem, represented as a zero. The next three entries are the cars that sell for the next lowest prices, between zero and 2 in the next decimal position. They are represented as 2s. You can see at a glance that, in this group of 100 cars, the lower odometer readings predominate. By the way, the stem() function discards NA values before processing the remainder. So there are only a total of 78 characters to the right of the stem on all the rows put together.\nThere is some difference of opinion as to whether stem() is graphical or numerical. What do you think?\n\n\n\n\nvan Buuren, Stef. 2018. Flexible Imputation of Missing Data. Boca Raton, FL: CRC Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd ed. O’Reilly Media, Inc. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "week04.html#recap-week-03",
    "href": "week04.html#recap-week-03",
    "title": "4  Introduction to Probability",
    "section": "4.1 Recap Week 03",
    "text": "4.1 Recap Week 03\nWe did some exercises, for which there are now solutions in the file week03exercises-soln.qmd and week03exercises-soln.html. You should examine and compare these two files.\nLast semester, for each student, we calculated the number of exercises successfully completed. Following is a stem and leaf diagram, as well as summary statistics.\n\nx&lt;-scan(\"week03exercisesList.txt\")\nstem(x)\n\n\n  The decimal point is at the |\n\n   0 | 000\n   2 | 0\n   4 | 0000000\n   6 | 00000000\n   8 | 0000000\n  10 | 0\n\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    4.50    6.00    5.63    8.00   10.00 \n\nsd(x)\n\n[1] 2.691127\n\n\nYou can make a number of inferences from this information. First, we should generally expect the average student to be able to complete six exercises during class time. If you completed fewer, you may need extra work outside class. If you completed more, you probably need less time outside of class. We can also see that the mean has been dragged downward by the students who didn’t turn anything in, so the median is a better measure of centrality.\nWe can compare the stem and leaf plot to the following histogram and see that the stem and leaf plot looks kind of like a histogram turned on its side, and with a bit more information, especially if we alter the scale, so that it’s not compressed to two exercises per row.\n\nhist(x)\n\n\n\nstem(x,scale=2)\n\n\n  The decimal point is at the |\n\n   0 | 000\n   1 | \n   2 | 0\n   3 | \n   4 | 000\n   5 | 0000\n   6 | 00000\n   7 | 000\n   8 | 000000\n   9 | 0\n  10 | 0\n\n\nWe also talked about milestone 1, for which there are some hints in the previous chapter.\n\n4.1.1 Name value pairs\nIt came to my attention that not everyone knows what a name value pair is or how to read it. The name is always on the left and the value is always on the right. Usually there is an equal sign in between, but it doesn’t mean equals. Instead it means gets. So, for example, in ggplot, there are aesthetics x and y. You can say something like x=bill_depth_mm and y=bill_length_mm. You would read the first one as “x gets bill depth in millimeters”. It would make no sense to say the reverse. Don’t ever say bill_depth_mm=x. Instead, let x and y be the names that ggplot knows about, the x-axis and the y-axis, and bill_depth_mm and bill_length_mm be the values that we plug into x and y.\nSimilarly, I found out that not everyone knows what we mean by local and remote. The local machine is your laptop. It is the machine close to you. The remote machine is the machine that hosts RStudio Server, the machine you connect to when you access the URL for RStudio. When you work in RStudio on the RStudio server, your work gets saved to the remote machine. Then you have to download it to your local machine and upload it to the machine that hosts Canvas. There is no direct path between the two remote machines that host RStudio Server and Canvas."
  },
  {
    "objectID": "week04.html#bayesian-approach",
    "href": "week04.html#bayesian-approach",
    "title": "4  Introduction to Probability",
    "section": "4.2 Bayesian approach",
    "text": "4.2 Bayesian approach\nThere are two competing schools of thought about what probability is. The bayesian approach is that probability is quantified belief or reasonable expectation of the outcomes of events based on a state of knowledge. This approach is recently taught in graduate schools. We will not extensively study this approach, but I want you to know that it exists and is rising in academic popularity."
  },
  {
    "objectID": "week04.html#frequentist-approach",
    "href": "week04.html#frequentist-approach",
    "title": "4  Introduction to Probability",
    "section": "4.3 Frequentist approach",
    "text": "4.3 Frequentist approach\nOur textbook takes a frequentist approach to probability, one of the two main approaches to probability and the one usually taught in undergraduate courses in the USA. This approach models probability of an outcome as the number of times the outcome would occur if we observed the random process that produced it an infinite number of times. For example, if we flip a fair coin an infinite number of times, it comes up heads half the time, so the probability of heads is 0.5."
  },
  {
    "objectID": "week04.html#the-law-of-large-numbers",
    "href": "week04.html#the-law-of-large-numbers",
    "title": "4  Introduction to Probability",
    "section": "4.4 The law of large numbers",
    "text": "4.4 The law of large numbers\nThis law claims that, as more outcomes are observed, the proportion of outcomes converges to the probability of the outcome. For example, if we flip a fair coin a hundred times, the probability of heads coming up half the time is greater than if we only flip it ten times.\nThe textbook uses the examples of rolling fair dice and flipping fair coins. Gaston Sanchez gives the example of flipping a fair coin modeled in R.\n\n#. number of flips\nnum_flips &lt;- 1000\n\n#. flips simulation\ncoin &lt;- c('heads', 'tails')\nflips &lt;- sample(coin, size = num_flips, replace = TRUE)\n\n#. number of heads and tails\nfreqs &lt;- table(flips)\nfreqs\n\nflips\nheads tails \n  511   489 \n\nheads_freq &lt;- cumsum(flips == 'heads') / 1:num_flips\nplot(heads_freq,      # vector\n     type = 'l',      # line type\n     lwd = 2,         # width of line\n     col = 'tomato',  # color of line\n     las = 1,         # orientation of tick-mark labels\n     ylim = c(0, 1),  # range of y-axis\n     xlab = \"number of tosses\",    # x-axis label\n     ylab = \"relative frequency\")  # y-axis label\nabline(h = 0.5, col = 'gray50')"
  },
  {
    "objectID": "week04.html#disjoint-outcomes",
    "href": "week04.html#disjoint-outcomes",
    "title": "4  Introduction to Probability",
    "section": "4.5 Disjoint outcomes",
    "text": "4.5 Disjoint outcomes\nThese are outcomes that can not both happen. For example, in the fair coin flipping case, the outcome cannot be both heads and tails. But the sum of all the disjoint probabilities is always 1."
  },
  {
    "objectID": "week04.html#probabilities-when-outcomes-are-not-disjoint",
    "href": "week04.html#probabilities-when-outcomes-are-not-disjoint",
    "title": "4  Introduction to Probability",
    "section": "4.6 Probabilities when outcomes are not disjoint",
    "text": "4.6 Probabilities when outcomes are not disjoint\nThe textbook uses playing cards to illustrate concepts like cards that are neither diamonds nor face cards. You have to familiarize yourself with playing cards to understand these examples. The textbook uses the following Venn diagram to illustrate the above example."
  },
  {
    "objectID": "week04.html#general-addition-rule",
    "href": "week04.html#general-addition-rule",
    "title": "4  Introduction to Probability",
    "section": "4.7 General addition rule",
    "text": "4.7 General addition rule\nThe textbook gives a general rule for multiple outcomes, whether they are disjoint or not.\nIf A and B are any two events, disjoint or not, then the probability that at least one of them will occur is\n\\[\nP (A\\text{ or }B) = P (A) + P (B) − P (A\\text{ and }B)\n\\]\nwhere \\(P (A\\text{ and }B)\\) is the probability that both events occur."
  },
  {
    "objectID": "week04.html#counting-permutations-and-combinations",
    "href": "week04.html#counting-permutations-and-combinations",
    "title": "4  Introduction to Probability",
    "section": "4.8 Counting Permutations and Combinations",
    "text": "4.8 Counting Permutations and Combinations\nPermutations can be thought of as lineups. For instance, suppose you have five people to put in a line. There are five people to choose from to be first in line, then four people remain to be second in line, and so on. You can count this up as \\(5 \\times 4 \\times 3 \\times 2 \\times 1 = 5!\\) or five factorial. This holds true for as many objects as you wish to line up.\nCombinations can be thought of as committees. There is no order as in a lineup. You’re either a member or you’re not. Suppose you want to form a committee of five people from among twenty people. It doesn’t matter what order they come in so you can’t use the factorial method to count them. There is another method, which you can find described in detail in Ash (1993). The main result is that, to choose a committee of 5 from among 20 people, use\n\\[\n\\binom{20}{5} = \\frac{20!}{5!(20-5)!}\n\\]\nThis is read as twenty choose 5.\nKeep in mind that\n\\[\n\\binom{n}{r}=\\binom{n}{n-r}\n\\]\nand\n\\[\n\\binom{n}{1}=n\n\\]\nand\n\\[\n\\binom{n}{n}={n}{0}=1\n\\]\nThis last result is because \\(0!=1\\) by definition.\nAsh (1993) gives the examples of finding and not finding the Queen of Spades (\\(Q_s\\)) in a poker hand. You can think of a poker hand as a committee of 5 cards drawn from 52, so the total number of poker hands is given by \\(\\binom{52}{5}\\). Finding hands containing the \\(Q_s\\) amounts to choosing a committee of size four (the remainder of the hand, from among 51 cards (the remainder of the deck. So there are \\(\\binom{51}{4}\\) such hands.\n\\[\nP(Q_s)=\\dfrac{\\binom{51}{4}}{\\binom{52}{5}}=\\frac{5}{52}\n\\]\nKeep in mind when canceling in stacked fractions that\n\\[\n\\dfrac{\\frac{a}{b}}{\\frac{c}{d}}=\\dfrac{a \\cdot d}{b \\cdot c}\n\\]"
  },
  {
    "objectID": "week04.html#probability-distributions",
    "href": "week04.html#probability-distributions",
    "title": "4  Introduction to Probability",
    "section": "4.9 Probability distributions",
    "text": "4.9 Probability distributions\nA probability distribution is a list of the possible outcomes with corresponding probabilities that satisfies three rules:\n\nThe outcomes listed must be disjoint.\nEach probability must be between 0 and 1.\nThe probabilities must total 1."
  },
  {
    "objectID": "week04.html#probability-distribution-for-two-fair-dice",
    "href": "week04.html#probability-distribution-for-two-fair-dice",
    "title": "4  Introduction to Probability",
    "section": "4.10 Probability distribution for two fair dice",
    "text": "4.10 Probability distribution for two fair dice\nFrancis DiTraglia shows the following example of plotting the probability distribution for rolling two fair dice on his website.\n\ntwo.dice &lt;- function() {\n  dice &lt;- sample(1:6, size = 2, replace = TRUE)\n  return(sum(dice))\n}\nsims &lt;- replicate(1000, two.dice())\nplot(table(sims), xlab = 'Sum', ylab = 'Frequency', main = '100 Rolls of 2 Fair Dice')\n\n\n\n\nWhy is 7 the most likely outcome?"
  },
  {
    "objectID": "week04.html#terms-from-set-theory",
    "href": "week04.html#terms-from-set-theory",
    "title": "4  Introduction to Probability",
    "section": "4.11 Terms from set theory",
    "text": "4.11 Terms from set theory\nSet theory is a mathematical discipline that uses tools like Venn diagrams to describes sets of objects. We can think of outcomes from random processes as objects, too, with the following terms.\n\nsample space: the set of all possible outcomes\nevent: a particular outcome\ncomplement of an event: outcomes in the sample space outside a given event or events\n\nThe complement of event \\(A\\) is denoted \\(A^c\\), and \\(A^c\\) represents all outcomes not in \\(A\\). \\(A\\) and \\(A^c\\) are mathematically related:\n\\[\nP (A) + P (A^c) = 1, \\text{ i.e. } P (A) = 1 − P (A^c)\n\\]"
  },
  {
    "objectID": "week04.html#independence",
    "href": "week04.html#independence",
    "title": "4  Introduction to Probability",
    "section": "4.12 Independence",
    "text": "4.12 Independence\nJust as variables and observations can be independent, random processes can be independent, too. Two processes are independent if knowing the outcome of one provides no useful information about the outcome of the other. For instance, flipping a coin and rolling a die are two independent processes—knowing the coin was heads does not help determine the outcome of a die roll. On the other hand, stock prices usually move up or down together, so they are not independent."
  },
  {
    "objectID": "week04.html#multiplication-rule-for-independent-processes",
    "href": "week04.html#multiplication-rule-for-independent-processes",
    "title": "4  Introduction to Probability",
    "section": "4.13 Multiplication rule for independent processes",
    "text": "4.13 Multiplication rule for independent processes\nIf \\(A\\) and \\(B\\) represent events from two different and independent processes, then the probability that both \\(A\\) and \\(B\\) occur can be calculated as the product of their separate probabilities:\n\\[\nP (A\\text{ and }B) = P (A) × P (B)\n\\]\nSimilarly, if there are \\(k\\) events \\(A_1, \\ldots, A_k\\) from \\(k\\) independent processes, then the probability they all occur is\n\\[\nP (A_1) × P (A_2) × \\cdots × P (A_k)\n\\]"
  },
  {
    "objectID": "week04.html#conditional-probability",
    "href": "week04.html#conditional-probability",
    "title": "4  Introduction to Probability",
    "section": "4.14 Conditional probability",
    "text": "4.14 Conditional probability\nThis is where probability gets interesting. Some things depend on other things! The textbook uses a contingency table of the photos_classify data frame, which you can download from OpenIntro Stats, to explore this concept.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/photo_classify.rda\"))\nstr(photo_classify)\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   1822 obs. of  2 variables:\n $ mach_learn: Factor w/ 2 levels \"pred_fashion\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ truth     : Factor w/ 2 levels \"fashion\",\"not\": 1 1 1 1 1 1 1 1 1 1 ...\n\naddmargins(table(photo_classify))\n\n              truth\nmach_learn     fashion  not  Sum\n  pred_fashion     197   22  219\n  pred_not         112 1491 1603\n  Sum              309 1513 1822\n\n\nWe can use the entries in the contingency table to make statements about probability.\n\nprobability that a fashion photo is correctly classified by ML: 197/309\nprobability that a given photo is about fashion when predicted by ML to be not: 112/1603\n\nMarginal probability is the probability in the margins of table (right column and bottom row), e.g., ML predicts fashion photo at all: 219/1822.\nJoint probability is the probabillity of two (or more) things being true, e.g., ML predicts fashion and truth is fashion: 197/1822. A joint probability would be any of the four interior cells divided by the lower right cell.\nConditional probability is the probability of some outcome given the condition of another outcome, such as the probability that ML predicts fashion when the photo is truly about fashion: 197/219.\nConditional probability is very important. It’s useful to know the general formula for conditional probability. The conditional probability of outcome \\(A\\) given condition \\(B\\) is computed as the following:\n\\[\nP (A|B) = \\frac{P (A\\text{ and }B)}{P (B)}\n\\]"
  },
  {
    "objectID": "week04.html#general-multiplication-rule",
    "href": "week04.html#general-multiplication-rule",
    "title": "4  Introduction to Probability",
    "section": "4.15 General multiplication rule",
    "text": "4.15 General multiplication rule\nWe already saw a specific multiplication rule for independent events. But there is a more general rule, applicable whether independence is true or not. If \\(A\\) and \\(B\\) represent two outcomes or events, then\n\\[\nP (A\\text{ and }B) = P (A|B) × P (B)\n\\]\nIt is useful to think of \\(A\\) as the outcome of interest and \\(B\\) as the condition."
  },
  {
    "objectID": "week04.html#tree-diagrams-of-probability",
    "href": "week04.html#tree-diagrams-of-probability",
    "title": "4  Introduction to Probability",
    "section": "4.16 Tree diagrams of probability",
    "text": "4.16 Tree diagrams of probability\nThe textbook claims that tree diagrams aid our thinking about conditional probabilities. Luckily, law professor Harry Surden provides an example of a tree diagram of probabilities on his website.\n\n#.. R Conditional Probability Tree Diagram\n\n#.. The Rgraphviz graphing package must be installed to do this\nlibrary(Rgraphviz)\n\n#.. Change the three variables below to match your actual values\n#.. These are the values that you can change for your own probability tree\n#.. From these three values, other probabilities (e.g. prob(b)) will be calculated\n\n#.. Probability of a\na&lt;-.01\n\n#.. Probability (b | a)\nbGivena&lt;-.99\n\n#. Probability (b | ¬a)\nbGivenNota&lt;-.10\n\n###################### Everything below here will be calculated\n\n#. Calculate the rest of the values based upon the 3 variables above\nnotbGivena&lt;-1-bGivena\nnotA&lt;-1-a\nnotbGivenNota&lt;-1-bGivenNota\n\n#. Joint Probabilities of a and B, a and notb, nota and b, nota and notb\naANDb&lt;-a*bGivena\naANDnotb&lt;-a*notbGivena\nnotaANDb &lt;- notA*bGivenNota\nnotaANDnotb &lt;- notA*notbGivenNota\n\n#. Probability of B\nb&lt;- aANDb + notaANDb\nnotB &lt;- 1-b\n\n#. Bayes theorum - probabiliyt of A | B\n#. (a | b) = Prob (a AND b) / prob (b)\naGivenb &lt;- aANDb / b\n\n#. These are the labels of the nodes on the graph\n#. To signify \"Not A\" - we use A' or A prime\n\nnode1&lt;-\"P\"\nnode2&lt;-\"A\"\nnode3&lt;-\"A'\"\nnode4&lt;-\"A&B\"\nnode5&lt;-\"A&B'\"\nnode6&lt;-\"A'&B\"\nnode7&lt;-\"A'&B'\"\nnodeNames&lt;-c(node1,node2,node3,node4, node5,node6, node7)\n\nrEG &lt;- new(\"graphNEL\", nodes=nodeNames, edgemode=\"directed\")\n\n#. Draw the \"lines\" or \"branches\" of the probability Tree\nrEG &lt;- addEdge(nodeNames[1], nodeNames[2], rEG, 1)\nrEG &lt;- addEdge(nodeNames[1], nodeNames[3], rEG, 1)\nrEG &lt;- addEdge(nodeNames[2], nodeNames[4], rEG, 1)\nrEG &lt;- addEdge(nodeNames[2], nodeNames[5], rEG, 1)\nrEG &lt;- addEdge(nodeNames[3], nodeNames[6], rEG, 1)\nrEG &lt;- addEdge(nodeNames[3], nodeNames[7], rEG, 10)\n\neAttrs &lt;- list()\n\nq&lt;-edgeNames(rEG)\n\n#. Add the probability values to the the branch lines\n\neAttrs$label &lt;- c(toString(a),toString(notA),\n toString(bGivena), toString(notbGivena),\n toString(bGivenNota), toString(notbGivenNota))\nnames(eAttrs$label) &lt;- c(q[1],q[2], q[3], q[4], q[5], q[6])\nedgeAttrs&lt;-eAttrs\n\n#. Set the color, etc, of the tree\nattributes&lt;-list(node=list(label=\"foo\", fillcolor=\"lightgreen\", fontsize=\"15\"),\n edge=list(color=\"red\"),graph=list(rankdir=\"LR\"))\n\n#. Plot the probability tree using Rgraphvis\nplot(rEG, edgeAttrs=eAttrs, attrs=attributes)\nnodes(rEG)\n\n[1] \"P\"     \"A\"     \"A'\"    \"A&B\"   \"A&B'\"  \"A'&B\"  \"A'&B'\"\n\nedges(rEG)\n\n$P\n[1] \"A\"  \"A'\"\n\n$A\n[1] \"A&B\"  \"A&B'\"\n\n$`A'`\n[1] \"A'&B\"  \"A'&B'\"\n\n$`A&B`\ncharacter(0)\n\n$`A&B'`\ncharacter(0)\n\n$`A'&B`\ncharacter(0)\n\n$`A'&B'`\ncharacter(0)\n\n#. Add the probability values to the leaves of A&B, A&B', A'&B, A'&B'\ntext(500,420,aANDb, cex=.8)\n\ntext(500,280,aANDnotb,cex=.8)\n\ntext(500,160,notaANDb,cex=.8)\n\ntext(500,30,notaANDnotb,cex=.8)\n\ntext(340,440,\"(B | A)\",cex=.8)\n\ntext(340,230,\"(B | A')\",cex=.8)\n\n#. Write a table in the lower left of the probablites of A and B\ntext(80,50,paste(\"P(A):\",a),cex=.9, col=\"darkgreen\")\ntext(80,20,paste(\"P(A'):\",notA),cex=.9, col=\"darkgreen\")\n\ntext(160,50,paste(\"P(B):\",round(b,digits=2)),cex=.9)\ntext(160,20,paste(\"P(B'):\",round(notB, 2)),cex=.9)\n\ntext(80,420,paste(\"P(A|B): \",round(aGivenb,digits=2)),cex=.9,col=\"blue\")\n\n\n\n\nIt’s pretty ugly because it was designed for a different width and height than I have allocated, but it gives the general idea."
  },
  {
    "objectID": "week04.html#bayes-theorem",
    "href": "week04.html#bayes-theorem",
    "title": "4  Introduction to Probability",
    "section": "4.17 Bayes’ Theorem",
    "text": "4.17 Bayes’ Theorem\nIt’s the centerpiece of Bayesian statistics so it’s a bit like a fish out of water in a frequentist course. It states, in words, that the posterior probability of an outcome \\(A\\) given an outcome \\(B\\) is the likelihood of the outcome \\(B\\) times the prior probability of the outcome \\(A\\) divided by the evidence of outcome \\(B\\). More succinctly,\n\\[\nP(A|B)=\\frac{P(B|A)P(A)}{P(B)}\n\\]\nThe intuition is that we usually know something and shouldn’t go into problem solving with no assumptions. An example of drug user testing given by Wikipedia is shown below graphically, where the test is ninety percent sensitive to a recipient being a drug user. The test is also eighty percent specific, meaning that it can detect that a non-user is a non-user eighty percent of the time.\n\nWhat does this tell us about drug testing? Even if someone tests positive, the probability that they are a drug user is only 19%! This assumes prior knowledge that five percent of the general population are users of the drug.\nIn Bayesian terms:\n\\[\n\\frac{0.9 \\times 0.05}{0.9\\times 0.05 + 0.2\\times0.95}\n\\] By the way, this formulation uses the law of total probability in the denominator, expanding \\(P(B)\\) into\n\\[\nP(B|A)P(A)+P(B|\\neg A)P(\\neg A)\n\\]\nwhere \\(\\neg A\\) is the complement of \\(A\\).\nThis is very important to know about because, for example, someone might only tell you that a test is ninety percent sensitive and eighty percent specific and leave out the Bayes rule result that says that, given that only five percent of the general population are drug users, there’s a nineteen percent chance that testing positive indicates that you are a drug user. Again, Bayes’ rule is very important when you have some prior knowledge. In this case, the prior knowledge is a study that says that five percent of the general population are users of this drug."
  },
  {
    "objectID": "week04.html#sampling-from-a-small-population",
    "href": "week04.html#sampling-from-a-small-population",
    "title": "4  Introduction to Probability",
    "section": "4.18 Sampling from a small population",
    "text": "4.18 Sampling from a small population\nRecall that the population includes every object and is usually not practical to measure. For examples, every fish, every person, every thunderstorm are too many to represent. So we take a sample. A typical rule of thumb is that, if we can sample more than ten percent of the population, we regard it as a small population.\nThe textbook gives an example of being called on by the professor. The chance that you are called on in a class of 15 is 1/15. If the professor calls on three different people in succession, the chance that you are called on increases to 1/5:\n\\[\\begin{align}\nP(\\neg\\text{3 in a row}) &=\\\\\n&=P(\\text{not picked first, second, third})\\\\\n&=\\frac{14}{15}\\times\\frac{13}{14}\\times\\frac{12}{13}\\\\\n&=\\frac{12}{15}\\\\\n\\end{align}\\]\nand the complement of 12/15 is 1/5."
  },
  {
    "objectID": "week04.html#random-variables",
    "href": "week04.html#random-variables",
    "title": "4  Introduction to Probability",
    "section": "4.19 Random variables",
    "text": "4.19 Random variables\nA process with a random numerical outcome is called a random variable. It’s kind of like a stochastic function in that there is an input (the process) and output (the outcome number).\nA random variable is usually represented as a capital, italicized Latin letter, e.g., \\(X, Y, Z\\). Specific outcomes are usually represented as a lowercase, italicized Latin letter with a subscript to denote which outcome, such as \\(x_1, x_2, x_3\\). The probability that a random variable \\(X\\) has a specific outcome \\(x_1\\) is represented as \\(P(X=x_1)\\).\nThe expectation of \\(X\\) is the expected value of \\(X\\), represented as \\(E(X)\\). The expected value is typically the average, but not always. If there are \\(k\\) possible outcomes, then\n\\[\nE(X)=\\sum_{i=1}^kx_iP(X=x_i)\n\\]\nThe Greek letter \\(\\sum\\) (Sigma) denotes a sum of a series of numbers, indexed in this case by \\(i\\). You can read it in English as the sum, going from \\(1\\) to \\(k\\), of the expressions to the right of the Sigma sign. In other words,\n\\[\nx_1P(X=x_1)+x_2P(X=x_2)+\\cdots+x_kP(X=x_k)\n\\]\nIt’s the average as we usually understand it if each outcome is equally probable, in which case it’s the sum of the outcomes divided by the number of outcomes.\nWriters often substitute \\(E(X)=\\mu\\) which can be confusing because Greek letters are usually used as parameters, while Latin letters are usually used as specific realizations of those parameters.\nThe above assumes there are \\(k\\) specific outcomes. We call this case a discrete random variable. It’s also possible to do math with a continuous random variable. In other words, \\(k=\\infty\\). However, that requires calculus so the textbook skips it for now.\n\n4.19.1 Variability in random variables\nRecall that random variables are a kind of mapping between a process and specific numerical outcomes. Those specific outcomes differ. For example, the revenue of a store varies day by day, and we can say something about that variability.\nWe usually express it by two related concepts: variance (denoted by a lowercase sigma squared or \\(\\sigma^2\\)), and its square root, standard deviation (denoted by a lowercase sigma or \\(\\sigma\\)). You might wonder why we don’t just choose one of these symbols. Most statisticians just use standard deviation, but to prove that it is an unbiased estimator of variance, we need to square it for mathematical reasons that are beyond the scope of this course.\nStandard deviation is expressed in the same units as the subject under consideration, whereas variance is expressed in squared units. So when I said at the beginning of this file that the standard deviation of your completed exercises was about 2.69, I meant that in terms of number of exercises, meaning that most of you were within 2.69 completed exercises either way of each other. In other words, if you had three or nine completed exercises, you were a kind of outlier.\n\n\n4.19.2 Linear combinations of random variables\nWe can put random variables together. For example, your GPA is calculated from a set of grades that may differ. If you play fantasy sports, your score comes from many different players. A recommender system for music listening may make calculations based on many different songs you listened to.\nA linear combination of two random variables can be expressed as \\(aX+bY\\), where \\(a\\) and \\(b\\) are fixed constants, for example, the number of credits applied to each class in your GPA portfolio. If you take four four credit classes, your GPA might be expressed as the linear combination\n\\[\n4E(X_1)+4E(X_2)+4E(X_3)+4E(X_4)\n\\]\nwhere \\(E(X_i)\\) is a function of your grade, such as your grade mapped to a number and divided by the number of credits you’re taking. (Of course, you are used to thinking of it as the average of the grades since most classes carry the same number of credits and your grades are mapped to numbers. But this formulation holds for all kinds of sets.)"
  },
  {
    "objectID": "week04.html#continuous-distributions",
    "href": "week04.html#continuous-distributions",
    "title": "4  Introduction to Probability",
    "section": "4.20 Continuous distributions",
    "text": "4.20 Continuous distributions\nSo far, we’ve considered discrete distributions, where \\(k\\) takes on one of a finite set of values. What about the continuous case? For example, temperature can theoretically take on an infinite number of values, even though we only have the tools for discrete measurements.\nThe most famous continuous distribution is the normal distribution. This picture illustrates the normal distribution. The mound-shaped curve represents the probability density function and the area between the curve and the horizontal line represents the value of the cumulative distribution function.\n\n\n\n\n\nConsider a normally distributed nationwide test.\nThe total shaded area between the curve and the straight horizontal line can be thought of as one hundred percent of that area. In the world of probability, we measure that area as 1. The curve is symmetrical, so measure all the area to the left of the highest point on the curve as 0.5. That is half, or fifty percent, of the total area between the curve and the horizontal line at the bottom. Instead of saying area between the curve and the horizontal line at the bottom, people usually say the area under the curve.\nFor any value along the \\(x\\)-axis, the \\(y\\)-value on the curve represents the value of the probability density function.\nThe area bounded by the vertical line between the \\(x\\)-axis and the corresponding \\(y\\)-value on the curve, though, is what we are usually interested in because that area represents probability.\nHere is a graph of the size of that area. It’s called the cumulative distribution function (cdf).\n\n\n\n\n\nThe above graph can be read as having an input and output that correspond to the previous graph of the probability density function. As we move from right to left on the \\(x\\)-axis, the area that would be to the left of a given point on the probability density function is the \\(y\\)-value on this graph. For example, if we go half way across the \\(x\\)-axis of the probability density function, the area to its left is one half of the total area, so the \\(y\\)-value on the cumulative distribution function graph is one half.\nThe shape of the cumulative distribution function is called a sigmoid curve. You can see how it gets this shape by looking again at the probability density function graph above. As you move from left to right on that graph, the area under the curve increases very slowly, then more rapidly, then slowly again. The places where the area grows more rapidly and then more slowly on the probability density function curve correspond to the s-shaped bends on the cumulative distribution curve.\nAt the left side of the cumulative distribution curve, the \\(y\\)-value is zero meaning zero probability. When we reach the right side of the cumulative distribution curve, the \\(y\\)-value is 1 or 100 percent of the probability.\nLet’s get back to the example of a nationwide test. If we say that students nationwide took an test that had a mean score of 75 and that the score was normally distributed, we’re saying that the value on the \\(x\\)-axis in the center of the curve is 75. Moreover, we’re saying that the area to the left of 75 is one half of the total area. We’re saying that the probability of a score less than 75 is 0.5 or fifty percent. We’re saying that half the students got a score below 75 and half got a score above 75.\nThat is called the frequentist interpretation of probability. In general, that interpretation says that a probability of 0.5 is properly measured by saying that, if we could repeat the event enough times, we would find the event happening half of those times.\nFurthermore, the frequentist interpretation of the normal distribution is that, if we could collect enough data, such as administering the above test to thousands of students, we would see that the graph of the frequency of their scores would look more and more like the bell curve in the picture, where \\(x\\) is a test score and \\(y\\) is the number of students receiving that score.\nSuppose we have the same test and the same distribution but that the mean score is 60. Then 60 is in the middle and half the students are on each side. That is easy to measure. But what if, in either case, we would like to know the probability associated with scores that are not at that convenient midpoint?\nIt’s hard to measure any other area under the normal curve except for \\(x\\)-values in the middle of the curve, corresponding to one half of the area. Why is this?\nTo see why it’s hard to measure the area corresponding to any value except the middle value, let’s first consider a different probability distribution, the uniform distribution. Suppose I have a machine that can generate any number between 0 and 1 at random. Further, suppose that any such number is just as likely as any other such number.\nHere’s a graph of the the uniform distribution of numbers generated by the machine. The horizontal line is the probability density function and the shaded area is the cumulative distribution function from 0 to 1/2. In other words, the probability of the machine generating numbers from 0 to 1/2 is 1/2. The probability of generating numbers from 0 to 1 is 1, the area of the entire rectangle.\n\n\n\n\n\nIt’s very easy to calculate any probability for this distribution, in contrast to the normal distribution. The reason it is easy is that you can just use the formula for the area of a rectangle, where area is base times side. The probability of being in the entire rectangle is \\(1\\times1=1\\), and the probability of being in the part from \\(x=0\\) to \\(x=1/4\\) is just \\(1\\times(1/4)=1/4\\). The cumulative distribution function of the uniform distribution is simpler than that of the normal distribution because area is being added at the same rate as we move from left to right on the above graph. Therefore it is just a straight diagonal line from (0,1) on the left to (1,1) on the right.\n\n\n\n\n\nReading it is the same as reading the cumulative distribution function for the normal distribution. For any value on the \\(x\\)-axis, say, 1/2, go up to the diagonal line and over to the value on the \\(y\\)-axis. In this case, that value is 1/2. That is the area under the horizontal line in the probability density function graph from 0 to 1/2 (the shaded area). For a rectangle, calculating area is trivial.\nCalculating the area of a curved region like the normal distribution can be more difficult. If you’ve studied any calculus, you know that there are techniques for calculating the area under a curve. These techniques are called integration techniques. In the case of the normal distribution the formula for the height of the curve at any point on the \\(x\\)-axis is\n\\[\\begin{equation*}\n\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}\n\\end{equation*}\\]\nand the area is the integral of that quantity from \\(-\\infty\\) to \\(x\\), which can be rewritten as\n\\[\\begin{equation*}\n\\frac{1}{\\sqrt{2\\pi}}\\int^x_{-\\infty}e^{-t^2/2}dt\n=(1/2)\\left(1+\\text{erf}\\left(\\frac{x-\\mu}{\\sigma\\sqrt{2}}\\right)\\right)\n\\end{equation*}\\]\nThe integral on the left is difficult to evaluate so people use numerical approximation techniques to find the expression on the right in the above equation. Those techniques are so time-consuming that, rather than recompute them every time they are needed, a very few people used to write the results into a table and publish it and most people working with probability would just consult the tables. Only in the past few decades have calculators become available that can do the tedious approximations. Hence, most statistics books, written by people who were educated decades ago, still teach you how to use such tables. There is some debate as to whether there is educational value in using the tables vs using calculators or smartphone apps or web-based tables or apps."
  },
  {
    "objectID": "week04.html#the-f-distribution",
    "href": "week04.html#the-f-distribution",
    "title": "4  Introduction to Probability",
    "section": "4.21 The F distribution",
    "text": "4.21 The F distribution\nAnother distribution that we’ll enounter later is called the F distribution. We’ll calculate an F-statistic when we build linear regression models, but I just want you to know the general shape of it for now. The region marked \\(\\alpha\\) corresponds inversely to the magnitude of the F statistic. In other words, a larger F statistic means a smaller \\(\\alpha\\).\n\n\n\n\n\n\n\n\n\nAsh, Carol. 1993. The Probability Tutoring Book. New York, NY: IEEE Press."
  },
  {
    "objectID": "week05.html#recap-week-04",
    "href": "week05.html#recap-week-04",
    "title": "5  Distributions of Random Variables",
    "section": "5.1 Recap Week 04",
    "text": "5.1 Recap Week 04\nWe did week 04 in one day, covering numerous probability basics and doing a few exercises in probability, mostly paper and pencil exercises. We saw three examples of continuous probability distributions: normal, uniform, and F. This week, we’ll examine continuous distributions in some more detail."
  },
  {
    "objectID": "week05.html#five-distributions",
    "href": "week05.html#five-distributions",
    "title": "5  Distributions of Random Variables",
    "section": "5.2 Five distributions",
    "text": "5.2 Five distributions\nWe previously looked at the normal distribution. In this Chapter of the textbook, we visit five distributions:\n\nNormal distribution\nGeometric distribution\nBinomial distribution\nNegative binomial distribution\nPoisson distribution\n\nTo save time, we will follow the textbook instead of lecture notes.\nThe following picture illustrates the normal distribution. The mound-shaped curve represents the probability density function and the area between the curve and the horizontal line represents the value of the cumulative distribution function.\n\n\n\n\n\nConsider a normally distributed nationwide test.\nThe total shaded area between the curve and the straight horizontal line can be thought of as one hundred percent of that area. In the world of probability, we measure that area as 1. The curve is symmetrical, so measure all the area to the left of the highest point on the curve as 0.5. That is half, or fifty percent, of the total area between the curve and the horizontal line at the bottom. Instead of saying area between the curve and the horizontal line at the bottom, people usually say the area under the curve.\nFor any value along the \\(x\\)-axis, the \\(y\\)-value on the curve represents the value of the probability density function.\nThe area bounded by the vertical line between the \\(x\\)-axis and the corresponding \\(y\\)-value on the curve, though, is what we are usually interested in because that area represents probability."
  },
  {
    "objectID": "week05.html#notation",
    "href": "week05.html#notation",
    "title": "5  Distributions of Random Variables",
    "section": "5.3 Notation",
    "text": "5.3 Notation\n\\(N(\\mu,\\sigma)\\) is said to characterize a normal distribution. If we know the parameters \\(\\mu\\), the mean, and \\(\\sigma\\), the standard deviation, we have a complete picture of the normal distribution, namely the height of the hump and its width."
  },
  {
    "objectID": "week05.html#r-function-to-find-probability",
    "href": "week05.html#r-function-to-find-probability",
    "title": "5  Distributions of Random Variables",
    "section": "5.4 R function to find probability",
    "text": "5.4 R function to find probability\nThe pnorm() function takes a value of a normally distributed random variable and returns the probability of that value or less. The textbook gives an example of the SAT and ACT scores. For the SAT, the textbook claims that \\(\\mu = 1100\\) and \\(\\sigma=200\\) so we can say the SAT score \\(x\\sim N(1100,200)\\) which is read as “x has the normal distribution with mean 1100 and standard deviation 200.” Suppose you score 1100. What is the probability that someone will score lower than you?\n\npnorm(1100,mean=1100,sd=200)\n\n[1] 0.5\n\n\nThis returns a value of 0.5 or fifty percent. That’s something we can tell without a computer, but for the normal distribution, other values are difficult to compute by hand or guess correctly. For instance,\n\npnorm(1400,mean=1100,sd=200)\n\n[1] 0.9331928\n\n\nreturns a value of 0.9331928, or that 93 percent of students get below this score.\nBecause all probabilities sum to 1, we can use this information to tell the probability of someone getting a higher score, or the probability of getting a score between two scores by subtracting the smaller from the larger.\nThe textbook gives another example as finding the probability that a particular student scores at least 1190:\n\n1 - pnorm(1190,mean=1100,sd=200)\n\n[1] 0.3263552\n\n\nwhich returns 32.64 percent. Note that we did not explicitly calculate the \\(Z\\)-score that leads to this probability. The \\(Z\\)-score is a way of standardizing so that instead of (in this case) \\(x\\sim N(1100,200)\\), we calculate the probability of \\(Z\\sim N(0,1)\\). Here, we standardize by recognizing that \\(Z=(x-\\mu)/\\sigma\\) or \\((1190-1100)/200=0.45\\) and then we can say\n\n1 - pnorm(0.45,mean=0,sd=1)\n\n[1] 0.3263552\n\n\nand we get the same answer as above, 32.64 percent."
  },
  {
    "objectID": "week05.html#geometric-distribution",
    "href": "week05.html#geometric-distribution",
    "title": "5  Distributions of Random Variables",
    "section": "5.5 Geometric distribution",
    "text": "5.5 Geometric distribution\nFirst we need Bernoulli random variables to understand the Geometric distribution.\nIf \\(X\\) is a random variable that takes value 1 with probability of success \\(p\\) and \\(0\\) with probability \\(1 − p\\), then \\(X\\) is a Bernoulli random variable with mean and standard deviation \\(\\mu=p\\) and \\(\\sigma=\\sqrt{p(1-p)}\\). A Bernoulli random variable is a process with only two outcomes: success or failure. Flipping a coin and calling it is an example of Bernoulli random variable.\nNow the question is “What happens if you flip a coin or roll the dice or some other win/lose process many times in a row?”\nThe geometric distribution describes how many times it takes to obtain success in a series of Bernoulli trials.\nThe textbook (p. 145) gives an example where an insurance company employee is looking for the first person to meet a criteria where the probability of meeting that criteria is 0.7 or seventy percent. We can calculate the probability of 0 failures (the first person, one failure (the second person), and so on, using R:\n\na &lt;- dgeom(x=0,prob=0.7) # the first person\nb &lt;- dgeom(x=1,prob=0.7) # the second person\nc &lt;- dgeom(x=2,prob=0.7) # the third person\nd &lt;- dgeom(x=3,prob=0.7) # the fourth person\ne &lt;- dgeom(x=4,prob=0.7) # the fifth person\n\nand if we graph these numbers, we’ll find they have the property of exponential decay.\n\nx&lt;-c(a,b,c,d,e)\nplot(x)\n\n\n\n\nThe textbook gives the following definition of the geometric distribution.\nIf the probability of a success in one trial is \\(p\\) and the probability of a failure is \\(1 − p\\), then the probability of finding the first success in the \\(n\\)th trial is given by \\((1 − p)^{n−1}p\\). The mean (i.e. expected value) and standard deviation of this wait time are given by \\(μ = 1/p\\), \\(σ = \\sqrt{(1 − p)/p^2}\\)."
  },
  {
    "objectID": "week05.html#binomial-distribution",
    "href": "week05.html#binomial-distribution",
    "title": "5  Distributions of Random Variables",
    "section": "5.6 Binomial distribution",
    "text": "5.6 Binomial distribution\nThe textbook says that the binomial distribution is used to describe the number of successes in a fixed number of trials. This is different from the geometric distribution, which described the number of trials we must wait before we observe a success.\nSuppose the probability of a single trial being a success is \\(p\\). Then the probability of observing exactly \\(k\\) successes in \\(n\\) independent trials is given by\n\\[\n\\binom{n}{k}p^k(1-p)^{n-k}\n\\]\nand \\(\\mu=np\\), \\(\\sigma=\\sqrt{np(1-p)}\\)\n\n5.6.1 Example using R\nThe website r-tutor.com gives the following problem as an example:\n\n5.6.1.1 Problem\nSuppose there are twelve multiple choice questions in an English class quiz. Each question has five possible answers, and only one of them is correct. Find the probability of having four or fewer correct answers if a student attempts to answer every question at random.\n\n\n5.6.1.2 Solution\nSince only one out of five possible answers is correct, the probability of answering a question correctly by random is 1/5=0.2. We can find the probability of having exactly 4 correct answers by random attempts as follows.\n\ndbinom(4, size=12, prob=0.2)\n\n[1] 0.1328756\n\n\nwhich returns 0.1329.\nTo find the probability of having four or less correct answers by random attempts, we apply the function dbinom with \\(x = 0,\\ldots,4\\).\n\ndbinom(0, size=12, prob=0.2) +\ndbinom(1, size=12, prob=0.2) +\ndbinom(2, size=12, prob=0.2) +\ndbinom(3, size=12, prob=0.2) +\ndbinom(4, size=12, prob=0.2)\n\n[1] 0.9274445\n\n\nwhich returns 0.9274.\nAlternatively, we can use the cumulative probability function for binomial distribution pbinom.\n\npbinom(4, size=12, prob=0.2)\n\n[1] 0.9274445\n\n\nwhich returns the same answer.\n\n\n5.6.1.3 Answer\nThe probability of four or fewer questions answered correctly by random in a twelve question multiple choice quiz is 92.7%."
  },
  {
    "objectID": "week05.html#negative-binomial-distribution",
    "href": "week05.html#negative-binomial-distribution",
    "title": "5  Distributions of Random Variables",
    "section": "5.7 Negative binomial distribution",
    "text": "5.7 Negative binomial distribution\nThis is a generalization of the geometric distribution, defined in the textbook as follows.\nThe negative binomial distribution describes the probability of observing the \\(k\\)th success on the \\(n\\)th trial, where all trials are independent:\n\\[\nP (\\text{the kth success on the nth trial}) =\n\\binom{n − 1}{k − 1} p^k(1 − p)^{n−k}\n\\]\nThe value \\(p\\) represents the probability that an individual trial is a success."
  },
  {
    "objectID": "week05.html#poisson-distribution",
    "href": "week05.html#poisson-distribution",
    "title": "5  Distributions of Random Variables",
    "section": "5.8 Poisson distribution",
    "text": "5.8 Poisson distribution\nThe textbook defines the Poisson distribution as follows.\nSuppose we are watching for events and the number of observed events follows a Poisson distribution with rate \\(λ\\). Then\n\\[\nP (\\text{observe k events}) = \\frac{λ^ke^{−λ}}{k!}\n\\]\nwhere \\(k\\) may take a value 0, 1, 2, and so on, and \\(k!\\) represents \\(k\\)-factorial, as described on page 150. The letter e ≈ 2.718 is the base of the natural logarithm. The mean and standard deviation of this distribution are λ and √λ, respectively.\nThe r-tutor website mentioned above offers the following example of a Poisson distribution problem solved using R.\n\n5.8.0.1 Problem\nIf there are twelve cars crossing a bridge per minute on average, find the probability of having seventeen or more cars crossing the bridge in a particular minute.\n\n\n5.8.0.2 Solution\nThe probability of having sixteen or fewer cars crossing the bridge in a particular minute is given by the function ppois.\n\nppois(16, lambda=12)   # lower tail\n\n[1] 0.898709\n\n\nwhich returns 0.89871\nHence the probability of having seventeen or more cars crossing the bridge in a minute is in the upper tail of the probability density function.\n\nppois(16, lambda=12, lower=FALSE)   # upper tail\n\n[1] 0.101291\n\n\nwhich returns 0.10129\n\n\n5.8.0.3 Answer\nIf there are twelve cars crossing a bridge per minute on average, the probability of having seventeen or more cars crossing the bridge in a particular minute is 10.1%."
  },
  {
    "objectID": "week06.html#recap-week-05",
    "href": "week06.html#recap-week-05",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.1 Recap Week 05",
    "text": "6.1 Recap Week 05\n\nNormal distribution\nGeometric distribution\nBinomial distribution\nNegative binomial distribution\nPoisson distribution"
  },
  {
    "objectID": "week06.html#ggplot2",
    "href": "week06.html#ggplot2",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.2 ggplot2",
    "text": "6.2 ggplot2\nWe’ll follow the ggplot2 cheatsheet, available at https://posit.co/wp-content/uploads/2022/10/data-visualization-1.pdf.\nBy the way, there are many more cheatsheets for different aspects of R and RStudio available at https://posit.co/resources/cheatsheets/."
  },
  {
    "objectID": "week06.html#base-layer",
    "href": "week06.html#base-layer",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.3 Base layer",
    "text": "6.3 Base layer\n\nggplot(data = mpg, aes(x = cty, y = hwy))\n\n\n\n\nThe above code doesn’t produce a plot by itself. We would have to add a layer, such as a geom or stat. (Every geom has a default stat and every stat has a default geom.) For example,\n\nggplot(data = mpg, aes(x = cty, y = hwy)) +\n  geom_point()\n\n\n\n\nadds a scatterplot, whereas\n\nggplot(data = mpg, aes(x = cty, y = hwy)) +\n  geom_point(aes(color=displ,size=displ))\n\n\n\n\nadds the aesthetics from the second example on the cheatsheet.\n\n6.3.1 Graphical primitives\n\na &lt;- ggplot(economics,aes(date,unemploy))\nb &lt;- ggplot(seals,aes(x=long,y=lat))\na + geom_blank()\n\n\n\na + expand_limits()\n\nWarning in max(lengths(data)): no non-missing arguments to max; returning -Inf\n\n\n\n\nb + geom_curve(aes(yend = lat + 1, xend = long + 1), curvature = 1)\n\n\n\na + geom_path(lineend = \"butt\", linejoin = \"round\", linemitre = 1)\n\n\n\na + geom_polygon(aes(alpha = 50))\n\n\n\nb + geom_rect(aes(xmin = long, ymin = lat, xmax = long + 1, ymax = lat + 1))\n\n\n\na + geom_ribbon(aes(ymin = unemploy - 900, ymax = unemploy + 900))\n\n\n\n\n\n6.3.1.1 Line segments\n\nb + geom_abline(aes(intercept = 0, slope = 1))\n\n\n\nb + geom_hline(aes(yintercept = lat))\n\n\n\nb + geom_vline(aes(xintercept = long))\n\n\n\n\n\n\n\n6.3.2 One variable—continuous\n\nc &lt;- ggplot(mpg, aes(hwy)); c2 &lt;- ggplot(mpg)\nc + geom_area(stat = \"bin\")\n\n\n\nc + geom_density(kernel = \"gaussian\")\n\n\n\nc + geom_dotplot()\n\n\n\nc + geom_freqpoly()\n\n\n\nc + geom_histogram(binwidth = 5)\n\n\n\nc2 + geom_qq(aes(sample = hwy))\n\n\n\n\n\n\n6.3.3 One variable—discrete\n\nd &lt;- ggplot(mpg, aes(fl))\nd + geom_bar()\n\n\n\n\n\n\n6.3.4 Two variables—both continuous\n\ne &lt;- ggplot(mpg,aes(cty,hwy))\ne + geom_label(aes(label = cty), nudge_x = 1, nudge_y = 1)\n\n\n\ne + geom_point()\n\n\n\ne + geom_quantile()\n\n\n\ne + geom_rug(sides = \"bl\")\n\n\n\ne + geom_smooth(method = lm)\n\n\n\ne + geom_text(aes(label = cty), nudge_x = 1, nudge_y = 1)\n\n\n\n\n\n\n6.3.5 Two variables—one discrete, one continuous\n\nf &lt;- ggplot(mpg,aes(class,hwy))\nf + geom_col()\n\n\n\nf + geom_boxplot()\n\n\n\nf + geom_dotplot(binaxis = \"y\", stackdir = \"center\")\n\n\n\nf + geom_violin(scale = \"area\")\n\n\n\n\n\n\n6.3.6 Two variables—both discrete\n\ng &lt;- ggplot(diamonds, aes(cut, color))\ng + geom_count()\n\n\n\ng + geom_jitter(height = 2, width = 2)\n\n\n\n\n\n\n6.3.7 Continuous bivariate distribution\n\nh &lt;- ggplot(diamonds, aes(carat, price))\nh + geom_bin2d(binwidth = c(0.25, 500))\n\n\n\nh + geom_density_2d()\n\n\n\nh + geom_hex()\n\nWarning: Computation failed in `stat_binhex()`\nCaused by error in `compute_group()`:\n! The package \"hexbin\" is required for `stat_binhex()`\n\n\n\n\n\n\n\n6.3.8 Continuous function\n\ni &lt;- ggplot(economics, aes(date, unemploy))\ni + geom_area()\n\n\n\ni + geom_line()\n\n\n\ni + geom_step(direction = \"hv\")\n\n\n\n\n\n\n6.3.9 Visualizing error\n\ndf &lt;- data.frame(grp = c(\"A\", \"B\"), fit = 4:5, se = 1:2)\nj &lt;- ggplot(df, aes(grp, fit, ymin = fit - se, ymax = fit + se))\nj + geom_crossbar(fatten = 2)\n\n\n\nj + geom_errorbar()\n\n\n\nj + geom_linerange()\n\n\n\nj + geom_pointrange()\n\n\n\n\n\n\n6.3.10 Maps\n\ndata &lt;- data.frame(murder = USArrests$Murder,\nstate = tolower(rownames(USArrests)))\nmap &lt;- map_data(\"state\")\nk &lt;- ggplot(data, aes(fill = murder))\nk + geom_map(aes(map_id = state), map = map) + expand_limits(x = map$long, y = map$lat)\n\n\n\n\nThere is more, but you can go through that on your own and, if you wish, add it to this file."
  },
  {
    "objectID": "week06.html#additional-plots",
    "href": "week06.html#additional-plots",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.4 Additional plots",
    "text": "6.4 Additional plots\n\nlibrary(tidyverse)\ndf &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.csv\"))\ndf$cylinders &lt;- as_factor(df$cylinders)\ndf &lt;- df[df$price&lt;500000&df$price&gt;500,]\ndf&lt;-df[df$cylinders==\"10 cylinders\"|df$cylinders==\"12 cylinders\",]\ndf$price&lt;-as.numeric(df$price)\noptions(scipen=999)\ndf |&gt; ggplot(aes(price,cylinders))+geom_boxplot()\n\nWarning: Removed 156391 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\ndfa &lt;- df |&gt; count(condition,cylinders)\nlibrary(treemap)\npalette.HCL.options &lt;- list(hue_start=270, hue_end=360+150)\ntreemap(dfa,\n  index=c(\"condition\",\"cylinders\"),\n  vSize=\"n\",\n  type=\"index\",\n  palette=\"Reds\"\n)\n\n\n\n\nThe palette in this case is selected from RColorbrewer. You can find the full set of RColorbrewer palettes at the R Graph Gallery."
  },
  {
    "objectID": "week06.html#a-complete-scatterplot",
    "href": "week06.html#a-complete-scatterplot",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.5 A complete scatterplot",
    "text": "6.5 A complete scatterplot\nThe following scatterplot is completed in stages in section 2.2 of R4DS, which is available on Canvas as Wickham2023.pdf or online by googling r4ds 2e.\n\nlibrary(palmerpenguins)\nlibrary(ggthemes)\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "week06.html#foundations-for-inference",
    "href": "week06.html#foundations-for-inference",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.6 Foundations for Inference",
    "text": "6.6 Foundations for Inference\nThe following material comes from Mick McQuaid’s study guide for a previous course. In that course, we used Mendenhall and Sincich (2012) as a textbook, so there are innumerable references to that textbook in the following material. You don’t actually need that textbook and I will eventually delete references to it from this file. I just don’t have time right now."
  },
  {
    "objectID": "week06.html#normal-distribution",
    "href": "week06.html#normal-distribution",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.7 normal distribution",
    "text": "6.7 normal distribution\nThis picture illustrates the normal distribution. The mound-shaped curve represents the probability density function and the area between the curve and the horizontal line represents the value of the cumulative distribution function.\n\n\n\n\n\nConsider a normally distributed nationwide test.\nThe total shaded area between the curve and the straight horizontal line can be thought of as one hundred percent of that area. In the world of probability, we measure that area as 1. The curve is symmetrical, so measure all the area to the left of the highest point on the curve as 0.5. That is half, or fifty percent, of the total area between the curve and the horizontal line at the bottom. Instead of saying area between the curve and the horizontal line at the bottom, people usually say the area under the curve.\nFor any value along the \\(x\\)-axis, the \\(y\\)-value on the curve represents the value of the probability density function.\nThe area bounded by the vertical line between the \\(x\\)-axis and the corresponding \\(y\\)-value on the curve, though, is what we are usually interested in because that area represents probability.\nHere is a graph of the size of that area. It’s called the cumulative distribution function.\n\n\n\n\n\nThe above graph can be read as having an input and output that correspond to the previous graph of the probability density function. As we move from right to left on the \\(x\\)-axis, the area that would be to the left of a given point on the probability density function is the \\(y\\)-value on this graph. For example, if we go half way across the \\(x\\)-axis of the probability density function, the area to its left is one half of the total area, so the \\(y\\)-value on the cumulative distribution function graph is one half.\nThe shape of the cumulative distribution function is called a sigmoid curve. You can see how it gets this shape by looking again at the probability density function graph above. As you move from left to right on that graph, the area under the curve increases very slowly, then more rapidly, then slowly again. The places where the area grows more rapidly and then more slowly on the probability density function curve correspond to the s-shaped bends on the cumulative distribution curve.\nAt the left side of the cumulative distribution curve, the \\(y\\)-value is zero meaning zero probability. When we reach the right side of the cumulative distribution curve, the \\(y\\)-value is 1 or 100 percent of the probability.\nLet’s get back to the example of a nationwide test. If we say that students nationwide took an test that had a mean score of 75 and that the score was normally distributed, we’re saying that the value on the \\(x\\)-axis in the center of the curve is 75. Moreover, we’re saying that the area to the left of 75 is one half of the total area. We’re saying that the probability of a score less than 75 is 0.5 or fifty percent. We’re saying that half the students got a score below 75 and half got a score above 75.\nThat is called the frequentist interpretation of probability. In general, that interpretation says that a probability of 0.5 is properly measured by saying that, if we could repeat the event enough times, we would find the event happening half of those times.\nFurthermore, the frequentist interpretation of the normal distribution is that, if we could collect enough data, such as administering the above test to thousands of students, we would see that the graph of the frequency of their scores would look more and more like the bell curve in the picture, where \\(x\\) is a test score and \\(y\\) is the number of students receiving that score.\nSuppose we have the same test and the same distribution but that the mean score is 60. Then 60 is in the middle and half the students are on each side. That is easy to measure. But what if, in either case, we would like to know the probability associated with scores that are not at that convenient midpoint?\nIt’s hard to measure any other area under the normal curve except for \\(x\\)-values in the middle of the curve, corresponding to one half of the area. Why is this?\nTo see why it’s hard to measure the area corresponding to any value except the middle value, let’s first consider a different probability distribution, the uniform distribution. Suppose I have a machine that can generate any number between 0 and 1 at random. Further, suppose that any such number is just as likely as any other such number.\nHere’s a graph of the the uniform distribution of numbers generated by the machine. The horizontal line is the probability density function and the shaded area is the cumulative distribution function from 0 to 1/2. In other words, the probability of the machine generating numbers from 0 to 1/2 is 1/2. The probability of generating numbers from 0 to 1 is 1, the area of the entire rectangle.\n\n\n\n\n\nIt’s very easy to calculate any probability for this distribution, in contrast to the normal distribution. The reason it is easy is that you can just use the formula for the area of a rectangle, where area is base times side. The probability of being in the entire rectangle is \\(1\\times1=1\\), and the probability of being in the part from \\(x=0\\) to \\(x=1/4\\) is just \\(1\\times(1/4)=1/4\\).\nThe cumulative distribution function of the uniform distribution is simpler than that of the normal distribution because area is being added at the same rate as we move from left to right on the above graph. Therefore it is just a straight diagonal line from (0,1) on the left to (1,1) on the right.\n\n\n\n\n\nReading it is the same as reading the cumulative distribution function for the normal distribution. For any value on the \\(x\\)-axis, say, 1/2, go up to the diagonal line and over to the value on the \\(y\\)-axis. In this case, that value is 1/2. That is the area under the horizontal line in the probability density function graph from 0 to 1/2 (the shaded area). For a rectangle, calculating area is trivial.\nCalculating the area of a curved region, like the normal distribution, though, can be more difficult. If you’ve studied any calculus, you know that there are techniques for calculating the area under a curve. These techniques are called integration techniques. In the case of the normal distribution the formula for the height of the curve at any point on the \\(x\\)-axis is \\[\\begin{equation*}\n\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}\n\\end{equation*}\\] and the area is the integral of that quantity from \\(-\\infty\\) to \\(x\\), which can be rewritten as \\[\\begin{equation*}\n\\frac{1}{\\sqrt{2\\pi}}\\int^x_{-\\infty}e^{-t^2/2}dt\n=(1/2)\\left(1+\\text{erf}\\left(\\frac{x-\\mu}{\\sigma\\sqrt{2}}\\right)\\right)\n\\end{equation*}\\] The integral on the left is difficult to evaluate so people use numerical approximation techniques to find the expression on the right in the above equation. Those techniques are so time-consuming that, rather than recompute them every time they are needed, a very few people used to write the results into a table and publish it and most people working with probability would just consult the tables. Only in the past few decades have calculators become available that can do the tedious approximations. Hence, most statistics books, written by people who were educated decades ago, still teach you how to use such tables. There is some debate as to whether there is educational value in using the tables vs using calculators or smartphone apps or web-based tables or apps. We’ll demonstrate them and assume that you use a calculator or smartphone app on exams.\n\n6.7.1 Using the normal distribution\nSince there is no convenient integration formula, people used tables until recently. Currently you can google tables or apps that do the work of tables. We’re going to do two exercises with tables that help give you an idea of what’s going on. You can use your calculator afterward. The main reason for what follows is so you understand the results produced by your calculator to avoid ridiculous mistakes.\n\n\n6.7.2 calculating z scores\n\\[z=\\frac{y-\\mu}{\\sigma}\\]\nCalculations use the fact that the bell curve is symmetric and adds up to 1, so you can calculate one side and add it, subtract it, or double it\nFollowing are four examples.\n\n\\(P(-1 \\leqslant z \\leqslant 1)\\): Since the bell curve is symmetric, find the area from \\(z=0\\) to \\(z=1\\) and double that area. The table entry for \\(z=1\\) gives the relevant area under the curve, .3413. Doubling this gives the area from -1 to 1: .6826. Using a calculator may give you .6827 since a more accurate value than that provided by the table would be .6826895. This is an example where no points would be deducted from your score for using either answer.\n\\(P(-1.96 \\leqslant z \\leqslant 1.96)\\): This is one of the three most common areas of interest in this course, the other two being the one in part (c) below and the one I will add on after I show part (d) below. Here again, we can read the value from the table as .4750 and double it, giving .95. This is really common because because 95% is the most commonly used confidence interval.\n\\(P(-1.645 \\leqslant z \\leqslant 1.645)\\): The table does not have an entry for this extremely commonly desired value. A statistical calculator or software package will show that the result is .45, which can be doubled to give .90, another of the three most frequently used confidence intervals. If you use interpolation, you will get the correct answer in this case. Interpolation means to take the average of the two closest values, in this case \\((.4495+ .4505) / 2\\). You will rarely, if ever need to use interpolation in real life because software has made the tables obsolete and we only use them to try to drive home the concept of \\(z\\)-scores relating to area under the curve, rather than risking the possibility that you learn to punch numbers into an app without understanding them. Our hope is that, by first learning this method, you will be quick to recognize the results of mistakes, rather than naively reporting wacky results like the probability is 1.5 just because you typed a wrong number.\n\\(P(-3 \\leqslant z \\leqslant 3)\\): The table gives .4987 and doubling that gives .9974. A calculator would give the more correct (but equally acceptable in this course) result of .9973.\n\nThe other common confidence interval I mentioned above is the 99% confidence interval, used in cases where the calculation relates to something life-threatening, such as a question involving a potentially life-saving drug or surgery. A table or calculator will show that the \\(z\\)-score that would lead to this result is 2.576. So if you were asked to compute \\(P(-2.576 \\leqslant z \\leqslant 2.576)\\), the correct answer would be .99 or 99%. To use a calculator or statistical app to find the \\(z\\)-score given the desired probability, you would look in an app for something called a quantile function.\n\n\n6.7.3 sketch the graphs of probabilities\nSketch the normal curve six times, identifying a different region on it each time. For these graphs, let \\(y\\sim N(100,8)\\)."
  },
  {
    "objectID": "week06.html#central-limit-theorem",
    "href": "week06.html#central-limit-theorem",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.8 central limit theorem",
    "text": "6.8 central limit theorem\n\n6.8.1 Here’s the definition of the central limit theorem.\nFor large sample sizes, the sample mean \\(\\overline{y}\\) from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) has a sampling distribution that is approximately normal, regardless of the probability distribution of the sampled population.\n\n\n6.8.2 Why care about the central limit theorem?\nIn business, distributions of phenomena like waiting times and customer choices from a catalog are typically not normally distributed, but instead long-tailed. The central limit theorem means that resampling the mean of any of these distributions can be done on a large scale using the normal distribution assumptions without regard to the underlying distribution. This simplifies many real-life calculations. For instance, waiting times at each bus stop are exponentially distributed but if we take the mean waiting time at each of 100 bus stops, the mean of those 100 times is normally distributed, even though the individual waiting times are drawn from an exponential distribution.\n\n\n\n\n6.8.3 conditions for the central limit to hold\nLet \\(p\\) be the parameter of interest, in this case a proportion. (This not the same thing as a \\(p\\)-value, which we will explore later.) We don’t know \\(p\\) unless we examine every object in the population, which is usually impossible. For example, there are regulations in the USA requiring cardboard boxes used in interstate commerce to have a certain strength, which is measured by crushing the box. It would be economically unsound to crush all the boxes, so we crush a sample and obtain an estimate \\(\\hat{p}\\) of \\(p\\). The estimate is pronounced p-hat.\nThe central limit theorem can be framed in terms of its mean and standard error:\n\\[\n\\mu_{\\hat{p}}=p \\qquad \\text{SE}_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}}\n\\]\nThe conditions under which this holds true are\n\nThe sample observations are independent of each other\nThe sample is sufficiently large that the success-failure condition holds, namely \\(np\\geqslant10\\) and \\(n(1-p)\\geqslant10\\).\n\nThe sample observations are independent if they are drawn from a random sample. Sometimes you have to use your best judgment to determine whether the sample is truly random. For example, friends are known to influence each other’s opinions about pop music, so a sample of their opinions of a given pop star may not be random. On the other hand, their opinions of a previously unheard song from a previously unheard artist may have a better chance of being random.\nThese conditions matter when conducting activities like constructing a confidence interval or modeling a point estimate.\n\n\n6.8.4 normal distribution example problem\nConsider an example for calculating a \\(z\\)-score where \\(x\\sim N(50,15)\\), which is a statistical notation for saying \\(x\\) is a random normal variable with mean 50 and standard deviation 15. It is also read as if you said that \\(x\\) has the normal distribution with mean 50 and standard deviation 15.\n\n6.8.4.1 Picture the example.\n\n\n\n\n\n\n\n6.8.4.2 Here’s what we input into the \\(z\\) calculation.\nIn order to identify the size of the shaded area, we can use the table of \\(z\\)-scores by standardizing the parameters we believe to apply, as if they were the population parameters \\(\\mu\\) and \\(\\sigma\\). We only do this if we have such a large sample that we have reason to believe that the sample values approach the population parameters. For the much, much more typical case where we have a limited amount of data, we’ll learn a more advanced technique that you will use much more frequently in practice.\n\n\n6.8.4.3 We input a \\(z\\) to get an output probability.\nThe table in our textbook contains an input in the left and top margins and an output in the body. The input is a \\(z\\)-score, the result of the calculation \\[z=\\frac{y-\\mu}{\\sigma}\\] where \\(z\\geq 0\\). The output is a number in the body of the table, expressing the probability for the area between the normal curve and the axis, from the mean (0) to \\(z\\). Note that the values of \\(z\\) start at the mean and grow toward the right end of the graph. If \\(z\\) were \\(\\infty\\), the shaded area would be 0.5, also known as 50 percent.\n\n\n6.8.4.4 This is the \\(z\\)-score table concept.\n\n\n\n\n\n\n\n6.8.4.5 Calculate the \\(z\\)-score to input.\nFor now, let’s calculate the \\(z\\)-score as \\[z=\\frac{y-\\mu}{\\sigma}=\\frac{70-50}{15}=1.33\\] giving half of the answer we’re seeking:"
  },
  {
    "objectID": "week06.html#apply-the-z-score-to-the-table.",
    "href": "week06.html#apply-the-z-score-to-the-table.",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.9 Apply the \\(z\\)-score to the table.",
    "text": "6.9 Apply the \\(z\\)-score to the table.\n\n\n\n\n\n\n6.9.0.1 Obtain an intermediate result.\nNow use this to read the table. The input is 1.33 and you’ll use the left and top margins to find it. The output is the corresponding entry in the body of the table, .4082, also known as 40.82 percent of the area under the curve.\n\n\n6.9.0.2 Finish the example.\nRecall that our initial problem was to find \\(P(30&lt;y&lt;70)\\) and what we’ve just found, .4082, is \\(P(50&lt;y&lt;70)\\). We must multiply this result by 2 to obtain the correct answer, .8164 or 81.64 percent. That is to say that the probability that \\(y\\) is somewhere between 30 and 70 is .8164 or 81.64 percent. As a reality check, all probabilities for any single event must sum to 1, and the total area under the curve is 1, so it is a relief to note that the answer we’ve found is less than 1. It’s also comforting to note that the shaded area in the original picture of the example looks like it could plausibly represent about 80 percent of the total area. It is easy to get lost in the mechanics of calculations and come up with a wildly incorrect answer because of a simple arithmetic error.\n\n\n6.9.1 Some \\(z\\)-score tables differ from ours.\nBear in mind that anyone can publish a \\(z\\)-score table using their own customs. Students have found such tables that define \\(z\\) as starting at the extreme left of the curve. If we used such a table for the above example, the output would have been .9082 instead of .4082 and we would have had to subtract the left side, .5, from that result before multiplying by 2.\n\n\n6.9.2 typical exam questions for \\(z\\)-scores\nMany exam-style problems will ask questions such that you must do more or less arithmetic with the result from the table. Consider these questions, still using the above example where \\(y\\sim N(50,15)\\):\n\nWhat is the probability that \\(y\\) is greater than 50?\nWhat is the probability that \\(y\\) is greater than 70?\nWhat is the probability that \\(y\\) is less than 30?\nWhat is the probability that \\(y\\) is between 30 and 50?\n\n\n6.9.2.1 Answer the preceding questions.\nEach of these questions can be answered using \\(z=1.33\\) except the first. Since we know that \\(y\\) is normally distributed, we also know that the probability of \\(y\\) being greater than its mean is one half, so the answer to the first question is 0.5 or fifty percent. The second question simply requires us to subtract the result from the table, .4082, from .5 to find the area to the right of 1.33, which is .0918 or 9.18 percent. The third question is symmetrical with the second, so we can just use the method from the second question to find that it is also .0918. Similarly, the fourth question is symmetrical with the first step from the book example, so the answer is the answer to that first step, .4082.\n\n\n6.9.2.2 This one takes an extra step.\nWhat is the probability that \\(y\\) is between 30 and 40?\n\n\n\n\n\n\n\n6.9.2.3 Find the difference between areas.\nSubtract the probability that \\(y\\) is between 50 and 60 from the probability that \\(y\\) is between 50 and 70.\n\n\n\n\n\n\n\n\n6.9.3 How do you find areas in \\(z\\)-score tables?\n\ndraw picture to help you understand the question\nstandardize the picture so you can use a table or a\ndraw the standardized picture\npick one of three kinds of tables / apps\nwrite the standardized result (may do this multiple times)\nfit the standardized result(s) into the original problem\n\nLet’s look at these steps with an example. Suppose that \\(y\\sim N(50,8)\\). In words, this means that \\(y\\) has the normal distribution with true mean 50 and true standard deviation 8. Let’s answer the question What’s the probability that \\(y&gt;40\\)?\nStep 1 is to draw a picture to make sense of the question. The picture shows the area under the curve where the scale marking is to the right of 40. This picture tells you right away that the number that will answer the question is less than 1 (the entire curve would be shaded if it were 1) and more than 1/2 (the portion to the right of 50 would be 1/2 and we have certainly shaded more than that).\nStep 2 is to standardize the question so we can use a table or app to find the probability / area. We use the equation \\(z=(y-\\mu)/\\sigma\\) with values from the original question: \\((50-40)/8=-10/8=-1.25\\). Now we know the labels that would go on a standardized picture similar to the picture above. Now we can ask the standardized question What’s the probability that \\(z&gt;-1.25\\)?\nStep 3 is to draw that standardized picture. It’s the same as the picture above except standardized so that it fits the tables / apps for calculating probability / area. Now, instead of \\(y\\) we’re looking for \\(z\\) and the probability associated with \\(z\\) on a standardized table will be the same as for \\(y\\) on a table for the parameters given in the original question.\nStep 4 is to pick one of the three kinds of tables / apps to input the standardized \\(z\\) score to get a probability as output. In this example, we only have to do this step once because we only want to know the area greater than \\(y\\). If we wanted to know a range between two \\(y\\) values, we’d need the \\(z\\) scores for each of them so we’d have to do it twice.\nThe three kinds of output from tables / apps are as follows.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first figure shows how the table works in some books. It provides the value from 0 to \\(|z|\\). If \\(z\\) is negative, using this table requires you to input \\(|z|\\) instead. In this question, the value you get will be .3944. To get the ultimate answer to this question, Step 5 will be to add this value to .5, giving a final answer of .8944.\nThe second figure shows how most tables and apps work. It gives the value from \\(-\\infty\\) to \\(z\\). In this question, the value you get will be .1056. To get the ultimate answer to this question, Step 5 will be to subtract this value from 1, giving a final answer of .8944.\nThe bottom figure shows how some tables and apps work. It gives the value from z to \\(+\\infty\\). In this question, the value you get will be .8944. To get the ultimate answer to this question, Step 5 will be to simply report this value, giving a final answer of .8944.\nNotice that all three types of tables / apps lead to the same result by different paths. In this case, the right figure is the most convenient but, for other questions, one of the others may be more convenient.\nStep 5, the final step is to use the value you got from a table or app in conjunction with the original picture you drew in Step 1. Since the procedure for step 5 depends on the table / app you use, I gave the procedure for Step 5 above in the paragraphs for top, middle, and bottom figure."
  },
  {
    "objectID": "week06.html#estimate-a-population-parameter",
    "href": "week06.html#estimate-a-population-parameter",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.10 estimate a population parameter",
    "text": "6.10 estimate a population parameter\n\n6.10.1 Use a large-sample confidence interval.\nA large-sample \\(100(1-\\alpha)\\%\\) confidence interval for a population mean, \\(\\mu\\), is given by\n\\[\\mu \\pm z_{\\alpha/2} \\sigma_y \\approx \\overline{y}\\pm z_{\\alpha/2} \\frac{s}{\\sqrt{n}}\\]"
  },
  {
    "objectID": "week06.html#elements-of-a-hypothesis-test",
    "href": "week06.html#elements-of-a-hypothesis-test",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.11 elements of a hypothesis test",
    "text": "6.11 elements of a hypothesis test\n\nNull hypothesis\nAlternative hypothesis\nTest statistic\nLevel of significance\nRejection region marker\n\\(p\\)-value\nConclusion\n\n\n6.11.1 Work out a standard deviation example.\nExample exam question: ten samples of gas mileage have been calculated for a new model of car by driving ten examples of the car each for a week. The average of the ten samples is 17. The sum of the squares of the sample is 3281. What is their standard deviation?\n\n6.11.1.1 Recognize appropriate formulas.\nThe information in the problem statement hints that you should use \\[s=\\sqrt{\\frac{\\sum_{i=1}^n y_i^2 -\nn(\\overline{y})^2}{n-1}}\\] so you write \\[s=\\sqrt{\\frac{3281 - 10(17)^2}{10-1}}=\\sqrt{\\frac{3281-2890}{9}}=\\sqrt{43.4444}=6.5912\\]\n\n\n6.11.1.2 Why was the subtraction result positive?\nThe sample of ten gas mileage estimates is 17, 18, 16, 20, 14, 17, 21, 13, 22, 12, 17. The sum of their squares is inevitably larger than or equal to the mean squared times the number of values. The easiest way to see this is to use a series of identical values. Hence, finding the sum of the squares is the same as calculating the mean squared times the number of values. There is no variance at all in such a sample, so it makes sense to arrive at a standard deviation of zero. Is there any way to alter such a sample so that the sum of the squared values falls below the mean? No.\n\n\n6.11.1.3 What should you do when you don’t understand?\nThe previous example was developed as an answer to the question, what do I do if I need to do a negative square root? You can figure out that you will never need to do so by the preceding process of finding a way to make \\(\\sum y_i^2 = n(\\overline{y}^2)\\) and then trying to alter the values to decrease the left side or increase the right side."
  },
  {
    "objectID": "week06.html#type-i-and-type-ii-error",
    "href": "week06.html#type-i-and-type-ii-error",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.12 Type I and Type II error",
    "text": "6.12 Type I and Type II error\nTable 4.6 from James et al. (2021) shows the possible results of classification.\n\n\n\n\n\n\n\n\n\n\n\nTrue class\n\n\n\\(-\\) or Null\n\\(+\\) or Non-null\nTotal\n\n\n\n\nPredicted class\n\\(-\\) or Null\nTrue Negative (TN)\nFalse Negative (FN)\nN\\(*\\)\n\n\n\\(+\\) or Non-null\nFalse Positive (FP)\nTrue Positive (TP)\nP\\(*\\)\n\n\nTotal\nN\nP\n\n\n\n\nTable 4.7 from James et al. (2021) gives some synonyms for important measures of correctness and error in various disciplines.\n\n\n\nname\ndefinition\nsynonyms\n\n\n\n\nFalse Positive rate\nFP/N\nType I error, 1 \\(-\\) Specificity\n\n\nTrue Positive rate\nTP/P\n1 \\(-\\) Type II error, power, sensitivity, recall\n\n\nPositive Predictive value\nTP/P\\(*\\)\nPrecision, 1 \\(-\\) false discovery proportion\n\n\nNegative Predictive value\nTN/N\\(*\\)"
  },
  {
    "objectID": "week06.html#make-an-inference-about-a-population-parameter.",
    "href": "week06.html#make-an-inference-about-a-population-parameter.",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.13 Make an inference about a population parameter.",
    "text": "6.13 Make an inference about a population parameter.\nAt the beginning of the course, I said that statistics describes data and makes inferences about data. This course is partly about the latter, making inferences. You can make two kinds of inferences about a population parameter: estimate it or test a hypothesis about it.\n\n6.13.1 First distinguish between small and large.\nYou may have a small sample or a large sample. The difference in the textbook is typically given as a cutoff of 30. Less is small, more is large. Other cutoffs are given, but this is the most prevalent.\nLarge samples with a normal distribution can be used to estimate a population mean using a \\(z\\)-score. Small samples can be used to estimate a population mean using a \\(t\\)-statistic.\n\n\n6.13.2 Sampling leads to a new statistic.\nIf we take more and more samples from a given population, the variability of the samples will decrease. This relationship gives rise to the standard error of an estimate \\[\\sigma_{\\overline{y}}=\\frac{\\sigma}{\\sqrt{n}}\\]\nThe standard error of the estimate is not exactly the standard deviation. It is the standard deviation divided by a function of the sample size and it shrinks as the sample size grows."
  },
  {
    "objectID": "week06.html#estimate-a-population-mean",
    "href": "week06.html#estimate-a-population-mean",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.14 estimate a population mean",
    "text": "6.14 estimate a population mean\n\n6.14.1 Find a confidence interval.\nIf the sample is larger than or equal to 30, use the formula for finding a large-sample confidence interval to estimate the mean.\nA large-sample \\(100(1-\\alpha)\\%\\) confidence interval for a population mean, \\(\\mu\\), is given by\n\\[\\mu \\pm z_{\\alpha/2} \\sigma_{\\overline{y}} \\approx \\overline{y}\\pm z_{\\alpha/2} \\frac{s}{\\sqrt{n}}\\]\n\n\n6.14.2 For a small sample, use \\(t\\).\nIf the sample is smaller than 30, calculate a \\(t\\)-statistic and use the formula for finding a small-sample confidence interval to estimate the mean.\nThe \\(t\\)-statistic you calculate from the small sample is \\[\nt=\\frac{\\overline{y}-\\mu_0}{s/\\sqrt{n}}\n\\]\nDoes your calculated \\(t\\) fit within a \\(100(1-\\alpha)\\%\\) confidence interval? Find out by calculating that interval. A small-sample \\(100(1-\\alpha)\\%\\) confidence interval for a population mean, \\(\\mu\\), is given by\n\\[\\mu \\pm t_{\\alpha/2} s_{\\overline{y}} \\approx\n\\overline{y}\\pm t_{\\alpha/2,\\nu} \\frac{s}{\\sqrt{n}}\\] Here, you are using a \\(t\\) statistic based on a chosen \\(\\alpha\\) to compare to your calculated \\(t\\)-statistic. The Greek letter \\(\\nu\\), pronounced nyoo, represents the number of degrees of freedom.\n\n\n6.14.3 Find a probability to estimate a mean.\nEstimating a mean involves finding a region where the mean is likely to be. The first question to ask is how likely? and to do so, we use a concept called alpha, denoted by the Greek letter \\(\\alpha\\). Traditionally, people have used three values of \\(\\alpha\\) for the past century, .10, .05, and .01. These correspond to regions of \\(1-\\alpha\\) under the normal distribution curve, so these regions are 90 percent of the area, 95 percent of the area, and 99 percent of the area. What we are saying, for instance, if \\(\\alpha=0.01\\) is that we are 99 percent confident that the true population mean lies within the region we’ve calculated, \\(\\overline{y}\\pm 2.576\\sigma_{\\overline{y}}\\)\n\n\n6.14.4 \\(\\alpha\\) selection is driven by criticality.\nTraditionally, \\(\\alpha=0.01\\) is used in cases where life could be threatened by failure.\nTraditionally, \\(\\alpha=0.05\\) is used in cases where money is riding on the outcome.\nTraditionally, \\(\\alpha=0.10\\) is used in cases where the consequences of failing to capture the true mean are not severe.\n\nThe above picture shows these three cases. The top version, life-or-death, has the smallest rejection region. Suppose the test is whether a radical cancer treatment gives longer life than a traditional cancer treatment. Let’s say that the traditional treatment gives an average of 15 months longer life. The null hypothesis is that the new treatment also gives 15 months longer life. The alternative hypothesis is that the radical treatment gives 22 months more life on average based on on only five patients who received the new treatment. A patient can only do one treatment or the other. The status quo would be to take the traditional treatment unless there is strong evidence that the radical treatment provides an average longer life. In the following picture, the shaded area is where the test statistic would have to fall for us to say that there is strong evidence that the radical treatment provides longer life. We want to make that shaded region as small as possible so we minimize the chance our test statistic lands in it by mistake.\nWe can afford to let that shaded area be bigger (increasing the chance of mistakenly landing in it) if only money, not life, is riding on the outcome. And we can afford to let it be bigger still if the consequences of the mistake are small. To choose an \\(\\alpha\\) level, ask yourself how severe are the consequences of a Type I error.\n\n\n6.14.5 practice estimating a population mean\nConsider an outlet store that purchased used display panels to refurbish and resell. The relevant statistics for failure time of the sample are \\(n=50, \\overline{y}=1.9350, s=0.92865\\).\nBut these are the only necessary numbers to solve the problem. The standard error, .1313 can be calculated from \\(s/\\sqrt{n} = 0.92865/\\sqrt{50}=.1313.\\)\nFirst, find the 95% confidence interval, which can be calculated from the above information. Since the sample is greater than or equal to 30, we use the large sample formula: \\[\\begin{align*}\n\\overline{y}\\pm z_{\\alpha/2} \\sigma_{\\overline{y}} \\approx \\overline{y}\\pm z_{\\alpha/2} \\frac{s}{\\sqrt{n}}\n&= 1.9350 \\pm 1.96 (.1313) \\\\\n&= 1.9350 \\pm 0.2573 \\\\\n&= (1.6777,2.1932)\n\\end{align*}\\] The correct answer to a question asking for the confidence interval is simply that pair of numbers, 1.6777 and 2.1932.\nNow interpret that. This result says that we are 95% confident that the true average time to failure for these panels is somewhere between 1.6777 years and 2.1932 years. It is tempting to rephrase the result. Be careful that you don’t say something with a different meaning. Suppose the store wants to offer a warranty on these panels. Knowing that we are 95% confident that the true mean is in the given range helps the store evaluate the risk of different warranty lengths. The correct answer is to say that we are 95% confident that the true mean time to failure for these panels is somewhere between 1.6777 years and 2.1932 years.\nThe meaning of the 95 percent confidence interval is that, if we repeatedly resample the population, computing a 95% confidence interval for each sample, we expect 95% of the confidence intervals generated to capture the true mean.\nAny statistics software can also offer a graphical interpretation, such as a stem-and-leaf plot or histogram. The stem-and-leaf plot uses the metaphor of a stem bearing some number of leaves. In the following stem-and-leaf plot, the stem represents the first digit of a two-digit number. The top row of the plot has the stem 0 and two leaves, 0 and 2. Each leaf represents a data point as the second digit of a two-digit number. If you count the leaves (the digits to the right of the vertical bar), you will see that there are fifty of them, one for each recorded failure time. You can think of each stem as holding all the leaves in a certain range. The top stem holds all the leaves in the range .00 to .04 and there are two of them. The next stem holds the six leaves in the range .05 to .09. The third stem holds all six leaves in the range .10 to .15. The stem-and-leaf plot resembles a sideways bar chart and helps us see that the distribution of the failure times is somewhat mound-shaped. The main advantages of the stem-and-leaf plot are that it is compact for the amount of information it conveys and that it does not require a graphics program or even a computer to quickly construct it from the raw data. The programming website http://rosettacode.org uses the stem-and-leaf plot as a programming task, demonstrating how to create one in 37 different programming languages.\n0 | 02\n0 | 567788\n1 | 122222\n1 | 55666667888999\n2 | 0223344\n2 | 666788\n3 | 00233\n3 | 5555\nMost statistics programs offer many different histogram types. The simplest is equivalent to a barchart as follows."
  },
  {
    "objectID": "week06.html#test-a-hypothesis-about-a-population-mean",
    "href": "week06.html#test-a-hypothesis-about-a-population-mean",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.15 test a hypothesis about a population mean",
    "text": "6.15 test a hypothesis about a population mean\n\n6.15.1 Choose a null and alternative hypothesis.\nTesting a hypothesis must be done modestly. As an example of what I mean by modestly, consider criminal trials in the USA. The suspect on trial is considered innocent until proven guilty. The more modest hypothesis (the null hypothesis) is that the person is innocent. The more immodest hypothesis (the alternative hypothesis) carries the burden of proof.\n\n\n6.15.2 Type I error is worse than Type II error.\nThe traditional view of the legal system in the USA is that if you imprison an innocent person, it constitutes a more serious error than letting a guilty person go free.\nImprisoning an innocent person is like a Type I error: we rejected the null hypothesis when it was true. Letting a guilty person go free is like a Type II error: we failed to reject the null hypothesis when it was false.\n\n\n6.15.3 identify rejection region marker\nHow do you identify rejection region markers? A rejection region marker is the value a test statistic that leads to rejection of the null hypothesis. It marks the edge of a region under the curve corresponding to the level of significance, \\(\\alpha\\). The marker is not a measure of the size of the region. The rejection region marker can vary from \\(-\\infty\\) to \\(+\\infty\\) while the size of the region is somewhere between zero and one.\nThe following table shows the relevant values of \\(\\alpha\\) and related quantities.\n\n\n\n\n\n\n\n\n\n\\(1-\\alpha\\)\n\\(\\alpha\\)\n\\(\\alpha/2\\)\n\\(z_{\\alpha/2}\\)\n\n\n\n\n.90\n.10\n.05\n1.645\n\n\n.95\n.05\n.025\n1.96\n\n\n.99\n.01\n.005\n2.576\n\n\n\nThe above table refers to two-tailed tests. Only examples (e) and (f) below refer to two-tailed tests. In the other four cases, the \\(z\\)-scores refer to \\(z_{\\alpha}\\) rather than \\(z_{\\alpha/2}\\).\n(a) \\(\\alpha=0.025\\), a one-tailed rejection region\n\n\n\n\n\n(b) \\(\\alpha=0.05\\), a one-tailed rejection region\n\n\n\n\n\n(c) \\(\\alpha=0.005\\), a one-tailed rejection region\n\n\n\n\n\n(d) \\(\\alpha=0.0985\\), a one-tailed rejection region\n\n\n\n\n\n(e) \\(\\alpha=0.10\\), a two-tailed rejection region\n\n\n\n\n\n(f) \\(\\alpha=0.01\\), a two-tailed rejection region"
  },
  {
    "objectID": "week06.html#test-a-hypothesis-about-a-sample-mean",
    "href": "week06.html#test-a-hypothesis-about-a-sample-mean",
    "title": "6  Details of ggplot2; Introduction to Inference",
    "section": "6.16 test a hypothesis about a sample mean",
    "text": "6.16 test a hypothesis about a sample mean\nThe test statistic is \\(t_c\\), where the \\(c\\) subscript stands for calculated. Most people just call it \\(t\\). In my mind, that leads to occasional confusion about whether it is a value calculated from a sample. We’ll compare the test statistic to \\(t_{\\alpha,\\nu}\\), the value of the statistic at a given level of significance, identified using a table or calculator.\nThe test leads to two situations. The first, pictured below, is the situation where we fail to reject the null hypothesis and conclude that we have not seen evidence in the sample that the point estimate differs from what the modest hypothesis claims it is. Often, this is an estimate of a mean, where the population mean, \\(\\mu_0\\), is being estimated by a sample mean, \\(\\bar{y}\\). The following picture doesn’t show \\(\\bar{y}\\), just the tail. \\(\\bar{y}\\) would be at the top of the hump, not shown here.\n\n\n\n\n\nThe above possibility shows the situation where \\(t_{\\alpha,\\nu}&gt;t_c\\), which is equivalent to saying that \\(p&gt;\\alpha\\).\nThe \\(x\\)-axis scale begins to the left of the fragment shown here, and values of \\(t\\) increase from left to right. At the same time, the shaded regions decrease in size from left to right. Note that the entire shaded region above is \\(p\\), while only the darker region at the right is \\(\\alpha\\).\nThe situation pictured below is the reverse. In this situation, we reject the null hypothesis and conclude instead that the point estimate does not equal the value in the modest hypothesis. In this case \\(t_{\\alpha,\\nu}&lt;t_c\\), which is equivalent to saying that \\(p&lt;\\alpha\\).\n\n\n\n\n\nLet’s make this more concrete with an example.\nSuppose that a bulk vending machine dispenses bags expected to contain 15 candies on average. The attendant who refills the machine claims it’s out of whack, dispensing more than 15 candies on average, requiring more frequent replenishment and costing the company some extra money. The company representative asks him for a sample from the machine, so he produces five bags containing 25, 23, 21, 21, and 20 candies. Develop a hypothesis test to consider the possibility that the vending machine is defective. Use a level of significance that makes sense given the situation described above.\nThere are seven steps to solving this problem, as follows.\nStep 1. Choose a null hypothesis.\nAt a high level, we can say that we are trying to choose between two alternatives:\n\nthat the machine is defective, or\nthat the machine is operating normally.\n\nWe need to reduce this high level view to numbers. The problem states that the machine is expected to dispense 15 candies per bag, on average. This is equivalent to saying that the true mean is 15 or \\(\\mu=15\\).\nIf the machine is defective in the way the attendant claims, then \\(\\mu&gt;15\\). So we could say that one of the hypotheses would be that the sample came from a population with \\(\\mu=15\\) and the other hypothesis would be that the sample did not come from a population with \\(\\mu=15\\). Which should be the null hypothesis?\nThe null hypothesis represents the status quo, what we would believe if we had no evidence either for or against. Do you believe that the machine is defective if there is no evidence either that it is or isn’t? Let’s put it another way. Suppose you arrested a person for a crime and then realized that you have no evidence that they did commit the crime and no evidence that they did not commit the crime. Would you imprison them or let them go free? If you let them go free, it means that your null hypothesis is that they are innocent unless proven guilty.\nThis suggests that if you have no evidence one way or the other, assume the machine is operating normally. We can translate this into the null hypothesis \\(\\mu=15\\). The formal way of writing the null hypothesis is to say \\(H_0: \\mu=\\mu_0\\), where \\(\\mu_0=15\\). Later, when refer to this population mean, we call it \\(\\mu_0\\) because it is the population mean associated with the hypothesis \\(H_0\\). So later we will say \\(\\mu_0=15\\).\nAt the end of step 1, we have chosen the null hypothesis: \\(H_0: \\mu=\\mu_0\\) with \\(\\mu_0=15\\).\nStep 2. Choose the alternative hypothesis. The appropriate alternative hypothesis can be selected from among three choices: \\(\\mu&lt;\\mu_0\\), \\(\\mu&gt;\\mu_0\\), or \\(\\mu \\ne \\mu_0\\). The appropriate choice here seems obvious: all the sample values are much larger than \\(\\mu_0\\), so if the mean we calculate differs from \\(\\mu_0\\) it will have to be larger than \\(\\mu_0\\). If all the values in our sample are larger than \\(\\mu_0\\), there is just no way their average can be smaller than \\(\\mu_0\\).\nAt the end of step 2, we have determined the alternative hypothesis to be\n\\(H_a: \\mu&gt;\\mu_0\\) with \\(\\mu_0=15\\).\nStep 3. Choose the test statistic. Previously, we have learned two test statistics, \\(z\\) and \\(t\\). We have learned that the choice between them is predicated on sample size. If \\(n\\geqslant30\\), use \\(z\\), otherwise use \\(t\\). Here \\(n=5\\) so use \\(t\\). We can calculate the \\(t\\)-statistic, which I called \\(t_c\\) for calculated above, for the sample using the formula \\[\nt=\\frac{\\overline{y}-\\mu_0}{s/\\sqrt{n}}\n\\]\nWe can calculate the values to use from the following formulas or by using a machine.\n\\[\\overline{y}=\\sum{y_i}/n=22\\]\n\\[s=\\sqrt{\\frac{\\sum y_i^2 -\nn(\\overline{y})^2}{n-1}}=2\\]\n\\(\\mu_0\\) was given to us in the problem statement and \\(\\sqrt{n}\\) can be determined with the use of a calculator or spreadsheet program. The calculated \\(t\\)-statistic is \\(t_c=(22-15)/(2/\\sqrt{5})=7.8262\\).\nAt the end of step 3, you have determined and calculated the test statistic, \\(t_c=7.8262\\).\nStep 4. Determine the level of significance, \\(\\alpha\\). You choose the appropriate value of \\(\\alpha\\) from the circumstances given in the problem statement. Previously in class, I claimed that there are three common levels of significance in use as sumarized in the table on page 35: 0.01, 0.05, and 0.10. I gave rules of thumb for these three as 0.01 life-or-death, 0.05 money is riding on it, and 0.10 casual / low budget. In this case, the consequences seem to be a small amount of money lost by the company if they are basically giving away candies for free. I suggest that this is a case where some money is riding on it so choose \\(\\alpha=0.05\\).\nAt the end of step 4, you have determined \\(\\alpha=0.05\\).\nStep 5. Identify the rejection region marker. This is simply a matter of calculating (or reading from a table) an appropriate \\(t\\)-statistic for the \\(\\alpha\\) you chose in the previous step. This is \\(t_{\\alpha,\\nu}=t_{0.05,4}=2.131847\\). Note that \\(\\nu\\) is the symbol the book uses for df, or degrees of freedom. It is a Greek letter pronounced nyoo. For a single sample \\(t\\)-statistic, df\\(=\\nu=n-1\\).\nThis can be found using R by saying\n\nqt(0.95,4)\n\n[1] 2.131847\n\n\nAt the end of step 5, you have calculated the location of the rejection region (but not its size). It is located everywhere between the \\(t\\) curve and the horizontal line to the right of the point \\(t=2.131847\\).\nStep 6. Calculate the \\(p\\)-value. This is the size of the region whose location was specified in the previous step, written as \\(p=P(t_{\\alpha,\\nu}&gt;t_c)\\). It is the probability of observing a \\(t\\)-statistic greater than the calculated \\(t\\)-statistic if the null hypothesis is true. It is found by a calculator or app or software. It can only be calculated by hand if you know quite a bit more math than is required for this course. In this case \\(p=P(t_{\\alpha,\\nu}&gt;t_c)=0.0007195\\).\nWe can identify both \\(t_\\alpha\\) and \\(t_c\\) using R as follows:\n\n#. t sub alpha\nqt(0.95,4)\n\n[1] 2.131847\n\n#. find the alpha region associated with t sub alpha\n1-pt(2.131847,4)\n\n[1] 0.04999999\n\nx &lt;- c(25,23,21,21,20)\nt.test(x,alternative=\"greater\",mu=15)\n\n\n    One Sample t-test\n\ndata:  x\nt = 7.8262, df = 4, p-value = 0.0007195\nalternative hypothesis: true mean is greater than 15\n95 percent confidence interval:\n 20.09322      Inf\nsample estimates:\nmean of x \n       22 \n\n\nAlso note that the \\(t\\) test tells us that the true mean is far from 15. If we tested repeatedly, we would find that it is greater than 20.09322 in 95 percent of the tests.\nAt the end of step 6, we have calculated the \\(p\\)-value, \\(p=P(t_{\\alpha,\\nu} &gt; t_c)=0.0007195\\).\nStep 7. Form a conclusion from the hypothesis test. We reject the null hypothesis that \\(\\mu=\\mu_0\\), or in other words, we reject the hypothesis that these five bags came from a vending machine that dispenses an average of 15 candies per bag. Notice we don’t that the machine is defective. Maybe the data were miscounted. We don’t know. We have to limit our conclusion to what we know about the data presented to us, which is that the data presented to us did not come from a machine that dispense an average of 15 candies per bag.\nTo summarize the answer, the seven elements of this statistical test of hypothesis are:\n\nnull hypothesis \\(H_0: \\mu=15\\)\nalternative hypothesis \\(H_{a}: \\mu&gt;15\\)\ntest statistic \\(t_c=7.8262\\)\nlevel of significance \\(\\alpha=0.05\\)\nrejection region marker \\(t_{0.05,4} = 2.131847\\)\n\\(p\\)-value \\(P(t_{\\alpha,\\nu}&gt;t_c)=0.0007195\\)\nconclusion Reject the null hypothesis that these five bags came from a machine that dispenses an average of 15 candies per bag.\n\nLet’s return to the two pictures we started with. Notice that \\(p&lt;\\alpha\\) in this case, which is equivalent to saying that \\(t_c&gt;t_{\\alpha,\\nu}\\), so we are looking at the second of the two pictures.\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning, 2nd Edition. Springer New York.\n\n\nMendenhall, William, and Terry Sincich. 2012. A Second Course in Statistics, Regression Analysis, Seventh Edition. Boston, MA, USA: Prentice Hall."
  },
  {
    "objectID": "week07.html#recap-week-06",
    "href": "week07.html#recap-week-06",
    "title": "7  More about Inference",
    "section": "7.1 Recap Week 06",
    "text": "7.1 Recap Week 06\n\nggplot2\nFoundations for Inference\n\nPoint estimates\nConfidence Intervals\nHypothesis Tests\n\n\n\n7.1.1 A useful ggplot trick\nSome of you have struggled to produce boxplots of variables like price and odometer because they have ridiculous outliers. One way to overcome this that we’ve explored is to remove the extreme observations. A possibly better way, though, is illustrated below. Saying outlier.shape=NA in the geom_boxplot() function removes outliers before the dimensions of the plot are calculated. The result is a much more readable boxplot containing most of the cars.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vehicles.Rdata\"))\nlibrary(tidyverse)\ndf |&gt; ggplot(aes(condition,odometer))+geom_boxplot(outlier.shape=NA)+  scale_y_continuous(limits = quantile(df$odometer, c(0.1, 0.8),na.rm=TRUE))\n\nWarning: Removed 131132 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\ndf |&gt; ggplot(aes(condition,price))+geom_boxplot(outlier.shape=NA)+  scale_y_continuous(limits = quantile(df$price, c(0.1, 0.8),na.rm=TRUE))\n\nWarning: Removed 127440 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nNote that, in the above example, I’ve stored df in vehicles.Rdata using the save() function. Before saving it I converted condition to a factor and both price and odometer to numeric.\n\n\n7.1.2 Preparing data\nNote that you can use the above quantile trick in other ways. For example, here I’m filtering out the extreme values of price, without necessarily knowing what they are.\n\ndf |&gt;\n  filter(price &gt;= quantile(price,.10) & price &lt;= quantile(price,.90)) |&gt;\n  count(state) |&gt;\n  arrange(-n) |&gt;\n  as_tibble() |&gt;\n  print(n=12)\n\n# A tibble: 51 × 2\n   state     n\n   &lt;chr&gt; &lt;int&gt;\n 1 ca    39707\n 2 fl    23260\n 3 tx    18285\n 4 ny    15956\n 5 oh    15086\n 6 mi    14959\n 7 pa    11787\n 8 nc    11105\n 9 or    10875\n10 wi    10194\n11 tn     9466\n12 co     9335\n# ℹ 39 more rows"
  },
  {
    "objectID": "week07.html#textbook-section-6.1-inference-for-a-single-proportion",
    "href": "week07.html#textbook-section-6.1-inference-for-a-single-proportion",
    "title": "7  More about Inference",
    "section": "7.2 Textbook section 6.1 Inference for a single proportion",
    "text": "7.2 Textbook section 6.1 Inference for a single proportion\n\n7.2.1 Is the sample proportion nearly normal?\nNote: The book uses \\(p\\) in two ways: as a \\(p\\)-value in a hypothesis test, and as \\(p\\), a population proportion. Related to the population proportion is the sample proportion, \\(\\hat{p}\\), pronounced p-hat. Too many \\(p\\)s!\nThe sampling distribution for \\(\\hat{p}\\) based on a sample of size \\(n\\) from a population with a true proportion \\(p\\) is nearly normal when:\n\nThe sample’s observations are independent, e.g., are from a simple random sample.\nWe expected to see at least 10 successes and 10 failures in the sample, i.e., \\(np \\geqslant 10\\) and \\(n(1 − p) \\geqslant 10\\). This is called the success-failure condition.\n\nWhen these conditions are met, then the sampling distribution of \\(\\hat{p}\\) is nearly normal with mean \\(p\\) and standard error \\(\\text{SE}=\\sqrt{p(1-p)/n}\\).\n\n\n7.2.2 Confidence interval for a proportion\nA confidence interval provides a range of plausible values for the parameter \\(p\\), and when \\(\\hat{p}\\) can be modeled using a normal distribution, the confidence interval for \\(p\\) takes the form\n\\[ \\hat{p} \\pm z^{*} \\times \\text{SE} \\]\nwhere \\(z^{*}\\) marks the \\(x\\)-axis for the selected confidence interval, e.g., 1.96 for a 95 percent confidence interval.\n\n\n7.2.3 Prepare, Check, Calculate, Conclude Cycle\nThe OpenIntro Stats book recommends a four step cycle for both confidence intervals and hypothesis tests. It differs a bit from the seven step hypothesis testing method given in Week 06, but achieves the same result.\n\nPrepare. Identify \\(\\hat{p}\\) and \\(n\\), and determine what confidence level you wish to use.\nCheck. Verify the conditions to ensure \\(\\hat{p}\\) is nearly normal. For one-proportion confidence intervals, use \\(\\hat{p}\\) in place of \\(p\\) to check the success-failure condition.\nCalculate. If the conditions hold, compute SE using \\(\\hat{p}\\), find \\(z^{*}\\), and construct the interval.\nConclude. Interpret the confidence interval in the context of the problem.\n\n\n\n7.2.4 Same cycle for hypothesis testing for a proportion\n\nPrepare. Identify the parameter of interest, list hypotheses, identify the significance level, and identify \\(\\hat{p}\\) and \\(n\\).\nCheck. Verify conditions to ensure \\(\\hat{p}\\) is nearly normal under \\(H_0\\). For one-proportion hypothesis tests, use the null value to check the success-failure condition.\nCalculate. If the conditions hold, compute the standard error, again using \\(p_0\\), compute the \\(Z\\)-score, and identify the \\(p\\)-value.\nConclude. Evaluate the hypothesis test by comparing the \\(p\\)-value to \\(\\alpha\\), and provide a conclusion in the context of the problem.\n\n\n\n7.2.5 Choosing sample size when estimating a proportion\nThis is probably the most important part of this section for practical purposes. The following expression denotes the margin of error:\n\\[ z^{*}\\sqrt{\\frac{p(1-p)}{n}} \\]\nYou have to choose the margin of error you want to report. The book gives an example of 0.04. So you want to find\n\\[ z^{*}\\sqrt{\\frac{p(1-p)}{n}} &lt; 0.04 \\]\nThe problem is that you don’t know \\(p\\). Since the worst-case scenario is \\(p=0.5\\), you have to use that unless you have some information about \\(p\\). Recall that \\(z^{*}\\) represents the \\(z\\)-score for the desired confidence level, so you have to choose that. The book gives an example where you want a 95 percent confidence level, so you choose 1.96. You could find this out in R by saying\n\nqnorm(0.025,lower.tail=FALSE)\n\n[1] 1.959964\n\n\nreturning the \\(z\\)-score for the upper tail. The reason for saying that 0.025 instead of 0.05 is that the probability of 0.05 is split between the tails. The complementary function is pnorm(1.959964,lower.tail=FALSE), which will return 0.025.\nOnce you have decided on values for \\(p\\) and \\(z^*\\), solve the above inequality for \\(n\\)."
  },
  {
    "objectID": "week07.html#textbook-section-6.2-difference-of-two-proportions",
    "href": "week07.html#textbook-section-6.2-difference-of-two-proportions",
    "title": "7  More about Inference",
    "section": "7.3 Textbook section 6.2 Difference of two proportions",
    "text": "7.3 Textbook section 6.2 Difference of two proportions\nIn this section, we’re just modifying the previous section to account for a difference instead of a single proportion.\nThe difference \\(\\hat{p}_1-\\hat{p}_2\\) can be modeled using the normal distribution when\n\nThe data are independent within and between the two groups (random samples or randomized experiment)\nThe success-failure condition holds for both groups (at least 10 successes and 10 failures in each sample)\n\n\n7.3.1 Confidence intervals for \\(p_1-p_2\\)\n\\[ \\text{point estimate } \\pm z^* \\times \\text{SE} \\rightarrow (\\hat{p}_1 - \\hat{p}_2) \\pm z^* \\times \\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}} \\]\n\n\n7.3.2 Hypothesis testing for \\(p_1-p_2\\)\nWhen the null hypothesis is that the proportions are equal, use the pooled proportion (\\(\\hat{p}_\\text{pooled}\\)) to verify the success-failure condition and estimate the standard error\n\\[ \\hat{p}_\\text{pooled} = \\frac{\\text{number of “successes”}}{\\text{number of cases}} = \\frac{\\hat{p}_1n_1+\\hat{p}_2n_2}{n_1+n_2} \\]\nHere \\(\\hat{p}_1n_1\\) represents the number of successes in sample 1 since\n\\[ \\hat{p}_1 = \\frac{\\text{number of successes in sample 1}}{n_1} \\]\nSimilarly, \\(\\hat{p}_2n_2\\) represents the number of successes in sample 2."
  },
  {
    "objectID": "week07.html#textbook-section-6.3-testing-goodness-of-fit-using-chi2",
    "href": "week07.html#textbook-section-6.3-testing-goodness-of-fit-using-chi2",
    "title": "7  More about Inference",
    "section": "7.4 Textbook section 6.3 Testing goodness of fit using \\(\\chi^2\\)",
    "text": "7.4 Textbook section 6.3 Testing goodness of fit using \\(\\chi^2\\)\nThe \\(\\chi^2\\) test, pronounced k\\(\\overline{\\text{i}}\\) square, is useful in many circumstances. The textbook treats two such circumstances:\n\nSuppose your sample can be divided into groups, as can the general population. Does your sample represent the general population?\nDoes your sample resemble a particular distribution, such as the normal distribution?\n\nFor the first circumstance, we could divide a sample of people into races or genders and we would like to examine all at once for resemblance to the general population, rather than in pairs. The \\(\\chi^2\\) statistic will permit an all-at-once comparison.\nThe \\(\\chi^2\\) statistic is given by the following formula for \\(g\\) groups.\n\\[ \\chi^2 = \\frac{(\\text{observed count}_1-\\text{null count}_1)^2}{\\text{null count}_1} + \\cdots + \\frac{(\\text{observed count}_g-\\text{null count}_g)^2}{\\text{null count}_g} \\]\nwhere the expression null count refers to the expected number of objects in the group. You have to be careful about how you determine the null count. For instance, the textbook gives an example of races of jurors. In such a case, the null counts should come from the population who can be selected as jurors. This might be a matter of some dispute since jurors are usually recruited through voting records and these records may not reflect the correct proportions. Statisticians tend to like things they can count, and some people are harder (more expensive) to count than others, particularly people in marginalized populations.\n\n7.4.1 The \\(\\chi^2\\) distribution\nThe \\(\\chi^2\\) distribution is sometimes used to characterize data sets and statistics that are always positive and typically right skewed. Recall a normal distribution had two parameters – mean and standard deviation – that could be used to describe its exact characteristics. The \\(\\chi^2\\) distribution has just one parameter called degrees of freedom (df), which influences the shape, center, and spread of the distribution. Here is a picture of the \\(\\chi^2\\) distribution for several values of df (1–8).\n\n\n\n\n\n\nIn the jurors example, we can calculate the appropriate \\(p\\)-value in R by using the \\(\\chi^2\\) statistic calculated from the sample, 5.89, and the parameter \\(k-1\\) which is the number of groups minus one, using R:\n\npchisq(5.89,3,lower.tail=FALSE)\n\n[1] 0.1170863\n\n\nThis is a relatively large \\(p\\)-value given our earlier choices of cutoffs of 0.1, 0.05, and 0.01.\n\n\n7.4.2 \\(\\chi^2\\) test\nThe \\(\\chi^2\\) test can be conducted in R for the juror example given in the book as follows.\n\no &lt;- c(205,26,25,19)\ne &lt;- c(198,19.25,33,24.75)/sum(o)\nchisq.test(o,p=e)\n\n\n    Chi-squared test for given probabilities\n\ndata:  o\nX-squared = 5.8896, df = 3, p-value = 0.1171\n\n\nNote that I had to make an adjustment in R. The R variable p is supposed to be a vector of probabilities summing to 1. The way the table in the book presented it, it was not a vector of probabilities summing to one. So I divided each element of the input vector for e by the sum of the vector o."
  },
  {
    "objectID": "week07.html#textbook-section-6.4-testing-independence-in-2-way-tables",
    "href": "week07.html#textbook-section-6.4-testing-independence-in-2-way-tables",
    "title": "7  More about Inference",
    "section": "7.5 Textbook section 6.4 Testing independence in 2-way tables",
    "text": "7.5 Textbook section 6.4 Testing independence in 2-way tables\nSuppose you have a two way table. Datacamp gives an example of gender and sport as the two ways. The following data frame lists the number of males and females who like the following three sports: archery, boxing, and cycling. The \\(\\chi^2\\) tests suggests that the genders are not independent for the three sports, meaning that the preferences may differ by gender.\n\nfemale &lt;- c(35,15,50)\nmale &lt;- c(10,30,60)\ndf &lt;- cbind(male,female)\nrownames(df) &lt;- c(\"archery\",\"boxing\",\"cycling\")\ndf\n\n        male female\narchery   10     35\nboxing    30     15\ncycling   60     50\n\nchisq.test(df)\n\n\n    Pearson's Chi-squared test\n\ndata:  df\nX-squared = 19.798, df = 2, p-value = 5.023e-05"
  },
  {
    "objectID": "week08.html#milestone-2-review",
    "href": "week08.html#milestone-2-review",
    "title": "8  Inference for Numerical Data",
    "section": "8.1 Milestone 2 review",
    "text": "8.1 Milestone 2 review\n\nThere should be an overall summary at the end of the visual description section, rather than a separate overall summary from each group member (!?)\nPlease make the .qmd file more readable by doing a few things like leaving a blank line before each header and leaving a blank line before and after each chunk.\nThere should be no bar charts with two or three bars!\nThere should be no pie charts with fewer than three slices nor more than seven slices.\nPlease use headings (hashtags for levels 1, 2, and 3, for instance) rather than bold face text to mark sections. That way, I can view the structure of your document. Note that there are already level 1 and level 2 headings built in. You can add more.\nThe visual description section should not have long lists of numbers but rather graphics.\nAny list of numbers more than two screensful is unreadable by a manager anyway.\nGroup members need to spend more time and effort coordinating with each other.\nExplanations of your graphics help me believe that you understand what you are doing.\nThe numerical description is supposed to be left in and improved where possible. You are trying to build the components of a single coherent report, being added to with each milestone.\nIt is unwise to sign your individual names to the individual sections. This is supposed to be a group effort. You should be helping each other and reviewing the total package when it’s assembled.\nThere are techniques you can use to stay out of each other’s way. For example, how could you avoid giving two data frames the same name? (Hint: include your initials in the names of data frames you generate.)\nThe first two milestones are about description. It is not always reliable to make inferences from the descriptions. The second two milestones are about inferences. For example, it makes no sense to say that paint colors are evenly distributed among warm and cold states without doing a hypothesis test. A simple bar chart can provide a hint as to which hypothesis test you can run, but can’t substitute for a hypothesis test.\nIt is well known that it is hard to spot the differences between two versions of a file. That is why there is a whole industry of diff programs to highlight differences between files. You should each obtain and use some sort of diff program. For example, I use vimdiff because I use the Vim text editor. For relatively sophisticated text editors, there is likely to be a matching diff program or module. When you send me a file and I make changes to it, you should compare it to your original file using some version of diff. All information professionals should know about diff. For example, it is at the root of github, a website used by many information professionals.\nIf you only do what you’re explicitly told to do, you can and will be replaced by a bot. You have to be adventurous to be a valuable information professional."
  },
  {
    "objectID": "week08.html#individual-work-1-review",
    "href": "week08.html#individual-work-1-review",
    "title": "8  Inference for Numerical Data",
    "section": "8.2 Individual Work 1 Review",
    "text": "8.2 Individual Work 1 Review\n\n8.2.1 Product and Process\nThere’s a difference between the product you produce and the process you go through to get there. Sometimes you need to include aspects of the process and sometimes you don’t.\nAs an example of both, I investigated the bikes data. Through googling, I discovered that it actually came from Washington, DC. Since it began on January 1, 2011 and the day of the week was listed as 6, I opened a terminal on my Mac and said cal jan 2011, returning a calendar showing that January 1st was a Saturday. That way, I knew how to encode the days of the week as shown below, with Sunday as zero.\nNext, I noticed that January 1st had a 1 in the holiday column, so I inferred that 1 means yes and 0 means no. So I coded the labels that way. The same was true of workingday.\nThe column weathersit was an interesting problem. Examining the lengthy description of that column, I decided to code it as an ordered factor, with one being the best weather and 4 being the worst.\nFinally, I noticed that temp was coded in a way that made it hard for a Fahrenheit user like me to relate to. So I followed the formula in the description to convert the temperature to Fahrenheit. I did that in several lines so I could check my work and I left it in the code, so I could come back to it easily if I later discovered a problem with it.\nFollowing is my creation of the types. Notice that I also saved a copy of the typed data in bikes.RData so I could quit R and quickly restart in case things got messed up.\n\nlibrary(tidyverse)\ndf &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/bikes.csv\"))\ndf$instant &lt;- as.integer(df$instant)\ndf$season &lt;- factor(df$season,levels=1:4,labels=c(\"winter\",\"spring\",\"summer\",\"fall\"))\ndf$yr &lt;- factor(df$yr,levels=0:1,labels=c(\"first\",\"second\"),ordered=TRUE)\ndf$mnth &lt;- factor(df$mnth,levels=1:12,labels=c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"),ordered=TRUE)\ndf$hr &lt;- factor(df$hr,levels=0:23,ordered=TRUE)\ndf$holiday &lt;- factor(df$holiday,levels=0:1,labels=c(\"no\",\"yes\"))\ndf$weekday &lt;- factor(df$weekday,levels=0:6,labels=c(\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"),ordered=TRUE)\ndf$workingday &lt;- factor(df$workingday,levels=0:1,labels=c(\"no\",\"yes\"))\ndf$weathersit &lt;- factor(df$weathersit,levels=1:4,labels=c(\"best\",\"good\",\"poor\",\"worst\"))\ndf$casual &lt;- as.integer(df$casual)\ndf$registered &lt;- as.integer(df$registered)\ndf$cnt &lt;- as.integer(df$cnt)\nt_min &lt;- -8\nt_max &lt;- 39\n#. df$temp == (t-t_min)/(t_max-t_min)\n#. (t_max-t_min)*df$temp == t-t_min\ndf$celsius &lt;- ((t_max-t_min)*df$temp)+t_min\ndf$fahr &lt;- df$celsius*9/5+32\n#. save(df,file=\"bikes.Rdata\")\n#. load(df,file=\"bikes.Rdata\")\n\nAfter setting up the types, I wanted to obtain a numerical summary of all the data. I started by saying things like summary(df$temp) but I noticed that it was pretty hard to read for a manager and the assignment spec calls for a report you can give to a manager. (Later, you’ll learn how to make the reports even more manager-friendly.)\nSo I googled an expression like r summary statistics for all columns and immediately came across vtable, a package to do just that. So I read its documentation, which mentioned that there are many similar packages. Instead of searching for those packages and comparing them, I just tried the st() function in vtable, with the following result.\n\nlibrary(vtable)\nst(df)\n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\ninstant\n17379\n8690\n5017\n1\n4346\n13034\n17379\n\n\nseason\n17379\n\n\n\n\n\n\n\n\n... winter\n4242\n24%\n\n\n\n\n\n\n\n... spring\n4409\n25%\n\n\n\n\n\n\n\n... summer\n4496\n26%\n\n\n\n\n\n\n\n... fall\n4232\n24%\n\n\n\n\n\n\n\nyr\n17379\n\n\n\n\n\n\n\n\n... first\n8645\n50%\n\n\n\n\n\n\n\n... second\n8734\n50%\n\n\n\n\n\n\n\nmnth\n17379\n\n\n\n\n\n\n\n\n... Jan\n1429\n8%\n\n\n\n\n\n\n\n... Feb\n1341\n8%\n\n\n\n\n\n\n\n... Mar\n1473\n8%\n\n\n\n\n\n\n\n... Apr\n1437\n8%\n\n\n\n\n\n\n\n... May\n1488\n9%\n\n\n\n\n\n\n\n... Jun\n1440\n8%\n\n\n\n\n\n\n\n... Jul\n1488\n9%\n\n\n\n\n\n\n\n... Aug\n1475\n8%\n\n\n\n\n\n\n\n... Sep\n1437\n8%\n\n\n\n\n\n\n\n... Oct\n1451\n8%\n\n\n\n\n\n\n\n... Nov\n1437\n8%\n\n\n\n\n\n\n\n... Dec\n1483\n9%\n\n\n\n\n\n\n\nhr\n17379\n\n\n\n\n\n\n\n\n... 0\n726\n4%\n\n\n\n\n\n\n\n... 1\n724\n4%\n\n\n\n\n\n\n\n... 2\n715\n4%\n\n\n\n\n\n\n\n... 3\n697\n4%\n\n\n\n\n\n\n\n... 4\n697\n4%\n\n\n\n\n\n\n\n... 5\n717\n4%\n\n\n\n\n\n\n\n... 6\n725\n4%\n\n\n\n\n\n\n\n... 7\n727\n4%\n\n\n\n\n\n\n\n... 8\n727\n4%\n\n\n\n\n\n\n\n... 9\n727\n4%\n\n\n\n\n\n\n\n... 10\n727\n4%\n\n\n\n\n\n\n\n... 11\n727\n4%\n\n\n\n\n\n\n\n... 12\n728\n4%\n\n\n\n\n\n\n\n... 13\n729\n4%\n\n\n\n\n\n\n\n... 14\n729\n4%\n\n\n\n\n\n\n\n... 15\n729\n4%\n\n\n\n\n\n\n\n... 16\n730\n4%\n\n\n\n\n\n\n\n... 17\n730\n4%\n\n\n\n\n\n\n\n... 18\n728\n4%\n\n\n\n\n\n\n\n... 19\n728\n4%\n\n\n\n\n\n\n\n... 20\n728\n4%\n\n\n\n\n\n\n\n... 21\n728\n4%\n\n\n\n\n\n\n\n... 22\n728\n4%\n\n\n\n\n\n\n\n... 23\n728\n4%\n\n\n\n\n\n\n\nholiday\n17379\n\n\n\n\n\n\n\n\n... no\n16879\n97%\n\n\n\n\n\n\n\n... yes\n500\n3%\n\n\n\n\n\n\n\nweekday\n17379\n\n\n\n\n\n\n\n\n... Sunday\n2502\n14%\n\n\n\n\n\n\n\n... Monday\n2479\n14%\n\n\n\n\n\n\n\n... Tuesday\n2453\n14%\n\n\n\n\n\n\n\n... Wednesday\n2475\n14%\n\n\n\n\n\n\n\n... Thursday\n2471\n14%\n\n\n\n\n\n\n\n... Friday\n2487\n14%\n\n\n\n\n\n\n\n... Saturday\n2512\n14%\n\n\n\n\n\n\n\nworkingday\n17379\n\n\n\n\n\n\n\n\n... no\n5514\n32%\n\n\n\n\n\n\n\n... yes\n11865\n68%\n\n\n\n\n\n\n\nweathersit\n17379\n\n\n\n\n\n\n\n\n... best\n11413\n66%\n\n\n\n\n\n\n\n... good\n4544\n26%\n\n\n\n\n\n\n\n... poor\n1419\n8%\n\n\n\n\n\n\n\n... worst\n3\n0%\n\n\n\n\n\n\n\ntemp\n17379\n0.5\n0.19\n0.02\n0.34\n0.66\n1\n\n\natemp\n17379\n0.48\n0.17\n0\n0.33\n0.62\n1\n\n\nhum\n17379\n0.63\n0.19\n0\n0.48\n0.78\n1\n\n\nwindspeed\n17379\n0.19\n0.12\n0\n0.1\n0.25\n0.85\n\n\ncasual\n17379\n36\n49\n0\n4\n48\n367\n\n\nregistered\n17379\n154\n151\n0\n34\n220\n886\n\n\ncnt\n17379\n189\n181\n1\n40\n281\n977\n\n\ncelsius\n17379\n15\n9.1\n-7.1\n8\n23\n39\n\n\nfahr\n17379\n60\n16\n19\n46\n73\n102\n\n\n\n\n\n\n\nI was fairly satisfied with that result, but it uncovered some errors in my data typing, so I went back and redid the previous step. Then I tried the st() function again, found more errors, and so on until I was completely satisfied that I had fixed everything.\nNext, I made a couple of contingency tables. These made me think about the meaning of the rows in the data and the fact that not all hours are covered. I went back to the output of st() above and learned more before I made the following description for the contingency tables.\nThe next two tables show the numbers of hours when riders experienced each weather condition, first during each season, then on each weekday.\n\naddmargins(table(df$season,df$weathersit))\n\n        \n          best  good  poor worst   Sum\n  winter  2665  1205   369     3  4242\n  spring  2859  1144   406     0  4409\n  summer  3280   947   269     0  4496\n  fall    2609  1248   375     0  4232\n  Sum    11413  4544  1419     3 17379\n\naddmargins(table(df$weekday,df$weathersit))\n\n           \n             best  good  poor worst   Sum\n  Sunday     1765   568   169     0  2502\n  Monday     1582   726   170     1  2479\n  Tuesday    1522   694   237     0  2453\n  Wednesday  1568   613   293     1  2475\n  Thursday   1656   636   179     0  2471\n  Friday     1645   659   183     0  2487\n  Saturday   1675   648   188     1  2512\n  Sum       11413  4544  1419     3 17379\n\n\nAfter a few more boring contingency tables, I switched to visualizations. First, I just started aimlessly dong visualizations and looking at them. Let me emphasize that I wouldn’t necessarily include these in my final report. They are just exploratory at this point to see if I find something interesting.\n\ndf |&gt; ggplot(aes(mnth,fahr)) + geom_line()\n\n\n\ndf |&gt; ggplot(aes(mnth,cnt)) + geom_line()\n\n\n\ndf |&gt; ggplot(aes(weathersit,cnt)) + geom_boxplot()\n\n\n\ndf |&gt; ggplot(aes(season,cnt)) + geom_boxplot()\n\n\n\ndf |&gt; ggplot(aes(weekday,cnt)) + geom_violin()\n\n\n\ndf |&gt; ggplot(aes(hr,cnt)) + geom_violin()\n\n\n\ndf |&gt; ggplot(aes(casual)) + geom_histogram()\n\n\n\ndf |&gt; ggplot(aes(registered)) + geom_histogram()\n\n\n\n\nBingo! I have found something interesting in the last two plots above. But the way they’re done doesn’t reveal enough. When drawing histograms of the casual riders and the registered riders, it really stands out that there’s a hump in the registered riders, while there’s exponential decay in the casual riders. You can illustrate this by combining the two, casual and registered, into a single density plot as follows.\n\ncas &lt;- data.frame(vol=df$casual)\nreg &lt;- data.frame(vol=df$registered)\ncas$type &lt;- \"casual\"\nreg$type &lt;- \"registered\"\ncombined &lt;- rbind(cas,reg)\nggplot(combined,aes(vol,fill=type)) + geom_density(alpha=0.3)\n\n\n\n\nHow did I come up with this? The main thing was a process of exploring the data through visualizing, making a discovery, then communicating that discovery. I would remove the two histograms (and most of the plots above them, too) and instead substitute the density plot in a report, never really showing the manager the process I went through.\nIncidentally, I simple googled tidyverse difference between two histograms to come up with the code for the density plot, which I adapted from one of many good answers on stackoverflow.\n\n\n8.2.2 Common Mistakes\nIt’s tough to grade this exercise because there’s a tension between two factors. First is the amount of work you put in, which, in most cases, was a lot. I want to reward you for that.\nSecond is that you made a lot of common sense mistakes. There’s a big difference between making mistakes in coding, which is only natural in a new language, and making mistakes that a manager unfamiliar with code will notice.\nLet me list a few such mistakes here.\n\nIncredibly long contingency tables: Think about what a manager can possibly learn from the artifacts you produce—if a table goes on for many screensful of output, it doesn’t do any more than presenting the raw data\nGraphics that are hard to read and look almost identical to each other: Think about what a manager can infer from a bunch of nearly identical graphics\nExplanations that don’t make any sense considering the data, such as assuming that the left side of a graphic that doesn’t include time refers to early data\nExplanations that are overly obvious, such as the fact that the data begins in winter and continues into summer"
  },
  {
    "objectID": "week08.html#recap-week-07",
    "href": "week08.html#recap-week-07",
    "title": "8  Inference for Numerical Data",
    "section": "8.3 Recap Week 07",
    "text": "8.3 Recap Week 07\n\nInference for Categorical Data\n\nInference for a single proportion\nDifference of two proportions\nTesting goodness of fit using \\(\\chi^2\\)\nTesting independence of contingency tables"
  },
  {
    "objectID": "week08.html#inference-for-numerical-data",
    "href": "week08.html#inference-for-numerical-data",
    "title": "8  Inference for Numerical Data",
    "section": "8.4 Inference for numerical data",
    "text": "8.4 Inference for numerical data\n\nTextbook section 7.1 One-sample means with the t-distribution\nTextbook section 7.2 Paired data\nTextbook section 7.3 Difference of two means\nTextbook section 7.4 Power calculations for a difference of means\nTextbook section 7.5 Comparing many means with ANOVA\n\n\n8.4.1 Textbook Section 7.1 One-sample means with the \\(t\\)-distribution\nModeling \\(\\bar{x}\\), the mean of a random sample from a normally distributed population, requires that the sample elements are\n\nindependent—a random sample or a sample from a random process\nnormally distributed—sample drawn from a normally distributed population\n\n\n8.4.1.1 Rule of thumb for normality\n\n\\(n &lt; 30\\) and no outliers, assume data come from a normally distributed population\n\\(n \\geqslant 30\\) and no extreme outliers, assume \\(\\bar{x}\\sim N(\\mu,\\sigma)\\) even if data come from a not normally distributed population\n\n\n\n8.4.1.2 \\(t\\)-distribution\nThe \\(t\\)-distribution is useful for small samples (\\(n&lt;30\\)). It was discovered when a man named Gossett was trying to figure out how few samples of beer he could get away with in tests for the Guinness brewery about 120 years ago. He preferred to remain anonymous at the time because he didn’t want his employers to question his outside activities, otherwise this would probably be called the Gossett’s \\(t\\)-distribution. Instead, he referred to himself as “A Student” so it came to be known as the Student’s \\(t\\)-distribution.\nFor sample sizes over thirty, it converges to looking like the normal distribution, but for smaller samples, it gets more and more peaked and the tails get thicker and thicker. For example, here is the density function for a sample size of 4.\n\nplot(function(x) dt(x, df = 3), -11, 11, ylim = c(0, 0.50), main = \"t density function\", yaxs = \"i\")\n\n\n\n\nBear in mind that the t() function in R has nothing to do with the \\(t\\)-distribution (it’s for transposing matrices and data frames). Instead, the functions for handling the \\(t\\)-distribution are the letter t prefaced by d, q, p, or r. You may have noticed that we saw functions like pnorm() and dnorm() when working with the normal distribution. These functions are analogous.\nThe \\(t\\)-distribution has \\(n-1\\) degrees of freedom, so you can tell that the above example has \\(n=4\\) since \\(\\text{df}=3\\).\nAlso keep in mind that the mean is always zero for the \\(t\\)-distribution, so it just has one parameter, df. So in the above example, you could say \\(\\bar{x}\\sim t(3)\\).\nAnalogous to the pnorm() function, you can calculate regions of the \\(t\\)-distribution using the pt() function. For example, if you conduct a test that returns a \\(t\\)-statistic of \\(-2.10\\) and \\(n=19\\), you can use the following to find out that the area to the left of the statistic is 0.025. (This example and the two following are illustrated in textbook Figure 7.4.)\n\npt(-2.1,18)\n\n[1] 0.0250452\n\n\nSuppose you obtain a \\(t\\)-statistic of 1.65 on 20 degrees of freedom. How much of the probability is in the upper tail? There are two obvious ways to do this.\n\npt(1.65,20,lower.tail=FALSE)\n\n[1] 0.05728041\n\n1-pt(1.65,20)\n\n[1] 0.05728041\n\n\nFind the probability in both tails for a \\(t\\)-statistic of \\(\\pm 3\\) and two degrees of freedom.\n\npt(-3,2)+pt(3,2,lower.tail=FALSE)\n\n[1] 0.09546597\n\n\nTextbook example 7.8 asks you to calculate the \\(t\\)-statistic when you know the proportion. In this case, \\(df=18\\) and you want to know the \\(t\\)-statistic corresponding to 0.025 in the upper tail. You can use the qt() function where q stands for quantile. The region 0.025 in the upper tail corresponds to a 95 percent confidence interval because there will be 0.025 in each of the two tails for a total of five percent. The \\(t\\)-statistic for the lower tail would simply be the negative of the \\(t\\)-statistic for the upper tail.\n\nqt(0.025,18,lower.tail=FALSE)\n\n[1] 2.100922\n\n\nTo construct a confidence interval, you’ll generally choose 90 or 95 percent, depending on the sensitivity of the real world problem. Then you’ll plug that into the following formula.\n\\[\n\\bar{x} \\pm t^*_{df}\\frac{s}{\\sqrt{n}}\n\\]\nThis assumes you have already checked the normality and independence constraints.\n\n\n8.4.1.3 Calculating a confidence interval\nFor the textbook examples, you are given components of the formula. It is quite a bit simpler if you are given the raw data. For example, calculate a 95 percent confidence interval for the body mass in grams of the penguins in the Palmer penguins data frame.\n\nlibrary(palmerpenguins)\nmodel &lt;- lm(body_mass_g ~ 1,penguins)\nconfint(model,level=0.95)\n\n               2.5 %  97.5 %\n(Intercept) 4116.458 4287.05\n\n\nThe above incantation may seem a little mysterious but you’ll explore it in excruciating detail when you learn linear regression.\n\n\n8.4.1.4 One sample \\(t\\)-tests\nThe textbook gives a lengthy example of the runner times of the Cherry Blossom race. I assume that the data are the run10samp and run17samp data frames from the textbook website, so I downloaded the RData versions of them and loaded them as follows.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/run10samp.rda\"))\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/run17samp.rda\"))\nt.test(run17samp$clock_sec/60,mu=93.29,alternative=\"t\")\n\n\n    One Sample t-test\n\ndata:  run17samp$clock_sec/60\nt = 1.9776, df = 99, p-value = 0.05075\nalternative hypothesis: true mean is not equal to 93.29\n95 percent confidence interval:\n  93.26973 105.46227\nsample estimates:\nmean of x \n   99.366 \n\n\nNote that neither time measure, clock_sec nor net_sec correspond to the mean in the textbook. The \\(t\\)-statistic is smaller and the \\(p\\)-value is larger than that given in the textbook. With a \\(p\\)-value of 0.05075 it is unclear whether you would reject the null hypothesis. Certainly the old mean is within, though at the edge, of the confidence interval. Personally, I would fail to reject in this case.\n\n\n\n8.4.2 Textbook Section 7.2 Paired data\nSuppose you want to know if two data frames were drawn from the same distribution or if they differ.\nThe textbook example is of the mean prices of textbooks on Amazon and in the UCLA campus bookstore. The data appear to be the textbooks data frame on the textbook website, although the statistics are different and the textbook says that there were two such samples (only one is on the website that I could find). Because they have precomputed the difference as the diff column, you can do this the same way as for a one sample test.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/textbooks.rda\"))\nwith(textbooks,t.test(diff))\n\n\n    One Sample t-test\n\ndata:  diff\nt = 7.6488, df = 72, p-value = 6.928e-11\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n  9.435636 16.087652\nsample estimates:\nmean of x \n 12.76164 \n\n\nNote that the default is that the difference is 0 and the 95 percent confidence interval is quite far from including 0. Also, the \\(p\\)-value is infinitesimal. We definitely reject the null hypothesis that the stores have similar prices.\nIf you didn’t have the diff column, you could get the same result by saying the following.\n\nwith(textbooks,t.test(ucla_new,amaz_new,paired=TRUE))\n\n\n    Paired t-test\n\ndata:  ucla_new and amaz_new\nt = 7.6488, df = 72, p-value = 6.928e-11\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n  9.435636 16.087652\nsample estimates:\nmean difference \n       12.76164 \n\n\n\n\n8.4.3 Textbook Section 7.3 Difference of two means\nIn the previous section, you considered the means of the differences but in this section you consider the differences of the means. In the Amazon and UCLA example, the items were paired and we subtracted the price of a particular title sold by one seller from the price of the same title sold by the other seller. But what if we have data that is not paired like this? The textbook gives an example of a radical stem cell treatment given to sheep. One of two treatments is given to each sheep, but there is no correspondence between individual pairs of sheep.\nIn this case, there may be different variance between the two groups, as well as different means. So the standard error is calculated as\n\\[\\text{SE}=\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\]\nNotice that this formula implies that the samples could differ in size as well as variance.\nFor the sheep example, heart pumping capacity was measured, where more is better. The stem_cell data frame on the textbook website seems to be the appropriate data frame here. Conducting the test in R follows.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/stem_cell.rda\"))\nwith(stem_cell,t.test(after-before~trmt,var.equal=FALSE))\n\n\n    Welch Two Sample t-test\n\ndata:  after - before by trmt\nt = -4.0073, df = 12.225, p-value = 0.001678\nalternative hypothesis: true difference in means between group ctrl and group esc is not equal to 0\n95 percent confidence interval:\n -12.083750  -3.582916\nsample estimates:\nmean in group ctrl  mean in group esc \n         -4.333333           3.500000 \n\n\nThe result is a statistically significant difference between the sheep in the control group and the sheep treated with stem cells. The sheep in the stem cell group enjoyed a 3.5 unit increase in heart pumping capacity, while the poor sheep in the control group lost four and a third units. Of course, the practical question you have to ask yourself is whether these numbers have a practical significance. You would need domain knowledge to tell whether 3.5 units is a lot of heart pumping capacity!\n\n\n8.4.4 Textbook Section 7.4 Power calculations for a difference of means\nThe pictures in section 7.4, particularly the two on page 280 of the textbook, are essential for understanding power calculation, so let’s use the textbook exclusively for this section. To do the calculations in R, you can use the pwr package.\n\n\n8.4.5 Textbook Section 7.5 Comparing many means with ANOVA\nTextbook exercise 7.54 compares eight methods for loosening rusty bolts. Four samples were collected for each method and the results are in the penetrating_oil data frame on the textbook website. You can conduct an ANOVA test on the results using R as follows.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/penetrating_oil.rda\"))\nwith(penetrating_oil,anova(lm(torque~treatment)))\n\nAnalysis of Variance Table\n\nResponse: torque\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \ntreatment  7 3603.4  514.78  4.0263 0.005569 **\nResiduals 22 2812.8  127.85                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere you are comparing whether any of the eight means of torque differ. They certainly seem to, with a large \\(F\\)-statistic and a small \\(p\\)-value. It might be helpful to visualize the differences with a combination violin plot and boxplot. There are really too few samples for each treatment.\n\nlibrary(tidyverse)\npenetrating_oil |&gt;\n  ggplot(aes(torque,treatment)) +\n  geom_violin() +\n  geom_boxplot(width=0.1)"
  },
  {
    "objectID": "week09.html#recap-week-8-inference-for-numerical-data",
    "href": "week09.html#recap-week-8-inference-for-numerical-data",
    "title": "9  Introduction to Linear Regression",
    "section": "9.1 Recap week 8: Inference for numerical data",
    "text": "9.1 Recap week 8: Inference for numerical data\n\nTextbook section 7.1 One-sample means with the t-distribution\nTextbook section 7.2 Paired data\nTextbook section 7.3 Difference of two means\nTextbook section 7.4 Power calculations for a difference of means\nTextbook section 7.5 Comparing many means with ANOVA"
  },
  {
    "objectID": "week09.html#linear-regression",
    "href": "week09.html#linear-regression",
    "title": "9  Introduction to Linear Regression",
    "section": "9.2 Linear Regression",
    "text": "9.2 Linear Regression\n\n9.2.1 Introduction\nLet’s start with an example. Suppose we know how many times a team has won and we can graph it as follows. I haven’t included a scale on the graph but the underlying numbers of wins are 9, 15, 18, 22.5 (a tie), and 23.\n\n\n\n\n\nWhat is our best prediction of the number of wins for a new team, irrespective of any other information. The answer is the mean of this number of wins, 17.5. It’s not a very good prediction but it’s the best we can do, given what little we know. Now suppose we know how much the team spends on player salaries. We’ll call that variable payroll and add it to the graph.\n\n\n\n\n\nNow we can see a pattern. Teams that spend more win more! But that’s not strictly true because there must be other variables at play. Right now, we don’t know what those variables are. But the best prediction we can make for the number of wins for a given new team depends in part on its payroll. We can now draw a diagonal line through this cloud of points and say that that line represents the best prediction for number of wins, like so.\n\n\n\n\n\nOne immediate question to ask is whether the diagonal line pictured above is the best line. People searched for ways of finding the best line for a long time before the most popular method, least squares, was published in 1805. That is the method used most frequently by software but other methods have emerged since then.\nThe best line is often called the least squares line and it is characterized by two numbers, the slope and the intercept. The intercept is the height at which it intersects the y axis and the slope is the ratio of its rise (its increaase or decrease on the y axis) over its run (its increase or decrease on the x axis). Both the slope and intercept can be positive or negative. A slope of zero or infinity is meaningless. In the statistics world, the intercept is usually called \\(\\beta_0\\), pronounced beta nought, and the slope is usually called \\(\\beta_1\\), pronounced beta one. The reason we use the numeral one is because additional slopes will be considered when we talk about multiple regression.\nThe Diez, Çetinkaya-Rundel, and Barr (2019) textbook differs a bit from standard practice, using the Latin letter b in place of the Greek \\(\\beta\\). This works well from my point of view because Latin letters are often used to denote estimates of parameters, while the parameters themselves are denoted by Greek letters. Keep in mind that we’re always working with samples so we’re estimating the true slope and intercept with \\(b_1\\) and \\(b_0\\). Other statistics books sometimes use \\(\\hat{\\beta}\\) to refer to estimates, which is kind of cumbersome.\nMost of our discussion of regression will focus on this least squares line and how good it is. Keep in mind that the word regression is more general than least squares. There are other methods and applications. Least squares is just the easiest way to introduce regression.\nWhat does the term least squares mean? In the following picture, we have added vertical lines connecting the dots to the least squares line. The squares of the lengths of these lines are the way we measure the quality of the line.\n\n\n\n\n\nIn the following pictures, the prediction line on the right is better than that on the left by an amount proportional to the difference between the total length of pink lines in the two pictures. Notice that both lines represent the very best possible prediction for that set of dots. It’s just that, on the right hand side, there’s a closer correspondence between payroll and wins.\n\n\n\n\n\n\n\n9.2.2 Correlation\nCorrelation is a concept that measures the strength of the linear relationship between two variables. We usually use Pearson’s correlation coefficient, \\(r\\), to measure this kind of relationship. Note that our textbook uses \\(R\\) instead of \\(r\\) to denote this relationship. This is unfortunate, because almost every other statistics book makes a distinction between \\(R\\) as the multiple correlation coefficient and \\(r\\) as Pearson’s correlation coefficient. They happen to be identical in the case of one \\(x\\) and one \\(y\\), but soon we will consider the case of more than one \\(x\\), where they differ.\n\\[r=\\frac{1}{n-1}\\sum^n_{i=1}\\frac{x_i-\\bar{x}}{s_x}\\frac{y_i-\\bar{y}}{s_y}\\]\n\n\n9.2.3 Least squares line\nDiez, Çetinkaya-Rundel, and Barr (2019) gives the following formulas for finding the least squares line.\n\nFind the slope.\n\n\\[b_1=\\frac{s_y}{s_x}r\\]\n\nFind the intercept.\n\n\\[b_0=\\bar{y}-b_1\\bar{x}\\]\nTypically, you use software to identify these numbers. For example, consider the payroll / wins example from above, calculated in R.\n\ny&lt;-c(9,18,15,23,22.5)\nx&lt;-c(5,10,15,20,25)\nsummary(lm(y~x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n   1    2    3    4    5 \n-2.1  3.7 -2.5  2.3 -1.4 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   7.9000     3.4039   2.321   0.1030  \nx             0.6400     0.2053   3.118   0.0526 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.246 on 3 degrees of freedom\nMultiple R-squared:  0.7642,    Adjusted R-squared:  0.6856 \nF-statistic: 9.722 on 1 and 3 DF,  p-value: 0.05256\n\n\nI input the number of wins as \\(y\\) and the amount of the payroll as \\(x\\). Then I constructed a linear model of \\(y\\) explained by \\(x\\). The lm() function constructs a linear model and the tilde character (\\(\\sim\\)) separates the response variable \\(y\\) from the explanatory variable \\(x\\). The summary() function is wrapped around the lm() function to provide the most commonly accessed values of the output of the lm() function.\nThe first value output by summary() is the call. This simply shows the formula entered, which in this case was \\(y\\sim x\\).\nThe next value output by summary() is a list of residuals. These are the differences between the predicted values and the actual values of wins.\nThe third value output by summary() is the coefficients table. The \\(b\\) values are listed in the Estimate column. Instead of being named \\(b_0\\) and \\(b_1\\) they are called (Intercept) and x. The remainder of this table consists of statistics about them. The second column is standard error, the third column is the \\(t\\)-statistic, which is the ratio of the estimate to its standard error. The third column is the \\(p\\)-value, which is the probability of seeing the preceding \\(t\\)-statistic or a larger one if the null hypothesis is true. The null hypothesis here is that \\(x\\) does not predict \\(y\\).\nThe last column in the coefficents table contains the significance codes. In this case, \\(x\\) gets a significance code of dot (.). Below the coefficients table is a legend for the significance codes. That tells us that dot means that the \\(p\\)-value for \\(x\\) is below 0.1. The blank in the other row tells us that the significance code for the intercept is less than 1, which it must be because probabilities can be no larger than 1.\nBelow the coefficients table we see four important values expressed - Multiple R-squared, which is identical to \\(r^2\\) above, - Adjusted R-squared, which we will discuss when we discuss multiple linear regression, - the F-statistic, which we will also discuss under multiple linear regression, and - the \\(p\\)-value of the F-statistic.\nThese latter values are all more interesting in the case of multiple linear regression. For simple linear regression we have enough information in the body of the coefficients table to make a judgment about whether the linear model \\(y\\sim x\\) is sufficient to explain a team’s wins. That judgment depends on whether we are being casual, in which case the model is sufficient, or whether we have money riding on it, in which case the model is just barely insufficient.\n\n\n9.2.4 Assumptions\nBear in mind that we make four big assumptions in using this model at all. The assumptions are mentioned in the book as follows.\n\nLinearity: the data show a linear trend, identified by a scatterplot for instance\nNormally distributed residuals: identified by a Q-Q plot, to be described later\nConstant variability: \\(x\\) does not vary more or less depending on \\(y\\)\nIndependent observations: there is not a pattern like seasonality or growth or decline in the underlying phenomenon being analyzed (special statistics tools are used for that)\n\nMost textbooks use more technical terms for these concepts, especially homoscedasticity for constant variability and heteroscedasticity for non-constant variability. This book just doesn’t want to introduce too much terminology.\nThe most common assumptions violated in my experience are the first and third. There is often a curvilinear pattern in data that is not captured by a linear model. Also, graphs of data often exhibit a pattern like the cross-section of a horn, which is non-constant variability or heteroscedasticity.\n\n\n9.2.5 The Multiple Coefficient of Determination\n\\(R^2\\) is the most common measure of the strength of a linear relationship, partly because it varies between 0 and 1. It is the proportion of variability in the data explained by the model. It is very domain dependent. For a lot of cases, anything below 0.8 indicates a poor fit. On the other hand, there are areas of physics where 0.1 explains enough of the data to be valuable. You have to consider the domain when evaluating \\(R^2\\).\n\n\n9.2.6 Categorical Variables\nWith linear regression, the \\(y\\) variable must NEVER be categorical. If you try to do regression in R with the \\(y\\) variable as categorical, you’ll get an error message. There is another procedure you can do, called logistic regression, which has a categorical \\(y\\). We’ll discuss that later. But for now, bear in mind when you form a model, the outcome is always a continuously valued variable.\nOn the other hand, any or all of the input variables may be categorical. Note the book’s example of Mario Kart sales. The categorical variable condition has two levels or categories, whether the game is new or used. The outcome variable is the price, which is of course a continuous variable. When you input a categorical variable in R, it automatically encodes it as a number. In the Mario Kart case, the numbers are zero and one. The condition new is shown in the same place on the R output as \\(b_1\\) and the condition used is shown as the intercept. The main idea to understand here is the difference between the two, where the used condition is zero and the new condition is the difference between the price of new and used.\n\n\n9.2.7 Outliers\nBack in the nineteen seventies, several prominent statisticians said that statistics needed visualization. Their influece is actually responsible for my teaching visualization in this course. One of them, Francis Anscombe, published a data frame that showed how misleading elementary statistics can be without visualization. This data frame has come to be called Anscombe’s quartet and it is often shown to students. Here it is.\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n\n\nEach \\(x,y\\) pair of this quartet has the same basic statistics and the exact same least squares line. But look at a visualization of them.\n\n\n\n\n\nThe power of outliers can be seen in datasets 3 and 4 and the power of a nonlinear relationship can be seen in dataset 2."
  },
  {
    "objectID": "week09.html#inference-for-linear-regression",
    "href": "week09.html#inference-for-linear-regression",
    "title": "9  Introduction to Linear Regression",
    "section": "9.3 Inference for linear regression",
    "text": "9.3 Inference for linear regression\n\n9.3.1 Confidence intervals\nThe textbook gives formulas for computing confidence intervals. Another way to do so is to use software, such as R. You can use the confint() function to find confidence intervals for coefficients of a linear model. For example, consider our payroll / wins example above.\n\nconfint(lm(y~x))\n\n                  2.5 %    97.5 %\n(Intercept) -2.93279043 18.732790\nx           -0.01324184  1.293242"
  },
  {
    "objectID": "week09.html#multiple-regression-intro-chapter-9",
    "href": "week09.html#multiple-regression-intro-chapter-9",
    "title": "9  Introduction to Linear Regression",
    "section": "9.4 Multiple regression intro (Chapter 9)",
    "text": "9.4 Multiple regression intro (Chapter 9)\nEverything we’ve done so far has assumed that we know one piece of information’s relationship to another piece of information. Take the example of teams, where we knew the payroll and want to know the number of wins. Suppose we also knew a number of other statistics that might affect wins. How would we incorporate them? The answer is simple. We add them. Because we’re using a linear equation, that is, the equation of a line to model the data, there’s no reason we can’t add terms to the equation. These terms are additive, meaning that we add each term and each term has a coefficient. So now, our estimate of \\(y\\), which we call \\(\\hat{y}\\), looks like this for \\(n\\) terms.\n\\[\\hat{y}=b_0+b_1x_1+b_2x_2+\\cdots+b_nx_n\\]\nIn R, we simply add the column names. For example, consider the built-in data frame mtcars where the outcome variable is mpg. We can construct a model of the relationship between mpg and two input variables we suspect of influencing mpg as follows.\n\nwith(mtcars,summary(lm(mpg ~ hp+wt)))\n\n\nCall:\nlm(formula = mpg ~ hp + wt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\nThe output looks a bit different now. First, there are 32 residuals, so the individual residuals are not listed. Instead, you see summary statistics for the residuals.\nNext, look at the coefficients table. There are three rows now, for the intercept, for hp, and for wt. Notice that all three have significance codes at the end of the row. Normally, you shouldn’t be concerned about the significance code for the intercept, but the other two are interesting. The code for hp is two stars, meaning that it is less than 0.01, while the code for wt is 1.12e-06, which is abbreviated scientific notation, meaning to take 1.12 and shift the decimal point six places to the left, giving 0.00000112 as the decimal equivalent.\nThe Multiple R-squared is 82 percent and the Adjusted R-squared is 81 percent. This is a good sign because the Adjusted R-squared is adjusted for the case where you have included too many variables on the right hand side of the linear model formula. If it’s similar to Multiple R-squared, that means you probably have not included too many variables.\nThe \\(F\\)-statistic is important now, because of its interpretation. The \\(F\\)-statistic tells you that at least one of the variables is significant, taken in combination with the others. The \\(t\\)-statistics only give the individual contribution of the variables, so it’s possible to have a significant \\(t\\)-statistic without a significant \\(F\\)-statistic. The first thing to check in regression output is the \\(F\\)-statistic. If it’s too small, i.e., has a large \\(p\\)-value, try a different model.\nYou might think that including more variables results in a strictly better model. This is not true for reasons to be explored later. For now, try including all the variables in the data frame by the shorthand of a dot on the right hand side of the formula.\n\nsummary(lm(mpg ~ ., data=mtcars))\n\n\nCall:\nlm(formula = mpg ~ ., data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4506 -1.6044 -0.1196  1.2193  4.6271 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 12.30337   18.71788   0.657   0.5181  \ncyl         -0.11144    1.04502  -0.107   0.9161  \ndisp         0.01334    0.01786   0.747   0.4635  \nhp          -0.02148    0.02177  -0.987   0.3350  \ndrat         0.78711    1.63537   0.481   0.6353  \nwt          -3.71530    1.89441  -1.961   0.0633 .\nqsec         0.82104    0.73084   1.123   0.2739  \nvs           0.31776    2.10451   0.151   0.8814  \nam           2.52023    2.05665   1.225   0.2340  \ngear         0.65541    1.49326   0.439   0.6652  \ncarb        -0.19942    0.82875  -0.241   0.8122  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.65 on 21 degrees of freedom\nMultiple R-squared:  0.869, Adjusted R-squared:  0.8066 \nF-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07\n\n\nYou might find this output a bit surprising. You know from the \\(F\\)-statistic that at least one of the variables is contributing significantly to the model but individually, the contributions seem minimal based on the small \\(t\\)-statistics. The model is only a bit better, explaining 86 percent of the variability in the data, and the adjusted \\(R^2\\) value hasn’t improved at all, suggesting that you may have too many variables.\nAt this stage, you would probably remove some variables, perhaps by trial and error. How would you do this? You could start by running linear models over and over again. For example, you could construct one linear model for each variable and see which one has the largest contribution. Then you could try adding a second variable from among the remaining variables, and do that with each remaining variable, until you find one that adds the Largest contribution. You could continue in this way until you’ve accounted for all the variables, but would take forever to do. Luckily, R has functions to assist with this process and run regressions for you over and over again. I’m going to demonstrate one of them now for which we have to add the leaps package. I should point out that this involves doing some machine learning which is not strictly in the scope of this class, but will save you a lot of time.\n\nlibrary(caret)\nlibrary(leaps)\nset.seed(123)\ntrain.control &lt;- trainControl(method = \"cv\", number = 10)\nm &lt;- train(mpg ~ ., data = mtcars,\n                    method = \"leapBackward\",\n                    tuneGrid = data.frame(nvmax = 1:10),\n                    trControl = train.control\n                    )\nm$results\n\n   nvmax     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1      1 3.528852 0.8077208 3.015705 1.765926  0.2320177 1.529370\n2      2 3.104015 0.8306301 2.507496 1.355870  0.2108183 0.884356\n3      3 3.211552 0.8255871 2.700867 1.359334  0.2077318 1.033360\n4      4 3.148479 0.8296845 2.630645 1.354017  0.1908016 1.074414\n5      5 3.254928 0.8164973 2.737739 1.266874  0.2309531 1.044970\n6      6 3.259540 0.8212797 2.749594 1.227337  0.2493678 1.043727\n7      7 3.322310 0.8570599 2.787698 1.408879  0.1592820 1.153164\n8      8 3.297613 0.8666992 2.744000 1.364396  0.1529011 1.114000\n9      9 3.330123 0.8632282 2.751539 1.385199  0.1600841 1.111120\n10    10 3.286242 0.8588116 2.715828 1.362054  0.1739366 1.120088\n\nm$bestTune[,1]\n\n[1] 2\n\nsummary(m$finalModel)\n\nSubset selection object\n10 Variables  (and intercept)\n     Forced in Forced out\ncyl      FALSE      FALSE\ndisp     FALSE      FALSE\nhp       FALSE      FALSE\ndrat     FALSE      FALSE\nwt       FALSE      FALSE\nqsec     FALSE      FALSE\nvs       FALSE      FALSE\nam       FALSE      FALSE\ngear     FALSE      FALSE\ncarb     FALSE      FALSE\n1 subsets of each size up to 2\nSelection Algorithm: backward\n         cyl disp hp  drat wt  qsec vs  am  gear carb\n1  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n2  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \"*\"  \" \" \" \" \" \"  \" \" \n\ncoef(m$finalModel,m$bestTune[,1])\n\n(Intercept)          wt        qsec \n  19.746223   -5.047982    0.929198 \n\nsummary(lm(mpg~wt+qsec,data=mtcars))\n\n\nCall:\nlm(formula = mpg ~ wt + qsec, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3962 -2.1431 -0.2129  1.4915  5.7486 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  19.7462     5.2521   3.760 0.000765 ***\nwt           -5.0480     0.4840 -10.430 2.52e-11 ***\nqsec          0.9292     0.2650   3.506 0.001500 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.596 on 29 degrees of freedom\nMultiple R-squared:  0.8264,    Adjusted R-squared:  0.8144 \nF-statistic: 69.03 on 2 and 29 DF,  p-value: 9.395e-12\n\n\nThe preceding code uses a process of backward selection of models and arrives at a best model with two variables. Backward selection starts with all the variables and gradually removes the worst one at each iteration.\nThe following code uses a process of sequential selection, which combines both forward and backward. It takes longer to run, but can result in a better model. In this case, it chooses four variables.\n\nm &lt;- train(mpg ~ ., data = mtcars,\n                    method = \"leapSeq\",\n                    tuneGrid = data.frame(nvmax = 1:10),\n                    trControl = train.control\n                    )\nm$results\n\n   nvmax     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD\n1      1 3.338459 0.9001241 2.890096 1.0951033  0.1013687 0.9516300\n2      2 3.189923 0.8859776 2.582903 0.6838624  0.1267838 0.4331819\n3      3 2.941144 0.8620702 2.488212 0.9202376  0.1449517 0.6399695\n4      4 2.879207 0.8617366 2.480590 1.0315877  0.1534301 0.8871564\n5      5 3.132200 0.8965810 2.747981 1.0959380  0.1336679 0.9911532\n6      6 3.010670 0.9150866 2.656875 1.0023198  0.1083813 0.8800647\n7      7 2.919346 0.9260098 2.499461 0.8913467  0.1133718 0.7935931\n8      8 2.985337 0.9085432 2.585924 0.9516997  0.1248117 0.8376843\n9      9 3.022897 0.9194308 2.609043 1.1395931  0.1111588 1.0913428\n10    10 3.257194 0.8988626 2.811998 1.3089386  0.1423077 1.2115681\n\nm$bestTune[,1]\n\n[1] 4\n\nsummary(m$finalModel)\n\nSubset selection object\n10 Variables  (and intercept)\n     Forced in Forced out\ncyl      FALSE      FALSE\ndisp     FALSE      FALSE\nhp       FALSE      FALSE\ndrat     FALSE      FALSE\nwt       FALSE      FALSE\nqsec     FALSE      FALSE\nvs       FALSE      FALSE\nam       FALSE      FALSE\ngear     FALSE      FALSE\ncarb     FALSE      FALSE\n1 subsets of each size up to 4\nSelection Algorithm: 'sequential replacement'\n         cyl disp hp  drat wt  qsec vs  am  gear carb\n1  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n2  ( 1 ) \"*\" \"*\"  \" \" \" \"  \" \" \" \"  \" \" \" \" \" \"  \" \" \n3  ( 1 ) \"*\" \" \"  \"*\" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n4  ( 1 ) \" \" \" \"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n\ncoef(m$finalModel,m$bestTune[,1])\n\n(Intercept)          hp          wt        qsec          am \n17.44019110 -0.01764654 -3.23809682  0.81060254  2.92550394 \n\nsummary(lm(mpg~hp+wt+qsec+am,data=mtcars))\n\n\nCall:\nlm(formula = mpg ~ hp + wt + qsec + am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4975 -1.5902 -0.1122  1.1795  4.5404 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 17.44019    9.31887   1.871  0.07215 . \nhp          -0.01765    0.01415  -1.247  0.22309   \nwt          -3.23810    0.88990  -3.639  0.00114 **\nqsec         0.81060    0.43887   1.847  0.07573 . \nam           2.92550    1.39715   2.094  0.04579 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.435 on 27 degrees of freedom\nMultiple R-squared:  0.8579,    Adjusted R-squared:  0.8368 \nF-statistic: 40.74 on 4 and 27 DF,  p-value: 4.589e-11\n\n\nWhich model is better? The latter model has the best adjusted \\(R^2\\) value. But it also has what appears to be a spurious variable, hp. It could be that hp is contributing indirectly, by being collinear with one of the other variables. Should we take it out and try again or should we accept the two variable model? That depends on several factors.\nThere is a principle called Occam’s Razor, named after William of Occam (who didn’t invent it, by the way—things often get named after popularizers rather than inventors). The principle states that, if two explanations have the same explanatory power, you should accept the simpler one. In this context, simpler means fewer variables. The tricky part is what is meant by the same explanatory power. Here we have a comparison of 0.8368 adjusted \\(R^2\\) vs 0.8144. Are those close enough to be considered the same? It depends on the context. If you’re a car buyer I would say yes but if you’re a car manufacturer I would say no. Your opinion might differ. It’s easy to teach the mechanics of these methods (even if you don’t think so yet!) but much harder to come up with the insights to interpret them. (Actually, I would probably choose the three variable model of wt, qsec, and am, but you can test that for yourself.)\n\n\n\n\nDiez, David, Mine Çetinkaya-Rundel, and Cristopher D Barr. 2019. OpenIntro Statistics, Fourth Edition. self-published. https://openintro.org/os."
  },
  {
    "objectID": "week10.html#recap-week-9-linear-regression",
    "href": "week10.html#recap-week-9-linear-regression",
    "title": "10  More Linear Regression; Logistic Regression",
    "section": "10.1 Recap week 9: Linear Regression",
    "text": "10.1 Recap week 9: Linear Regression\n\nTextbook section 8.2 Linear Regression\nTextbook section 8.4 Inference for Linear Regression\nTextbook section 9.1 Multiple Regression"
  },
  {
    "objectID": "week10.html#more-on-multiple-regression",
    "href": "week10.html#more-on-multiple-regression",
    "title": "10  More Linear Regression; Logistic Regression",
    "section": "10.2 More on Multiple regression",
    "text": "10.2 More on Multiple regression\nThe OpenIntro Stats book gives an example of multiple regression with the mariokart data frame from their website. This involves the sale of 143 copies of the game Mario Kart for the Wii platform on eBay. They first predict the price based on most of the variables, like so.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/mariokart.rda\"))\nm&lt;-(lm(total_pr~cond+stock_photo+duration+wheels,data=mariokart))\nsummary(m)\n\n\nCall:\nlm(formula = total_pr ~ cond + stock_photo + duration + wheels, \n    data = mariokart)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.485  -6.511  -2.530   1.836 263.025 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     43.5201     8.3701   5.199 7.05e-07 ***\ncondused        -2.5816     5.2272  -0.494 0.622183    \nstock_photoyes  -6.7542     5.1729  -1.306 0.193836    \nduration         0.3788     0.9388   0.403 0.687206    \nwheels           9.9476     2.7184   3.659 0.000359 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.4 on 138 degrees of freedom\nMultiple R-squared:  0.1235,    Adjusted R-squared:  0.09808 \nF-statistic:  4.86 on 4 and 138 DF,  p-value: 0.001069\n\nplot(m)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are four diagnostic plots in the above output. Each one gives us information about the quality of the model.\n\n10.2.1 Residuals vs Fitted\nThis plot tells you the magnitude of the difference between the residuals and the fitted values. There are three things to watch for here. First, are there any drastic outliers? Yes, there are two, points 65 and 20. (Those are row numbers in the data frame.) You need to investigate those and decide whether to omit them from further analysis. Were they typos? Mismeasurements? Or is the process from which they derive intrinsically subject to occasional extreme variation. In the third case, you probably don’t want to omit them.\nSecond, is the solid red line near the dashed zero line? Yes it is, indicating that the residuals have a mean of approximately zero. (The red line shows the mean of the residuals in the immediate region of the \\(x\\)-values of the observed data.)\nThird, is there a pattern to the residuals? No, there is not. The residuals appear to be of the same general magnitude at one end as the other. The things that would need action would be a curve or multiple curves, or a widening or narrowing shape, like the cross section of a horn.\n\n\n10.2.2 Normal Q-Q\nThis is an important plot. I see many students erroneously claiming that residuals are normally distributed because they have a vague bell shape. That is not good enough to detect normality. The Q-Q plot is the standard way to detect normality. If the points lie along the dashed line, you can be reasonably safe in an assumption of normality. If they deviate from the dashed line, the residuals are probably not normally distributed.\n\n\n10.2.3 Scale-Location\nLook for two things here. First, the red line should be approximately horizontal, meaning that there is not much variability in the standardized residuals. Second, look at the spread of the points around the red line. If they don’t show a pattrn, this reinforces the assumption of homoscedasticity that we already found evidence for in the first plot.\n\n\n10.2.4 Residuals vs Leverage\nThis shows you influential points that you may want to remove. Point 84 has high leverage (potential for influence) but is probably not actually very influential because it is so far from Cook’s Distance. Points 20 and 65 are outliers but only point 20 is more than Cook’s Distance away from the mean. In this case, you would likely remove point 20 from consideration unless there were a mitigating reason. For example, game collectors often pay extra for a game that has unusual attributes, such as shrink-wrapped original edition. As an example of a point you would definitely remove, draw a horizontal line from point 20 to a vertical line from point 84. Where they meet would be a high-leverage outlier that is unduly affecting the model no matter what it’s underlying cause. On the other hand, what if you have many such points? Unfortunately, that probably means the model isn’t very good.\n\n\n10.2.5 Removing offending observations\nSuppose we want to get rid of points 20 and 65 and rerun the regression. We could either do this using plain R or the tidyverse. I prefer the tidyverse method because of clarity of exposition.\n\ndf &lt;- mariokart |&gt;\n  filter(!row_number() %in% c(20, 65))\nm&lt;-(lm(total_pr~cond+stock_photo+duration+wheels,data=df))\nsummary(m)\n\n\nCall:\nlm(formula = total_pr ~ cond + stock_photo + duration + wheels, \n    data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.3788  -2.9854  -0.9654   2.6915  14.0346 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    41.34153    1.71167  24.153  &lt; 2e-16 ***\ncondused       -5.13056    1.05112  -4.881 2.91e-06 ***\nstock_photoyes  1.08031    1.05682   1.022    0.308    \nduration       -0.02681    0.19041  -0.141    0.888    \nwheels          7.28518    0.55469  13.134  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.901 on 136 degrees of freedom\nMultiple R-squared:  0.719, Adjusted R-squared:  0.7108 \nF-statistic: 87.01 on 4 and 136 DF,  p-value: &lt; 2.2e-16\n\nplot(m)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat a difference this makes in the output and the statistics and plots about the output! Keep in mind, though, that I just did this as an example. Points 20 and 65 may be totally legitimate in this case. Also, note that you could use plain R without the tidyverse to eliminate those rows by saying something like df &lt;- mariokart[-c(20,65),]. The bracket notation assumes anything before the comma refers to a row and anything after a comma refers to a column. In this case, I didn’t say anything about the columns, so the square brackets just have a dangling comma in them. The important point is that one method or another may seem more natural to you. For most students, the tidyverse approach is probably more natural, so I highlight that."
  },
  {
    "objectID": "week10.html#logistic-regression",
    "href": "week10.html#logistic-regression",
    "title": "10  More Linear Regression; Logistic Regression",
    "section": "10.3 Logistic Regression",
    "text": "10.3 Logistic Regression\nLogistic regression is a kind classification rather than regression. The book doesn’t make this point, but most textbooks do. You can divide machine learning problems into problems of regression and problems of classification. In regression, the \\(y\\) variable is more or less continuous, whereas in the classification problem, \\(y\\) is a set of categories, ordered or not. The word logistic comes from the logistic function, which is illustrated below. This interesting function takes an input from \\(-\\infty\\) to \\(+\\infty\\) and gives an output between zero and one. It can be used to reduce wildly varying inputs into a yes / no decision. It is also known as the sigmoid function.\n\n\n\n\n\nNote that zero and one happen to be the boundaries of a probability measure. Hence, you can use the logistic function to reduce arbitrary numbers to a probability.\n\nload(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/resume.rda\"))\nnames(resume)\n\n [1] \"job_ad_id\"              \"job_city\"               \"job_industry\"          \n [4] \"job_type\"               \"job_fed_contractor\"     \"job_equal_opp_employer\"\n [7] \"job_ownership\"          \"job_req_any\"            \"job_req_communication\" \n[10] \"job_req_education\"      \"job_req_min_experience\" \"job_req_computer\"      \n[13] \"job_req_organization\"   \"job_req_school\"         \"received_callback\"     \n[16] \"firstname\"              \"race\"                   \"gender\"                \n[19] \"years_college\"          \"college_degree\"         \"honors\"                \n[22] \"worked_during_school\"   \"years_experience\"       \"computer_skills\"       \n[25] \"special_skills\"         \"volunteer\"              \"military\"              \n[28] \"employment_holes\"       \"has_email_address\"      \"resume_quality\"        \n\nwith(resume,table(race,received_callback))\n\n       received_callback\nrace       0    1\n  black 2278  157\n  white 2200  235\n\nwith(resume,table(gender,received_callback))\n\n      received_callback\ngender    0    1\n     f 3437  309\n     m 1041   83\n\nwith(resume,table(honors,received_callback))\n\n      received_callback\nhonors    0    1\n     0 4263  350\n     1  215   42\n\nsummary(glm(received_callback ~ honors,data=resume,family=\"binomial\"))\n\n\nCall:\nglm(formula = received_callback ~ honors, family = \"binomial\", \n    data = resume)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -2.4998     0.0556  -44.96  &lt; 2e-16 ***\nhonors        0.8668     0.1776    4.88 1.06e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2726.9  on 4869  degrees of freedom\nResidual deviance: 2706.7  on 4868  degrees of freedom\nAIC: 2710.7\n\nNumber of Fisher Scoring iterations: 5\n\nsummary(glm(received_callback ~ race,data=resume,family=\"binomial\"))\n\n\nCall:\nglm(formula = received_callback ~ race, family = \"binomial\", \n    data = resume)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.67481    0.08251 -32.417  &lt; 2e-16 ***\nracewhite    0.43818    0.10732   4.083 4.45e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2726.9  on 4869  degrees of freedom\nResidual deviance: 2709.9  on 4868  degrees of freedom\nAIC: 2713.9\n\nNumber of Fisher Scoring iterations: 5\n\nsummary(glm(received_callback ~ gender,data=resume,family=\"binomial\"))\n\n\nCall:\nglm(formula = received_callback ~ gender, family = \"binomial\", \n    data = resume)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.40901    0.05939 -40.562   &lt;2e-16 ***\ngenderm     -0.12008    0.12859  -0.934     0.35    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2726.9  on 4869  degrees of freedom\nResidual deviance: 2726.0  on 4868  degrees of freedom\nAIC: 2730\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n10.3.1 tidymodels approach\nDatacamp shows a different way, using tidymodels in one of their tutorials. In this example, the bank wants to divide customers into those likely to buy and those unlikely to buy some banking product. They would like to divide the customers into these two groups using logistic regression, with a cutoff point of fifty-fifty. If there’s better than a fifty-fifty chance, they will send a salesperson but if there’s less than a fifty-fifty chance, they won’t send a salesperson.\n\nlibrary(tidymodels)\n\n#. Read the dataset and convert the target variable to a factor\nbank_df &lt;- read_csv2(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/bank-full.csv\"))\nbank_df$y = as.factor(bank_df$y)\n\n#. Plot job occupation against the target variable\nggplot(bank_df, aes(job, fill = y)) +\n    geom_bar() +\n    coord_flip()\n\n\n\n\nA crucial concept you’ll learn if you take a more advanced class, say 310D, is the notion of dividing data into two data frames, a training frame and a test frame. This is the conventional way to test machine learning models, of which logistic regression is one. You train the model on one set of data, then test it on another, previously unseen set. That’s the next thing done in this example.\n\n#. Split data into train and test\nset.seed(421)\nsplit &lt;- initial_split(bank_df, prop = 0.8, strata = y)\ntrain &lt;- split |&gt; \n         training()\ntest &lt;- split |&gt;\n        testing()\n#. Train a logistic regression model\nm &lt;- logistic_reg(mixture = double(1), penalty = double(1)) |&gt;\n  set_engine(\"glmnet\") |&gt;\n  set_mode(\"classification\") |&gt;\n  fit(y ~ ., data = train)\n\n#. Model summary\ntidy(m)\n\n# A tibble: 43 × 3\n   term              estimate penalty\n   &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)      -2.59           0\n 2 age              -0.000477       0\n 3 jobblue-collar   -0.183          0\n 4 jobentrepreneur  -0.206          0\n 5 jobhousemaid     -0.270          0\n 6 jobmanagement    -0.0190         0\n 7 jobretired        0.360          0\n 8 jobself-employed -0.101          0\n 9 jobservices      -0.105          0\n10 jobstudent        0.415          0\n# ℹ 33 more rows\n\n#. Class Predictions\npred_class &lt;- predict(m,\n                      new_data = test,\n                      type = \"class\")\n\n#. Class Probabilities\npred_proba &lt;- predict(m,\n                      new_data = test,\n                      type = \"prob\")\nresults &lt;- test |&gt;\n           select(y) |&gt;\n           bind_cols(pred_class, pred_proba)\n\naccuracy(results, truth = y, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.902\n\n\n\n\n10.3.2 Hyperparameter tuning\nThere are aspects of this approach, called hyperparameters, that influence the quality of the model. It can be tedious to adjust these aspects, called penalty and mixture, so here’s a technique for doing it automatically. You’ll learn about this and similar techniques if you take a more advanced course like 310D, Intro to Data Science.\n\n#. Define the logistic regression model with penalty and mixture hyperparameters\nlog_reg &lt;- logistic_reg(mixture = tune(), penalty = tune(), engine = \"glmnet\")\n\n#. Define the grid search for the hyperparameters\ngrid &lt;- grid_regular(mixture(), penalty(), levels = c(mixture = 4, penalty = 3))\n\n#. Define the workflow for the model\nlog_reg_wf &lt;- workflow() |&gt;\n  add_model(log_reg) |&gt;\n  add_formula(y ~ .)\n\n#. Define the resampling method for the grid search\nfolds &lt;- vfold_cv(train, v = 5)\n\n#. Tune the hyperparameters using the grid search\nlog_reg_tuned &lt;- tune_grid(\n  log_reg_wf,\n  resamples = folds,\n  grid = grid,\n  control = control_grid(save_pred = TRUE)\n)\n\nselect_best(log_reg_tuned, metric = \"roc_auc\")\n\n# A tibble: 1 × 3\n       penalty mixture .config              \n         &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.0000000001       0 Preprocessor1_Model01\n\n\n\n#. Fit the model using the optimal hyperparameters\nlog_reg_final &lt;- logistic_reg(penalty = 0.0000000001, mixture = 0) |&gt;\n                 set_engine(\"glmnet\") |&gt;\n                 set_mode(\"classification\") |&gt;\n                 fit(y~., data = train)\n\n#. Evaluate the model performance on the testing set\npred_class &lt;- predict(log_reg_final,\n                      new_data = test,\n                      type = \"class\")\nresults &lt;- test |&gt;\n  select(y) |&gt;\n  bind_cols(pred_class, pred_proba)\n\n#. Create confusion matrix\nconf_mat(results, truth = y,\n         estimate = .pred_class)\n\n          Truth\nPrediction   no  yes\n       no  7838  738\n       yes  147  320\n\nprecision(results, truth = y,\n          estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 precision binary         0.914\n\nrecall(results, truth = y,\n          estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 recall  binary         0.982\n\n\n\n\n10.3.3 Evaluation metrics\nFollowing are two tables from James et al. (2021) that you can use to evaluate a classification model.\n\n\nAnother view is provided at Wikipedia in the following picture\n\n\n\n\n\n\ncoeff &lt;- tidy(log_reg_final) |&gt;\n  arrange(desc(abs(estimate))) |&gt;\n  filter(abs(estimate) &gt; 0.5)\ncoeff\n\n# A tibble: 10 × 3\n   term            estimate      penalty\n   &lt;chr&gt;              &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)       -2.59  0.0000000001\n 2 poutcomesuccess    2.08  0.0000000001\n 3 monthmar           1.62  0.0000000001\n 4 monthoct           1.08  0.0000000001\n 5 monthsep           1.03  0.0000000001\n 6 contactunknown    -1.01  0.0000000001\n 7 monthdec           0.861 0.0000000001\n 8 monthjan          -0.820 0.0000000001\n 9 housingyes        -0.550 0.0000000001\n10 monthnov          -0.517 0.0000000001\n\nggplot(coeff, aes(x = term, y = estimate, fill = term)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning, 2nd Edition. Springer New York."
  },
  {
    "objectID": "week11.html#recap-week-10-multiple-regression-logistic-regression",
    "href": "week11.html#recap-week-10-multiple-regression-logistic-regression",
    "title": "11  Coping with Time and Joins",
    "section": "11.1 Recap week 10: Multiple Regression; Logistic Regression",
    "text": "11.1 Recap week 10: Multiple Regression; Logistic Regression\n\nMultiple regression: one \\(y\\) and multiple \\(x\\) variables\nLogistic regression: \\(y\\) is a factor and multiple \\(x\\) variables"
  },
  {
    "objectID": "week11.html#milestones",
    "href": "week11.html#milestones",
    "title": "11  Coping with Time and Joins",
    "section": "11.2 Milestones",
    "text": "11.2 Milestones\nMilestone 4 will be graded partly on the diagnostic plots and their explanations and partly on improvements to the rest of the report.\n\n11.2.1 Tips for Milestone 4\n\nDon’t use fread() or data.table any more\nInstead use read_csv (not read.csv)\nLabel the r chunks\nBreak your file up into smaller files to prepare, then assemble\nMake plots look better\n\nInclude a title\nDon’t use scientific notation\ndon’t overprint labels\n\n\n\n\n11.2.2 More tips\n\nDon’t produce long outputs that a manager would be unable to use\n\nExample: a list of 50 states with some statistic about them\n\nDon’t produce barplots where all bars are roughly the same size\nDon’t produce stem-and-leaf plots with so much output that no one can read them\nSort barplots for easier comparison\nIt’s hard to use tables with more than about 30 rows\n\n\n\n11.2.3 Why is this plot unsuccessful?\n\n\n\n11.2.4 Don’t include barplots with all same bars\n\n(just say there was little difference along this dimension)\n\n\n11.2.5 More tips\n\nDon’t include names of group members in sections of the report, only in the header\nPut titles on plots even if you have section titles as well\nUse small alpha values to reduce the impact of overplotting\n\n\n\n11.2.6 Significant overplotting\n\n\n\n11.2.7 Don’t use barplots with two bars for presentations\n\n(There may be exceptions in exploration but not presentation)\n\n\n11.2.8 Don’t use barplots with effectively one bar\n\n(Also don’t include the blank entries!)\n\n\n11.2.9 Unreadable stem-and-leaf plot\n\n\n\n11.2.10 This one has no meaning that I can see"
  },
  {
    "objectID": "week11.html#dates-and-times",
    "href": "week11.html#dates-and-times",
    "title": "11  Coping with Time and Joins",
    "section": "11.3 Dates and times",
    "text": "11.3 Dates and times\nFor the final exam, you will have to create the \\(y\\) variable as a time span. Chapter 18 of Wickham, Çetinkaya-Rundel, and Grolemund (2023) tells you how to do this. Let’s review that chapter.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\ntoday()\n\n[1] \"2023-09-30\"\n\nnow()\n\n[1] \"2023-09-30 18:05:49 CDT\"\n\n\nSuppose you have a comma-separated-values (csv) file containing ISO-formatted dates or date-times. It’s automatically recognized.\n\ncsv &lt;- \"\n  date,datetime\n  2022-01-02,2022-01-02 05:12\n\"\nread_csv(csv)\n\n# A tibble: 1 × 2\n  date       datetime           \n  &lt;date&gt;     &lt;dttm&gt;             \n1 2022-01-02 2022-01-02 05:12:00\n\n\nSuppose your input has dates not in standard format. You can do this for an ambiguous format:\n\ncsv &lt;- \"\n  date\n  01/02/15\n\"\n\nread_csv(csv, col_types = cols(date = col_date(\"%m/%d/%y\")))\n\n# A tibble: 1 × 1\n  date      \n  &lt;date&gt;    \n1 2015-01-02\n\nread_csv(csv, col_types = cols(date = col_date(\"%d/%m/%y\")))\n\n# A tibble: 1 × 1\n  date      \n  &lt;date&gt;    \n1 2015-02-01\n\nread_csv(csv, col_types = cols(date = col_date(\"%y/%m/%d\")))\n\n# A tibble: 1 × 1\n  date      \n  &lt;date&gt;    \n1 2001-02-15\n\n\nThe letters after the percent signs are format specifiers. Wickham, Çetinkaya-Rundel, and Grolemund (2023) has a long list of them in Chapter 18.\nAn alternative to the above approach is to use the helpers in the lubridate package, which is part of the tidyverse collection of packages. There are two kinds of helpers. First are the date helpers.\n\nymd(\"2017-01-31\")\n\n[1] \"2017-01-31\"\n\nmdy(\"January 31st, 2017\")\n\n[1] \"2017-01-31\"\n\ndmy(\"31-Jan-2017\")\n\n[1] \"2017-01-31\"\n\n\nSecond are the date-time helpers.\n\nymd_hms(\"2017-01-31 20:11:59\")\n\n[1] \"2017-01-31 20:11:59 UTC\"\n\nmdy_hm(\"01/31/2017 08:01\")\n\n[1] \"2017-01-31 08:01:00 UTC\"\n\n\nThe nycflights13 data frame, which we loaded above, contains information about 336,000 flights originating from the three NYC area airports in 2013. It contains dats and times spread across different columns.\n\nflights |&gt;\n  select(year, month, day, hour, minute)\n\n# A tibble: 336,776 × 5\n    year month   day  hour minute\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  2013     1     1     5     15\n 2  2013     1     1     5     29\n 3  2013     1     1     5     40\n 4  2013     1     1     5     45\n 5  2013     1     1     6      0\n 6  2013     1     1     5     58\n 7  2013     1     1     6      0\n 8  2013     1     1     6      0\n 9  2013     1     1     6      0\n10  2013     1     1     6      0\n# ℹ 336,766 more rows\n\n\nYou can handle this kind of input by using the make_datetime() function.\n\nflights |&gt;\n  select(year, month, day, hour, minute) |&gt;\n  mutate(departure = make_datetime(year, month, day, hour, minute))\n\n# A tibble: 336,776 × 6\n    year month   day  hour minute departure          \n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dttm&gt;             \n 1  2013     1     1     5     15 2013-01-01 05:15:00\n 2  2013     1     1     5     29 2013-01-01 05:29:00\n 3  2013     1     1     5     40 2013-01-01 05:40:00\n 4  2013     1     1     5     45 2013-01-01 05:45:00\n 5  2013     1     1     6      0 2013-01-01 06:00:00\n 6  2013     1     1     5     58 2013-01-01 05:58:00\n 7  2013     1     1     6      0 2013-01-01 06:00:00\n 8  2013     1     1     6      0 2013-01-01 06:00:00\n 9  2013     1     1     6      0 2013-01-01 06:00:00\n10  2013     1     1     6      0 2013-01-01 06:00:00\n# ℹ 336,766 more rows\n\n\nYou can do the same with the other time columns.\n\nmake_datetime_100 &lt;- function(year, month, day, time) {\n  make_datetime(year, month, day, time %/% 100, time %% 100)\n}\n\nflights_dt &lt;- flights |&gt;\n  filter(!is.na(dep_time), !is.na(arr_time)) |&gt;\n  mutate(\n    dep_time = make_datetime_100(year, month, day, dep_time),\n    arr_time = make_datetime_100(year, month, day, arr_time),\n    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),\n    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)\n  ) |&gt;\n  select(origin, dest, ends_with(\"delay\"), ends_with(\"time\"))\n\nflights_dt\n\n# A tibble: 328,063 × 9\n   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n 1 EWR    IAH           2        11 2013-01-01 05:17:00 2013-01-01 05:15:00\n 2 LGA    IAH           4        20 2013-01-01 05:33:00 2013-01-01 05:29:00\n 3 JFK    MIA           2        33 2013-01-01 05:42:00 2013-01-01 05:40:00\n 4 JFK    BQN          -1       -18 2013-01-01 05:44:00 2013-01-01 05:45:00\n 5 LGA    ATL          -6       -25 2013-01-01 05:54:00 2013-01-01 06:00:00\n 6 EWR    ORD          -4        12 2013-01-01 05:54:00 2013-01-01 05:58:00\n 7 EWR    FLL          -5        19 2013-01-01 05:55:00 2013-01-01 06:00:00\n 8 LGA    IAD          -3       -14 2013-01-01 05:57:00 2013-01-01 06:00:00\n 9 JFK    MCO          -3        -8 2013-01-01 05:57:00 2013-01-01 06:00:00\n10 LGA    ORD          -2         8 2013-01-01 05:58:00 2013-01-01 06:00:00\n# ℹ 328,053 more rows\n# ℹ 3 more variables: arr_time &lt;dttm&gt;, sched_arr_time &lt;dttm&gt;, air_time &lt;dbl&gt;\n\n\nHere are the departure times for January 2nd, 2013.\n\nflights_dt |&gt;\n  filter(dep_time &lt; ymd(20130102)) |&gt;\n  ggplot(aes(x = dep_time)) +\n  geom_freqpoly(binwidth = 600) # 600 s = 10 minutes"
  },
  {
    "objectID": "week11.html#time-spans",
    "href": "week11.html#time-spans",
    "title": "11  Coping with Time and Joins",
    "section": "11.4 Time spans",
    "text": "11.4 Time spans\n\nDurations, which represent an exact number of seconds.\nPeriods, which represent human units like weeks and months.\nIntervals, which represent a starting and ending point.\n\n\n11.4.1 Durations\nBase R provides a problematic construct for durations, the difftime object.\n\n#. How old is Hadley?\nh_age &lt;- today() - ymd(\"1979-10-14\")\nh_age\n\nTime difference of 16057 days\n\n\nThe lubridate package provides a construct called duration.\n\nas.duration(h_age)\n\n[1] \"1387324800s (~43.96 years)\"\n\n\nThere are numerous duration constructors.\n\ndseconds(15)\n\n[1] \"15s\"\n\ndminutes(10)\n\n[1] \"600s (~10 minutes)\"\n\ndhours(c(12, 24))\n\n[1] \"43200s (~12 hours)\" \"86400s (~1 days)\"  \n\nddays(0:5)\n\n[1] \"0s\"                \"86400s (~1 days)\"  \"172800s (~2 days)\"\n[4] \"259200s (~3 days)\" \"345600s (~4 days)\" \"432000s (~5 days)\"\n\ndweeks(3)\n\n[1] \"1814400s (~3 weeks)\"\n\ndyears(1)\n\n[1] \"31557600s (~1 years)\"\n\n\nYou can add and multiply durations.\n\n2 * dyears(1)\n\n[1] \"63115200s (~2 years)\"\n\ndyears(1) + dweeks(12) + dhours(15)\n\n[1] \"38869200s (~1.23 years)\"\n\n\nYou can add and subtract durations to and from days.\n\ntomorrow &lt;- today() + ddays(1)\nlast_year &lt;- today() - dyears(1)\n\nProblem! Add one day to this particular date as a duration, but this particular date only has 23 hours because of daylight savings time.\n\none_am &lt;- ymd_hms(\"2026-03-08 01:00:00\", tz = \"America/New_York\")\n\none_am\n\n[1] \"2026-03-08 01:00:00 EST\"\n\none_am + ddays(1)\n\n[1] \"2026-03-09 02:00:00 EDT\""
  },
  {
    "objectID": "week11.html#periods",
    "href": "week11.html#periods",
    "title": "11  Coping with Time and Joins",
    "section": "11.5 Periods",
    "text": "11.5 Periods\nThis construct gets over some problems with durations, which are always exact numbers of seconds and take into account time zones and daylight savings time and leap years.\nPeriods have constructors, too.\n\nhours(c(12, 24))\n\n[1] \"12H 0M 0S\" \"24H 0M 0S\"\n\ndays(7)\n\n[1] \"7d 0H 0M 0S\"\n\nmonths(1:6)\n\n[1] \"1m 0d 0H 0M 0S\" \"2m 0d 0H 0M 0S\" \"3m 0d 0H 0M 0S\" \"4m 0d 0H 0M 0S\"\n[5] \"5m 0d 0H 0M 0S\" \"6m 0d 0H 0M 0S\"\n\n\nYou can add and multiply periods.\n\n10 * (months(6) + days(1))\n\n[1] \"60m 10d 0H 0M 0S\"\n\ndays(50) + hours(25) + minutes(2)\n\n[1] \"50d 25H 2M 0S\"\n\n\nAdd them to dates and get the results you expect in the case of daylight savings time and leap years.\n\n#. A leap year\nymd(\"2024-01-01\") + dyears(1)\n\n[1] \"2024-12-31 06:00:00 UTC\"\n\nymd(\"2024-01-01\") + years(1)\n\n[1] \"2025-01-01\"\n\n#. Daylight Savings Time\none_am + ddays(1)\n\n[1] \"2026-03-09 02:00:00 EDT\"\n\none_am + days(1)\n\n[1] \"2026-03-09 01:00:00 EDT\"\n\n\nPeriods can fix the problem that some planes appear to arrive before they depart.\n\nflights_dt |&gt;\n  filter(arr_time &lt; dep_time)\n\n# A tibble: 10,633 × 9\n   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n 1 EWR    BQN           9        -4 2013-01-01 19:29:00 2013-01-01 19:20:00\n 2 JFK    DFW          59        NA 2013-01-01 19:39:00 2013-01-01 18:40:00\n 3 EWR    TPA          -2         9 2013-01-01 20:58:00 2013-01-01 21:00:00\n 4 EWR    SJU          -6       -12 2013-01-01 21:02:00 2013-01-01 21:08:00\n 5 EWR    SFO          11       -14 2013-01-01 21:08:00 2013-01-01 20:57:00\n 6 LGA    FLL         -10        -2 2013-01-01 21:20:00 2013-01-01 21:30:00\n 7 EWR    MCO          41        43 2013-01-01 21:21:00 2013-01-01 20:40:00\n 8 JFK    LAX          -7       -24 2013-01-01 21:28:00 2013-01-01 21:35:00\n 9 EWR    FLL          49        28 2013-01-01 21:34:00 2013-01-01 20:45:00\n10 EWR    FLL          -9       -14 2013-01-01 21:36:00 2013-01-01 21:45:00\n# ℹ 10,623 more rows\n# ℹ 3 more variables: arr_time &lt;dttm&gt;, sched_arr_time &lt;dttm&gt;, air_time &lt;dbl&gt;\n\n\nThese are overnight flights so fix the problem by adding a day.\n\nflights_dt &lt;- flights_dt |&gt;\n  mutate(\n    overnight = arr_time &lt; dep_time,\n    arr_time = arr_time + days(!overnight),\n    sched_arr_time = sched_arr_time + days(overnight)\n  )\n\n\n11.5.1 Intervals\nIntervals are like durations but with a specific starting point. They get around the problem that, for example, some years are longer than others, so that a year on average is 365.25 days. With an interval you can have a specific year of 365 days or a specific leap year of 366 days.\n\ny2023 &lt;- ymd(\"2023-01-01\") %--% ymd(\"2024-01-01\")\ny2024 &lt;- ymd(\"2024-01-01\") %--% ymd(\"2025-01-01\")\n\ny2023\n\n[1] 2023-01-01 UTC--2024-01-01 UTC\n\ny2024\n\n[1] 2024-01-01 UTC--2025-01-01 UTC\n\ny2023 / days(1)\n\n[1] 365\n\ny2024 / days(1)\n\n[1] 366\n\n\nThe book also provides extensive information about time zones but for the final exam you’ll only have one time zone, so that discussion is not strictly necessary for us."
  },
  {
    "objectID": "week11.html#joins",
    "href": "week11.html#joins",
    "title": "11  Coping with Time and Joins",
    "section": "11.6 Joins",
    "text": "11.6 Joins\nThe nycflights13 package provides five data frames that can be joined together.\n\nWhy would you store data this way? (Think about using the data over a long term and think about maintenance of the data.)\nYou can add the airline names to the flights by a left_join() function. It’s easier to see if you first limit the flights data frame to a few essential columns.\n\nflights2 &lt;- flights |&gt;\n  select(year, time_hour, origin, dest, tailnum, carrier)\nflights2 |&gt; left_join(airlines)\n\n# A tibble: 336,776 × 7\n    year time_hour           origin dest  tailnum carrier name                  \n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                 \n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      United Air Lines Inc. \n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      United Air Lines Inc. \n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      American Airlines Inc.\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      JetBlue Airways       \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Delta Air Lines Inc.  \n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      United Air Lines Inc. \n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      JetBlue Airways       \n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      ExpressJet Airlines I…\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      JetBlue Airways       \n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      American Airlines Inc.\n# ℹ 336,766 more rows\n\n\nThere are several different join functions described in Wickham, Çetinkaya-Rundel, and Grolemund (2023) in Chapter 20. You’ll only need the left join for this week’s exercises, but reading Chapter 20 is still a very good idea.\nYou should also read about sqldf, a package for running SQL statements on R data frames. Following is an example of its use.\n\nlibrary(sqldf)\nsqldf(\"SELECT carrier, COUNT(*)\n         FROM flights\n         GROUP BY carrier\n         ORDER BY 2 DESC;\")\n\n   carrier COUNT(*)\n1       UA    58665\n2       B6    54635\n3       EV    54173\n4       DL    48110\n5       AA    32729\n6       MQ    26397\n7       US    20536\n8       9E    18460\n9       WN    12275\n10      VX     5162\n11      FL     3260\n12      AS      714\n13      F9      685\n14      YV      601\n15      HA      342\n16      OO       32\n\nsqlFlightsWnames &lt;- sqldf(\"SELECT fl.carrier, name\n                             FROM flights fl\n                             LEFT join airlines ai\n                             ON fl.carrier=ai.carrier;\")\nsqldf(\"SELECT name, COUNT(*)\n         FROM sqlFlightsWnames\n         GROUP BY name\n         ORDER BY 2 DESC;\")\n\n                          name COUNT(*)\n1        United Air Lines Inc.    58665\n2              JetBlue Airways    54635\n3     ExpressJet Airlines Inc.    54173\n4         Delta Air Lines Inc.    48110\n5       American Airlines Inc.    32729\n6                    Envoy Air    26397\n7              US Airways Inc.    20536\n8            Endeavor Air Inc.    18460\n9       Southwest Airlines Co.    12275\n10              Virgin America     5162\n11 AirTran Airways Corporation     3260\n12        Alaska Airlines Inc.      714\n13      Frontier Airlines Inc.      685\n14          Mesa Airlines Inc.      601\n15      Hawaiian Airlines Inc.      342\n16       SkyWest Airlines Inc.       32\n\nsort(table(flights$carrier),decreasing=TRUE)\n\n\n   UA    B6    EV    DL    AA    MQ    US    9E    WN    VX    FL    AS    F9 \n58665 54635 54173 48110 32729 26397 20536 18460 12275  5162  3260   714   685 \n   YV    HA    OO \n  601   342    32 \n\nflightsWnames &lt;- flights |&gt; left_join(airlines)\nsort(table(flightsWnames$name),decreasing=TRUE)\n\n\n      United Air Lines Inc.             JetBlue Airways \n                      58665                       54635 \n   ExpressJet Airlines Inc.        Delta Air Lines Inc. \n                      54173                       48110 \n     American Airlines Inc.                   Envoy Air \n                      32729                       26397 \n            US Airways Inc.           Endeavor Air Inc. \n                      20536                       18460 \n     Southwest Airlines Co.              Virgin America \n                      12275                        5162 \nAirTran Airways Corporation        Alaska Airlines Inc. \n                       3260                         714 \n     Frontier Airlines Inc.          Mesa Airlines Inc. \n                        685                         601 \n     Hawaiian Airlines Inc.       SkyWest Airlines Inc. \n                        342                          32 \n\n\nThe only difference between the output of these two approaches is that the native R plus tidyverse version uses more horizontal space in the output because of its use of a variable that records how wide your display is. The SQL version is piped (by default) to SQLite3, which doesn’t know the width of your display and which returns a single column response. You can substitute other database engines for SQLite3, such as PostgreSQL and MySQL. SQLite3 is an extremely fast, tiny database engine which is useful for single-user applications. For example, most smartphone applications (including all Android and iOS) use SQLite3 to store information, making SQLite3 the world’s most popular database (by some measures). SQLite3 is also used by most web browsers to store information.\nRecently (like within the last month) it was brought to the attention of the faculty that students want to learn more SQL. That’s the reason I’ve added this mention of sqldf. In a later iteration of the course, I will require some use of SQL but I learned of the need for this too late to incorporate it this semester. So your study of sqldf this semester will be optional.\n\n\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd ed. O’Reilly Media, Inc. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "week12.html#recap-week-11-time-joins",
    "href": "week12.html#recap-week-11-time-joins",
    "title": "12  Milestone 4; Take-home Exam",
    "section": "12.1 Recap week 11: Time; Joins",
    "text": "12.1 Recap week 11: Time; Joins\n\nUse the lubridate package to do arithmetic with dates and times; you’ll need to create a variable in the final exam for a time period\nYou often need to join tables or data frames together in the workplace; two facilities for doing so are sqldf and the dplyr _join functions"
  },
  {
    "objectID": "week12.html#working-on-m4",
    "href": "week12.html#working-on-m4",
    "title": "12  Milestone 4; Take-home Exam",
    "section": "12.2 Working on m4",
    "text": "12.2 Working on m4\nFor this milestone, you need to do two things and you should work on them together with coordination. It’s easy to split tasks apart, but then the report doesn’t look coherent. You have to try to work as a team. That’s why I’m giving you class time to work together. The two things are the regression diagnostics and fixing the earlier problems.\n\n12.2.1 Some tips on m4\n\nName the files m4.qmd and m4.html\nWhen I download them, I will automatically strip off any -1 or similar things that Canvas has put on\nThe title in the report header should be “Final Report” and must include the names of all contributing team members\nIt will be graded on the full contents, not just the regression diagnostics\n\n\n\n12.2.2 Strategy\n\nI previously suggested that you append your initials to df to name data frames, but almost no one did that.\nI suggest you work together on a first code chunk that massages the original data frame into what you want, then have individual work on chunks where you give the modified data frames unique names that don’t overlap.\nDon’t keep reading the original file in over and over again during your report. This overwrites any previous modifications you made to it.\nDon’t leave the final report to one group member. You must all look it over.\n\n\n\n12.2.3 More tips\n\nIf you have a Mac or if you have WSL2 on Windows, you can have a terminal\nYou can use the terminal to find out whether you are overwriting code with other code\nsay grep -n &lt;- m4.qmd at the terminal prompt and the output will be the lines and line numbers of all the places you assign names. You can then sort this output by extending the previous command as follows\nsay grep -n '&lt;-' m4.qmd | sort -k 2 -t : at the terminal prompt to get the assignments sorted by variable. That makes it easy to see if you’ve assigned to the same variable multiple times."
  },
  {
    "objectID": "week12.html#a-word-about-todays-conference",
    "href": "week12.html#a-word-about-todays-conference",
    "title": "12  Milestone 4; Take-home Exam",
    "section": "12.3 A word about today’s conference",
    "text": "12.3 A word about today’s conference\n\nI attended a conference on ethical AI earlier today\nThere was some discussion of the recent open letter suggesting a pause on development\n\nSome signatures were bogus, some were by people such as Elon Musk who are probably privately urging their employees to do the opposite of what the letter says\n\nThere is a lot of room for data scientists and ux / ui designers to help with the ethical ai problem\nThe recent paper on ChatGPT4 gave no details, leading one participant to say OpenAI should be renamed ClosedAI\nUnder-resourced communities are under-measured—this is a critical problem\nAttribution of sources by generative AI may be a big problem\nUsing generative AI requires some skill—ChatGPT hallucinates names of court cases and academic publications that don’t exist"
  },
  {
    "objectID": "week13.html#how-many-prefer-this-over-that-tests-of-proportions",
    "href": "week13.html#how-many-prefer-this-over-that-tests-of-proportions",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.1 How many prefer this over that? (Tests of proportions)",
    "text": "13.1 How many prefer this over that? (Tests of proportions)\n\n13.1.1 How many prefer website A over B? (One sample test of proportions in two categories)\nSixty subjects were asked whether they preferred website A or B. Their answer and a subject ID were recorded. Read the data and describe it.\n\nprefsAB &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/prefsAB.csv\"))\ntail(prefsAB) # displays the last few rows of the data frame\n\n# A tibble: 6 × 2\n  Subject Pref \n    &lt;dbl&gt; &lt;chr&gt;\n1      55 A    \n2      56 B    \n3      57 A    \n4      58 B    \n5      59 B    \n6      60 A    \n\nprefsAB$Subject &lt;- factor(prefsAB$Subject) # convert to nominal factor\nprefsAB$Pref &lt;- factor(prefsAB$Pref) # convert to nominal factor\nsummary(prefsAB)\n\n    Subject   Pref  \n 1      : 1   A:14  \n 2      : 1   B:46  \n 3      : 1         \n 4      : 1         \n 5      : 1         \n 6      : 1         \n (Other):54         \n\nggplot(prefsAB,aes(Pref)) +\n  geom_bar(width=0.5,alpha=0.4,fill=\"lightskyblue1\") +\n  theme_tufte(base_size=7)\n\n\n\n\nIs the difference between preferences significant? A default \\(\\chi^2\\) test examines the proportions in two bins, expecting them to be equally apportioned.\nTo do the \\(\\chi^2\\) test, first crosstabulate the data with xtabs().\n\n#. Pearson chi-square test\nprfs &lt;- xtabs( ~ Pref, data=prefsAB)\nprfs # show counts\n\nPref\n A  B \n14 46 \n\nchisq.test(prfs)\n\n\n    Chi-squared test for given probabilities\n\ndata:  prfs\nX-squared = 17.067, df = 1, p-value = 3.609e-05\n\n\nWe don’t really need an exact binomial test yet because the \\(\\chi^2\\) test told us enough: that the difference is not likely due to chance. That was only because there are only two choices. If there were more than two, we’d need a binomial test for every pair if the \\(\\chi^2\\) test turned up a significant difference. This binomial test just foreshadows what we’ll need when we face three categories.\n\n#. binomial test\n#. binom.test(prfs,split.table=Inf)\nbinom.test(prfs)\n\n\n    Exact binomial test\n\ndata:  prfs\nnumber of successes = 14, number of trials = 60, p-value = 4.224e-05\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.1338373 0.3603828\nsample estimates:\nprobability of success \n             0.2333333 \n\n\n\n\n13.1.2 How many prefer website A, B, or C? (One sample test of proportions in three categories)\nFirst, read in and describe the data. Convert Subject to a factor because R reads any numerical data as, well, numeric, but we don’t want to treat it as such. R interprets any data with characters as a factor. We want Subject to be treated as a factor.\n\nprefsABC &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/prefsABC.csv\"))\nhead(prefsABC) # displays the first few rows of the data frame\n\n# A tibble: 6 × 2\n  Subject Pref \n    &lt;dbl&gt; &lt;chr&gt;\n1       1 C    \n2       2 C    \n3       3 B    \n4       4 C    \n5       5 C    \n6       6 B    \n\nprefsABC$Subject &lt;- factor(prefsABC$Subject)\nprefsABC$Pref &lt;- factor(prefsABC$Pref)\nsummary(prefsABC)\n\n    Subject   Pref  \n 1      : 1   A: 8  \n 2      : 1   B:21  \n 3      : 1   C:31  \n 4      : 1         \n 5      : 1         \n 6      : 1         \n (Other):54         \n\npar(pin=c(2.75,1.25),cex=0.5)\nggplot(prefsABC,aes(Pref))+\n  geom_bar(width=0.5,alpha=0.4,fill=\"lightskyblue1\")+\n  theme_tufte(base_size=7)\n\n\n\n\nYou can think of the three websites as representing three bins and the preferences as filling up those bins. Either each bin gets one third of the preferences or there is a discrepancy. The Pearson \\(\\chi^2\\) test functions as an omnibus test to tell whether there is any discrepancy in the proportions of the three bins.\n\nprfs &lt;- xtabs( ~ Pref, data=prefsABC)\nprfs # show counts\n\nPref\n A  B  C \n 8 21 31 \n\nchisq.test(prfs)\n\n\n    Chi-squared test for given probabilities\n\ndata:  prfs\nX-squared = 13.3, df = 2, p-value = 0.001294\n\n\nA multinomial test can test for other than an even distribution across bins. Here’s an example with a one third distribution in each bin.\n\nlibrary(XNomial)\nxmulti(prfs, c(1/3, 1/3, 1/3), statName=\"Prob\")\n\n\nP value (Prob) = 0.0008024\n\n\nNow we don’t know which pair(s) differed so it makes sense to conduct post hoc binomial tests with correction for multiple comparisons. The correction, made by p.adjust(), is because the more hypotheses we check, the higher the probability of a Type I error, a false positive. That is, the more hypotheses we test, the higher the probability that one will appear true by chance. Wikipedia has more detail in its “Multiple Comparisons Problem” article.\nHere, we test separately for whether each one has a third of the preferences.\n\naa &lt;- binom.test(sum(prefsABC$Pref == \"A\"),\n        nrow(prefsABC), p=1/3)\nbb &lt;- binom.test(sum(prefsABC$Pref == \"B\"),\n        nrow(prefsABC), p=1/3)\ncc &lt;- binom.test(sum(prefsABC$Pref == \"C\"),\n        nrow(prefsABC), p=1/3)\np.adjust(c(aa$p.value, bb$p.value, cc$p.value), method=\"holm\")\n\n[1] 0.001659954 0.785201685 0.007446980\n\n\nThe adjusted \\(p\\)-values tell us that A and C differ significantly from a third of the preferences.\n\n\n13.1.3 How many males vs females prefer website A over B? (Two-sample tests of proportions in two categories)\nRevisit our data file with 2 response categories, but now with sex (M/F).\n\nprefsABsex &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/prefsABsex.csv\"))\ntail(prefsABsex)\n\n# A tibble: 6 × 3\n  Subject Pref  Sex  \n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1      55 A     M    \n2      56 B     F    \n3      57 A     M    \n4      58 B     M    \n5      59 B     M    \n6      60 A     M    \n\nprefsABsex$Subject &lt;- factor(prefsABsex$Subject)\nprefsABsex$Pref &lt;- factor(prefsABsex$Pref)\nprefsABsex$Sex &lt;- factor(prefsABsex$Sex)\nsummary(prefsABsex)\n\n    Subject   Pref   Sex   \n 1      : 1   A:14   F:31  \n 2      : 1   B:46   M:29  \n 3      : 1                \n 4      : 1                \n 5      : 1                \n 6      : 1                \n (Other):54                \n\n\nPlotting is slightly more complicated by the fact that we want to represent two groups. There are many ways to do this, including stacked bar charts, side-by-side bars, or the method chosen here, using facet_wrap(~Sex) to cause two separate plots based on Sex to be created.\n\nggplot(prefsABsex,aes(Pref)) +\n  geom_bar(width=0.5,alpha=0.4,fill=\"lightskyblue1\") +\n  facet_wrap(~Sex) +\n  theme_tufte(base_size=7)\n\n\n\n\nAlthough we can guess by looking at the above plot that the difference for females is significant and the difference for males is not, a Pearson chi-square test provides some statistical evidence for this hunch.\n\nprfs &lt;- xtabs( ~ Pref + Sex, data=prefsABsex) # the '+' sign indicates two vars\nprfs\n\n    Sex\nPref  F  M\n   A  2 12\n   B 29 17\n\nchisq.test(prfs)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  prfs\nX-squared = 8.3588, df = 1, p-value = 0.003838\n\n\n\n\n13.1.4 What if the data are lopsided? (G-test, alternative to chi-square)\nWikipedia tells us that the \\(G\\)-test dominates the \\(\\chi^2\\) test when \\(O_i&gt;2E_i\\) in the formula\n\\[\\chi^2=\\sum_i \\frac{(O_i-E_i)^2}{E_i}\\]\nwhere \\(O_i\\) is the observed and \\(E_i\\) is the expected proportion in the \\(i\\)th bin. This situation may occur in small sample sizes. For large sample sizes, both tests give the same conclusion. In our case, we’re on the borderline for this rule in the bin where 29 females prefer B. All females would have to prefer B for the rule to dictate a switch to the \\(G\\)-test.\n\nlibrary(RVAideMemoire)\n\n*** Package RVAideMemoire v 0.9-83-3 ***\n\nG.test(prfs)\n\n\n    G-test\n\ndata:  prfs\nG = 11.025, df = 1, p-value = 0.0008989\n\n#. Fisher's exact test\nfisher.test(prfs)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  prfs\np-value = 0.001877\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.009898352 0.537050159\nsample estimates:\nodds ratio \n 0.1015763 \n\n\n\n\n13.1.5 How many males vs females prefer website A, B, or C? (Two-sample tests of proportions in three categories)\nRevisit our data file with 3 response categories, but now with sex (M/F).\n\nprefsABCsex &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/prefsABCsex.csv\"))\nhead(prefsABCsex)\n\n# A tibble: 6 × 3\n  Subject Pref  Sex  \n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1       1 C     F    \n2       2 C     M    \n3       3 B     M    \n4       4 C     M    \n5       5 C     M    \n6       6 B     F    \n\nprefsABCsex$Subject &lt;- factor(prefsABCsex$Subject)\nprefsABCsex$Pref &lt;- factor(prefsABCsex$Pref)\nprefsABCsex$Sex &lt;- factor(prefsABCsex$Sex)\nsummary(prefsABCsex)\n\n    Subject   Pref   Sex   \n 1      : 1   A: 8   F:29  \n 2      : 1   B:21   M:31  \n 3      : 1   C:31         \n 4      : 1                \n 5      : 1                \n 6      : 1                \n (Other):54                \n\nggplot(prefsABCsex,aes(Pref)) +\n  geom_bar(width=0.5,alpha=0.4,fill=\"lightskyblue1\") +\n  facet_wrap(~Sex) +\n  theme_tufte(base_size=7)\n\n\n\n#. Pearson chi-square test\nprfs &lt;- xtabs( ~ Pref + Sex, data=prefsABCsex)\nprfs\n\n    Sex\nPref  F  M\n   A  3  5\n   B 15  6\n   C 11 20\n\nchisq.test(prfs)\n\nWarning in chisq.test(prfs): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  prfs\nX-squared = 6.9111, df = 2, p-value = 0.03157\n\n#. G-test\nG.test(prfs)\n\n\n    G-test\n\ndata:  prfs\nG = 7.0744, df = 2, p-value = 0.02909\n\n#. Fisher's exact test\nfisher.test(prfs)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  prfs\np-value = 0.03261\nalternative hypothesis: two.sided\n\n\nNow conduct manual post hoc binomial tests for (m)ales—do any prefs for A–C significantly differ from chance for males?\n\nma &lt;- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"A\"),\n        nrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\nmb &lt;- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"B\"),\n        nrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\nmc &lt;- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"C\"),\n        nrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\n#. correct for multiple comparisons\np.adjust(c(ma$p.value, mb$p.value, mc$p.value), method=\"holm\")\n\n[1] 0.109473564 0.126622172 0.001296754\n\n\nNext, conduct manual post hoc binomial tests for (f)emales—do any prefs for A–C significantly differ from chance for females?\n\nfa &lt;- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"A\"),\n        nrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\nfb &lt;- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"B\"),\n        nrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\nfc &lt;- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"C\"),\n        nrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\n#. correct for multiple comparisons\np.adjust(c(fa$p.value, fb$p.value, fc$p.value), method=\"holm\")\n\n[1] 0.02703274 0.09447821 0.69396951"
  },
  {
    "objectID": "week13.html#how-do-groups-compare-in-reading-performance-independent-samples-t-test",
    "href": "week13.html#how-do-groups-compare-in-reading-performance-independent-samples-t-test",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.2 How do groups compare in reading performance? (Independent samples \\(t\\)-test)",
    "text": "13.2 How do groups compare in reading performance? (Independent samples \\(t\\)-test)\nHere we are asking which group read more pages on a particular website.\n\npgviews &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/pgviews.csv\"))\npgviews$Subject &lt;- factor(pgviews$Subject)\npgviews$Site &lt;- factor(pgviews$Site)\nsummary(pgviews)\n\n    Subject    Site        Pages       \n 1      :  1   A:245   Min.   : 1.000  \n 2      :  1   B:255   1st Qu.: 3.000  \n 3      :  1           Median : 4.000  \n 4      :  1           Mean   : 3.958  \n 5      :  1           3rd Qu.: 5.000  \n 6      :  1           Max.   :11.000  \n (Other):494                           \n\ntail(pgviews)\n\n# A tibble: 6 × 3\n  Subject Site  Pages\n  &lt;fct&gt;   &lt;fct&gt; &lt;dbl&gt;\n1 495     A         3\n2 496     B         6\n3 497     B         6\n4 498     A         3\n5 499     A         4\n6 500     B         6\n\n#. descriptive statistics by Site\nplyr::ddply(pgviews, ~ Site, function(data) summary(data$Pages))\n\n  Site Min. 1st Qu. Median     Mean 3rd Qu. Max.\n1    A    1       3      3 3.404082       4    6\n2    B    1       3      4 4.490196       6   11\n\nplyr::ddply(pgviews, ~ Site, summarise, Pages.mean=mean(Pages), Pages.sd=sd(Pages))\n\n  Site Pages.mean Pages.sd\n1    A   3.404082 1.038197\n2    B   4.490196 2.127552\n\n#. graph histograms and a boxplot\nggplot(pgviews,aes(Pages,fill=Site,color=Site)) +\n  geom_bar(alpha=0.5,position=\"identity\",color=\"white\") +\n  scale_color_grey() +\n  scale_fill_grey() +\n  theme_tufte(base_size=7)\n\n\n\nggplot(pgviews,aes(Site,Pages,fill=Site)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n\n\n\n#. independent-samples t-test\nt.test(Pages ~ Site, data=pgviews, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  Pages by Site\nt = -7.2083, df = 498, p-value = 2.115e-12\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -1.3821544 -0.7900745\nsample estimates:\nmean in group A mean in group B \n       3.404082        4.490196"
  },
  {
    "objectID": "week13.html#anova",
    "href": "week13.html#anova",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.3 ANOVA",
    "text": "13.3 ANOVA\nANOVA stands for analysis of variance and is a way to generalize the \\(t\\)-test to more groups.\n\n13.3.1 How long does it take to perform tasks on two IDEs?\n\nide2 &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/ide2.csv\"))\nide2$Subject &lt;- factor(ide2$Subject) # convert to nominal factor\nide2$IDE &lt;- factor(ide2$IDE) # convert to nominal factor\nsummary(ide2)\n\n    Subject        IDE          Time      \n 1      : 1   Eclipse:20   Min.   :155.0  \n 2      : 1   VStudio:20   1st Qu.:271.8  \n 3      : 1                Median :313.5  \n 4      : 1                Mean   :385.1  \n 5      : 1                3rd Qu.:422.0  \n 6      : 1                Max.   :952.0  \n (Other):34                               \n\n#. view descriptive statistics by IDE\nplyr::ddply(ide2, ~ IDE, function(data) summary(data$Time))\n\n      IDE Min. 1st Qu. Median   Mean 3rd Qu. Max.\n1 Eclipse  232  294.75  393.5 468.15  585.50  952\n2 VStudio  155  246.50  287.0 302.10  335.25  632\n\nplyr::ddply(ide2, ~ IDE, summarise, Time.mean=mean(Time), Time.sd=sd(Time))\n\n      IDE Time.mean  Time.sd\n1 Eclipse    468.15 218.1241\n2 VStudio    302.10 101.0778\n\n#. graph histograms and a boxplot\nggplot(ide2,aes(Time,fill=IDE)) +\n  geom_histogram(binwidth=50,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Paired\") +\n  theme_tufte(base_size=7)\n\n\n\nggplot(ide2,aes(IDE,Time,fill=IDE)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n\n\n\n#. independent-samples t-test (suitable? maybe not, because...)\nt.test(Time ~ IDE, data=ide2, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  Time by IDE\nt = 3.0889, df = 38, p-value = 0.003745\nalternative hypothesis: true difference in means between group Eclipse and group VStudio is not equal to 0\n95 percent confidence interval:\n  57.226 274.874\nsample estimates:\nmean in group Eclipse mean in group VStudio \n               468.15                302.10 \n\n\n\n\n13.3.2 Testing ANOVA assumptions\n\n#. Shapiro-Wilk normality test on response\nshapiro.test(ide2[ide2$IDE == \"VStudio\",]$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ide2[ide2$IDE == \"VStudio\", ]$Time\nW = 0.84372, p-value = 0.004191\n\nshapiro.test(ide2[ide2$IDE == \"Eclipse\",]$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ide2[ide2$IDE == \"Eclipse\", ]$Time\nW = 0.87213, p-value = 0.01281\n\n#. but really what matters most is the residuals\nm = aov(Time ~ IDE, data=ide2) # fit model\nshapiro.test(residuals(m)) # test residuals\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.894, p-value = 0.001285\n\npar(pin=c(2.75,1.25),cex=0.5)\nqqnorm(residuals(m)); qqline(residuals(m)) # plot residuals\n\n\n\n\n\n\n13.3.3 Kolmogorov-Smirnov test for log-normality\nFit the distribution to a lognormal to estimate fit parameters then supply those to a K-S test with the lognormal distribution fn (see ?plnorm). See ?distributions for many other named probability distributions.\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nfit &lt;- fitdistr(ide2[ide2$IDE == \"VStudio\",]$Time,\n           \"lognormal\")$estimate\nks.test(ide2[ide2$IDE == \"VStudio\",]$Time, \"plnorm\",\n    meanlog=fit[1], sdlog=fit[2], exact=TRUE)\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  ide2[ide2$IDE == \"VStudio\", ]$Time\nD = 0.13421, p-value = 0.8181\nalternative hypothesis: two-sided\n\nfit &lt;- fitdistr(ide2[ide2$IDE == \"Eclipse\",]$Time,\n           \"lognormal\")$estimate\nks.test(ide2[ide2$IDE == \"Eclipse\",]$Time, \"plnorm\",\n    meanlog=fit[1], sdlog=fit[2], exact=TRUE)\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  ide2[ide2$IDE == \"Eclipse\", ]$Time\nD = 0.12583, p-value = 0.871\nalternative hypothesis: two-sided\n\n#. tests for homoscedasticity (homogeneity of variance)\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nleveneTest(Time ~ IDE, data=ide2, center=mean) # Levene's test\n\nLevene's Test for Homogeneity of Variance (center = mean)\n      Df F value   Pr(&gt;F)   \ngroup  1  11.959 0.001356 **\n      38                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nleveneTest(Time ~ IDE, data=ide2, center=median) # Brown-Forsythe test\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)  \ngroup  1  5.9144 0.01984 *\n      38                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#. Welch t-test for unequal variances handles\n#. the violation of homoscedasticity. but not\n#. the violation of normality.\nt.test(Time ~ IDE, data=ide2, var.equal=FALSE) # Welch t-test\n\n\n    Welch Two Sample t-test\n\ndata:  Time by IDE\nt = 3.0889, df = 26.8, p-value = 0.004639\nalternative hypothesis: true difference in means between group Eclipse and group VStudio is not equal to 0\n95 percent confidence interval:\n  55.71265 276.38735\nsample estimates:\nmean in group Eclipse mean in group VStudio \n               468.15                302.10 \n\n\n\n\n13.3.4 Data transformation\n\n#. create a new column in ide2 defined as log(Time)\nide2$logTime &lt;- log(ide2$Time) # log transform\nhead(ide2) # verify\n\n# A tibble: 6 × 4\n  Subject IDE      Time logTime\n  &lt;fct&gt;   &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 1       VStudio   341    5.83\n2 2       VStudio   291    5.67\n3 3       VStudio   283    5.65\n4 4       VStudio   155    5.04\n5 5       VStudio   271    5.60\n6 6       VStudio   270    5.60\n\n#. explore for intuition-building\nggplot(ide2,aes(logTime,fill=IDE)) +\n  geom_histogram(binwidth=0.2,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Paired\") +\n  theme_tufte(base_size=7)\n\n\n\nggplot(ide2,aes(IDE,logTime,fill=IDE)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n\n\n\n#. re-test for normality\nshapiro.test(ide2[ide2$IDE == \"VStudio\",]$logTime)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ide2[ide2$IDE == \"VStudio\", ]$logTime\nW = 0.95825, p-value = 0.5094\n\nshapiro.test(ide2[ide2$IDE == \"Eclipse\",]$logTime)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ide2[ide2$IDE == \"Eclipse\", ]$logTime\nW = 0.93905, p-value = 0.23\n\nm &lt;- aov(logTime ~ IDE, data=ide2) # fit model\nshapiro.test(residuals(m)) # test residuals\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.96218, p-value = 0.1987\n\npar(pin=c(2.75,1.25),cex=0.5)\nqqnorm(residuals(m)); qqline(residuals(m)) # plot residuals\n\n\n\n#. re-test for homoscedasticity\nleveneTest(logTime ~ IDE, data=ide2, center=median) # Brown-Forsythe test\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)  \ngroup  1  3.2638 0.07875 .\n      38                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#. independent-samples t-test (now suitable for logTime)\nt.test(logTime ~ IDE, data=ide2, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  logTime by IDE\nt = 3.3121, df = 38, p-value = 0.002039\nalternative hypothesis: true difference in means between group Eclipse and group VStudio is not equal to 0\n95 percent confidence interval:\n 0.1514416 0.6276133\nsample estimates:\nmean in group Eclipse mean in group VStudio \n             6.055645              5.666118 \n\n\n\n\n13.3.5 What if ANOVA assumptions don’t hold? (Nonparametric equivalent of independent-samples t-test)\n\n\n13.3.6 Mann-Whitney U test\n\nlibrary(coin)\n\nLoading required package: survival\n\nwilcox_test(Time ~ IDE, data=ide2, distribution=\"exact\")\n\n\n    Exact Wilcoxon-Mann-Whitney Test\n\ndata:  Time by IDE (Eclipse, VStudio)\nZ = 2.9487, p-value = 0.002577\nalternative hypothesis: true mu is not equal to 0\n\nwilcox_test(logTime ~ IDE, data=ide2, distribution=\"exact\") # note: same result\n\n\n    Exact Wilcoxon-Mann-Whitney Test\n\ndata:  logTime by IDE (Eclipse, VStudio)\nZ = 2.9487, p-value = 0.002577\nalternative hypothesis: true mu is not equal to 0\n\n\n\n\n13.3.7 How long does it take to do tasks on one of three tools? (One-way ANOVA preparation)\n\n#. read in a data file with task completion times (min) now from 3 tools\nide3 &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/ide3.csv\"))\nide3$Subject &lt;- factor(ide3$Subject) # convert to nominal factor\nide3$IDE &lt;- factor(ide3$IDE) # convert to nominal factor\nsummary(ide3)\n\n    Subject        IDE          Time      \n 1      : 1   Eclipse:20   Min.   :143.0  \n 2      : 1   PyCharm:20   1st Qu.:248.8  \n 3      : 1   VStudio:20   Median :295.0  \n 4      : 1                Mean   :353.9  \n 5      : 1                3rd Qu.:391.2  \n 6      : 1                Max.   :952.0  \n (Other):54                               \n\n#. view descriptive statistics by IDE\nplyr::ddply(ide3, ~ IDE, function(data) summary(data$Time))\n\n      IDE Min. 1st Qu. Median   Mean 3rd Qu. Max.\n1 Eclipse  232  294.75  393.5 468.15  585.50  952\n2 PyCharm  143  232.25  279.5 291.45  300.00  572\n3 VStudio  155  246.50  287.0 302.10  335.25  632\n\nplyr::ddply(ide3, ~ IDE, summarise, Time.mean=mean(Time), Time.sd=sd(Time))\n\n      IDE Time.mean  Time.sd\n1 Eclipse    468.15 218.1241\n2 PyCharm    291.45 106.8922\n3 VStudio    302.10 101.0778\n\nide3 |&gt;\n  group_by(IDE) |&gt;\n  summarize(median=median(Time),mean=mean(Time),sd=sd(Time))\n\n# A tibble: 3 × 4\n  IDE     median  mean    sd\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Eclipse   394.  468.  218.\n2 PyCharm   280.  291.  107.\n3 VStudio   287   302.  101.\n\n#. explore new response distribution\nggplot(ide3,aes(Time,fill=IDE)) +\n  geom_histogram(binwidth=50,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n\n\n\nggplot(ide3,aes(IDE,Time,fill=IDE)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n\n\n\n#. test normality for new IDE\nshapiro.test(ide3[ide3$IDE == \"PyCharm\",]$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ide3[ide3$IDE == \"PyCharm\", ]$Time\nW = 0.88623, p-value = 0.02294\n\nm &lt;- aov(Time ~ IDE, data=ide3) # fit model\nshapiro.test(residuals(m)) # test residuals\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.89706, p-value = 0.000103\n\npar(pin=c(2.75,1.25),cex=0.5)\nqqnorm(residuals(m)); qqline(residuals(m)) # plot residuals\n\n\n\n#. test log-normality of new IDE\nfit &lt;- fitdistr(ide3[ide3$IDE == \"PyCharm\",]$Time, \"lognormal\")$estimate\nks.test(ide3[ide3$IDE == \"PyCharm\",]$Time,\n    \"plnorm\", meanlog=fit[1], sdlog=fit[2], exact=TRUE) # lognormality\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  ide3[ide3$IDE == \"PyCharm\", ]$Time\nD = 0.1864, p-value = 0.4377\nalternative hypothesis: two-sided\n\n#. compute new log(Time) column and re-test\nide3$logTime &lt;- log(ide3$Time) # add new column\nshapiro.test(ide3[ide3$IDE == \"PyCharm\",]$logTime)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ide3[ide3$IDE == \"PyCharm\", ]$logTime\nW = 0.96579, p-value = 0.6648\n\nm &lt;- aov(logTime ~ IDE, data=ide3) # fit model\nshapiro.test(residuals(m)) # test residuals\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.96563, p-value = 0.08893\n\npar(pin=c(2.75,1.25),cex=0.5)\nqqnorm(residuals(m)); qqline(residuals(m)) # plot residuals\n\n\n\n#. test homoscedasticity\nleveneTest(logTime ~ IDE, data=ide3, center=median) # Brown-Forsythe test\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2  1.7797 0.1779\n      57               \n\n\n\n\n13.3.8 Can we transform data so it fits assumptions? (One-way ANOVA, suitable now to logTime)\n\nm &lt;- aov(logTime ~ IDE, data=ide3) # fit model\nanova(m) # report anova\n\nAnalysis of Variance Table\n\nResponse: logTime\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nIDE        2 2.3064  1.1532   8.796 0.0004685 ***\nResiduals 57 7.4729  0.1311                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#. post hoc independent-samples t-tests\nlibrary(multcomp)\n\nLoading required package: mvtnorm\n\n\nLoading required package: TH.data\n\n\n\nAttaching package: 'TH.data'\n\n\nThe following object is masked from 'package:MASS':\n\n    geyser\n\nsummary(glht(m, mcp(IDE=\"Tukey\")), test=adjusted(type=\"holm\")) # Tukey means compare all pairs\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = logTime ~ IDE, data = ide3)\n\nLinear Hypotheses:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \nPyCharm - Eclipse == 0  -0.4380     0.1145  -3.826 0.000978 ***\nVStudio - Eclipse == 0  -0.3895     0.1145  -3.402 0.002458 ** \nVStudio - PyCharm == 0   0.0485     0.1145   0.424 0.673438    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n#. note: equivalent to this using lsm instead of mcp\nlibrary(emmeans)\nsummary(glht(m, lsm(pairwise ~ IDE)), test=adjusted(type=\"holm\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: aov(formula = logTime ~ IDE, data = ide3)\n\nLinear Hypotheses:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \nEclipse - PyCharm == 0   0.4380     0.1145   3.826 0.000978 ***\nEclipse - VStudio == 0   0.3895     0.1145   3.402 0.002458 ** \nPyCharm - VStudio == 0  -0.0485     0.1145  -0.424 0.673438    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\n\n\n13.3.9 What if we can’t transform data to fit ANOVA assumptions? (Nonparametric equivalent of one-way ANOVA)\n\n#. Kruskal-Wallis test\nkruskal_test(Time ~ IDE, data=ide3, distribution=\"asymptotic\") # can't do exact with 3 levels\n\n\n    Asymptotic Kruskal-Wallis Test\n\ndata:  Time by IDE (Eclipse, PyCharm, VStudio)\nchi-squared = 12.17, df = 2, p-value = 0.002277\n\nkruskal_test(logTime ~ IDE, data=ide3, distribution=\"asymptotic\") # note: same result\n\n\n    Asymptotic Kruskal-Wallis Test\n\ndata:  logTime by IDE (Eclipse, PyCharm, VStudio)\nchi-squared = 12.17, df = 2, p-value = 0.002277\n\n#. for reporting Kruskal-Wallis as chi-square, we can get N with nrow(ide3)\n\n#. manual post hoc Mann-Whitney U pairwise comparisons\n#. note: wilcox_test we used above doesn't take two data vectors, so use wilcox.test\nvs.ec &lt;- wilcox.test(ide3[ide3$IDE == \"VStudio\",]$Time,\n            ide3[ide3$IDE == \"Eclipse\",]$Time, exact=FALSE)\nvs.py &lt;- wilcox.test(ide3[ide3$IDE == \"VStudio\",]$Time,\n            ide3[ide3$IDE == \"PyCharm\",]$Time, exact=FALSE)\nec.py &lt;- wilcox.test(ide3[ide3$IDE == \"Eclipse\",]$Time,\n            ide3[ide3$IDE == \"PyCharm\",]$Time, exact=FALSE)\np.adjust(c(vs.ec$p.value, vs.py$p.value, ec.py$p.value), method=\"holm\")\n\n[1] 0.007681846 0.588488864 0.007681846\n\n#. alternative approach is using PMCMRplus for nonparam pairwise comparisons\nlibrary(PMCMRplus)\nkwAllPairsConoverTest(Time ~ IDE, data=ide3, p.adjust.method=\"holm\")\n\nWarning in kwAllPairsConoverTest.default(c(341, 291, 283, 155, 271, 270, : Ties\nare present. Quantiles were corrected for ties.\n\n\n\n    Pairwise comparisons using Conover's all-pairs test\n\n\ndata: Time by IDE\n\n\n        Eclipse PyCharm\nPyCharm 0.0025  -      \nVStudio 0.0062  0.6620 \n\n\n\nP value adjustment method: holm\n\n\nThe above test was reported by W. J. Conover and R. L. Iman (1979), On multiple-comparisons procedures, Tech. Rep. LA-7677-MS, Los Alamos Scientific Laboratory.\n\n\n13.3.10 Another example of tasks using two tools (More on oneway ANOVA)\nThe designtime data records task times in minutes to complete the same project in Illustrator or InDesign.\nRead the designtime data into R. Determine how many subjects participated.\n\ndt &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/designtime.csv\"))\n#. convert Subject to a factor\ndt$Subject&lt;-as.factor(dt$Subject)\ndt$Tool&lt;-as.factor(dt$Tool)\nsummary(dt)\n\n    Subject            Tool         Time       \n 1      : 1   Illustrator:30   Min.   : 98.19  \n 2      : 1   InDesign   :30   1st Qu.:149.34  \n 3      : 1                    Median :205.54  \n 4      : 1                    Mean   :275.41  \n 5      : 1                    3rd Qu.:361.99  \n 6      : 1                    Max.   :926.15  \n (Other):54                                    \n\nlength(dt$Subject)\n\n[1] 60\n\ntail(dt)\n\n# A tibble: 6 × 3\n  Subject Tool         Time\n  &lt;fct&gt;   &lt;fct&gt;       &lt;dbl&gt;\n1 55      Illustrator  218.\n2 56      InDesign     180.\n3 57      Illustrator  170.\n4 58      InDesign     186.\n5 59      Illustrator  241.\n6 60      InDesign     159.\n\n\nWe see from the summary that there are sixty observations. We can see the same by checking the length() of the Subject (or any other) variable in the data.\nCreate a boxplot of the task time for each tool and comment on the medians and variances.\n\nggplot(dt,aes(Tool,Time,fill=Tool)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n\n\n\n\nBoth the median and the variance is much larger for Illustrator than for InDesign.\nConduct a Shapiro-Wilk test for normality for each tool and comment.\n\nshapiro.test(dt[dt$Tool==\"Illustrator\",]$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dt[dt$Tool == \"Illustrator\", ]$Time\nW = 0.90521, p-value = 0.01129\n\nshapiro.test(dt[dt$Tool==\"InDesign\",]$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dt[dt$Tool == \"InDesign\", ]$Time\nW = 0.95675, p-value = 0.2553\n\n\nIn the case of InDesign, we fail to reject the null hypothesis that the data are drawn from a normal distribution. In the case of Illustrator, we reject the null hypothesis at the five percent level but not at the one percent level (just barely).\nConduct a Shapiro-Wilk test for normality on the residuals and comment.\n\nm&lt;-aov(Time~Tool,data=dt)\nshapiro.test(residuals(m))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.85077, p-value = 3.211e-06\n\n\nWe reject the null hypothesis that the residuals are normally distributed.\nConduct a Brown-Forsythe test of homoscedasticity.\n\nleveneTest(Time~Tool,data=dt,center=median)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value    Pr(&gt;F)    \ngroup  1  20.082 3.545e-05 ***\n      58                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe reject the null hypothesis that the two samples are drawn from populations with equal variance.\nFit a lognormal distribution to the Time response for each Tool. Conduct a Kolmogorov-Smirnov goodness-of-fit test and comment.\n\nfit&lt;-fitdistr(dt[dt$Tool==\"Illustrator\",]$Time,\n    \"lognormal\")$estimate\ntst&lt;-ks.test(dt[dt$Tool==\"Illustrator\",]$Time,\n    \"plnorm\",meanlog=fit[1],sdlog=fit[2],exact=TRUE)\ntst\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  dt[dt$Tool == \"Illustrator\", ]$Time\nD = 0.093358, p-value = 0.9344\nalternative hypothesis: two-sided\n\nfit&lt;-fitdistr(dt[dt$Tool==\"InDesign\",]$Time,\n    \"lognormal\")$estimate\ntst&lt;-ks.test(dt[dt$Tool==\"InDesign\",]$Time,\n    \"plnorm\",meanlog=fit[1],sdlog=fit[2],exact=TRUE)\ntst\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  dt[dt$Tool == \"InDesign\", ]$Time\nD = 0.10005, p-value = 0.8958\nalternative hypothesis: two-sided\n\n\nWe fail to reject the null hypothesis that the Illustrator sample is drawn from a lognormal distribution. We fail to reject the null hypothesis that the InDesign sample is drawn from a lognormal distribution.\nCreate a log-transformed Time response column. Compute the mean for each tool and comment.\n\ndt$logTime&lt;-log(dt$Time)\nmean(dt$logTime[dt$Tool==\"Illustrator\"])\n\n[1] 5.894288\n\nmean(dt$logTime[dt$Tool==\"InDesign\"])\n\n[1] 5.03047\n\ndt |&gt;\n  group_by(Tool) |&gt;\n  summarize(mean=mean(logTime),sd=sd(logTime))\n\n# A tibble: 2 × 3\n  Tool         mean    sd\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Illustrator  5.89 0.411\n2 InDesign     5.03 0.211\n\n\nThe mean for Illustrator appears to be larger than the mean for InDesign.\nConduct an independent-samples \\(t\\)-test on the log-transformed Time response, using the Welch version for unequal variances and comment.\n\nt.test(logTime~Tool,data=dt,var.equal=FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  logTime by Tool\nt = 10.23, df = 43.293, p-value = 3.98e-13\nalternative hypothesis: true difference in means between group Illustrator and group InDesign is not equal to 0\n95 percent confidence interval:\n 0.6935646 1.0340718\nsample estimates:\nmean in group Illustrator    mean in group InDesign \n                 5.894288                  5.030470 \n\n\nWe reject the null hypothesis that the true difference in means is equal to 0.\nConduct an exact nonparametric Mann-Whitney \\(U\\) test on the Time response and comment.\n\nwilcox_test(Time~Tool,data=dt,distribution=\"exact\")\n\n\n    Exact Wilcoxon-Mann-Whitney Test\n\ndata:  Time by Tool (Illustrator, InDesign)\nZ = 6.3425, p-value = 5.929e-14\nalternative hypothesis: true mu is not equal to 0\n\n\nWe reject the null hypothesis that the samples were drawn from populations with the same distribution.\n\n\n13.3.11 Differences in writing speed among three tools (Three levels of a factor in ANOVA)\nWe’ll examine three levels of a factor, which is an alphabet system used for writing. The three levels are named for the text entry systems, EdgeWrite, Graffiti, and Unistrokes.\n\nalpha &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/alphabets.csv\"))\nalpha$Subject&lt;-as.factor(alpha$Subject)\nalpha$Alphabet&lt;-as.factor(alpha$Alphabet)\nsummary(alpha)\n\n    Subject         Alphabet       WPM        \n 1      : 1   EdgeWrite :20   Min.   : 3.960  \n 2      : 1   Graffiti  :20   1st Qu.: 9.738  \n 3      : 1   Unistrokes:20   Median :13.795  \n 4      : 1                   Mean   :14.517  \n 5      : 1                   3rd Qu.:18.348  \n 6      : 1                   Max.   :28.350  \n (Other):54                                   \n\n\nPlot the three text entry systems. ::: {.cell}\nggplot(alpha,aes(Alphabet,WPM,fill=Alphabet)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n\n\n\n:::\nIdentify the average words per minute written with EdgeWrite.\n\nmean(alpha[alpha$Alphabet==\"EdgeWrite\",]$WPM)\n\n[1] 17.14\n\n\nConduct a Shapiro-Wilk test for normality on each method.\n\nshapiro.test(alpha$WPM[alpha$Alphabet==\"EdgeWrite\"])\n\n\n    Shapiro-Wilk normality test\n\ndata:  alpha$WPM[alpha$Alphabet == \"EdgeWrite\"]\nW = 0.95958, p-value = 0.5355\n\nshapiro.test(alpha$WPM[alpha$Alphabet==\"Graffiti\"])\n\n\n    Shapiro-Wilk normality test\n\ndata:  alpha$WPM[alpha$Alphabet == \"Graffiti\"]\nW = 0.94311, p-value = 0.2743\n\nshapiro.test(alpha$WPM[alpha$Alphabet==\"Unistrokes\"])\n\n\n    Shapiro-Wilk normality test\n\ndata:  alpha$WPM[alpha$Alphabet == \"Unistrokes\"]\nW = 0.94042, p-value = 0.2442\n\n\nConduct a Shapiro-Wilk test for normality on the residuals of an ANOVA model stipulating that Alphabet affects WPM.\n\nm&lt;-aov(WPM~Alphabet,data=alpha)\nshapiro.test(residuals(m))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.97762, p-value = 0.3363\n\n\nTest for homoscedasticity.\n\nleveneTest(alpha$WPM~alpha$Alphabet,center=\"median\")\n\nLevene's Test for Homogeneity of Variance (center = \"median\")\n      Df F value Pr(&gt;F)\ngroup  2  1.6219 0.2065\n      57               \n\n\nNow test all three. The mcp function tests multiple means. The keyword Tukey means to do all the possible pairwise comparisons of Alphabet, i.e., Graffiti and EdgeWrite, Graffiti and Unistrokes, and EdgeWrite and Unistrokes. m is the oneway ANOVA model we created above.\n\nsummary(multcomp::glht(m,multcomp::mcp(Alphabet=\"Tukey\")),test=adjusted(type=\"holm\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = WPM ~ Alphabet, data = alpha)\n\nLinear Hypotheses:\n                            Estimate Std. Error t value Pr(&gt;|t|)   \nGraffiti - EdgeWrite == 0     -2.101      1.693  -1.241  0.21982   \nUnistrokes - EdgeWrite == 0   -5.769      1.693  -3.407  0.00363 **\nUnistrokes - Graffiti == 0    -3.668      1.693  -2.166  0.06894 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\nConduct a nonparametric oneway ANOVA using the Kruskal-Wallis test to see if the samples have the same distribution. The null hypothesis is that the samples come from the same distribution.\n\nkruskal_test(alpha$WPM~alpha$Alphabet,distribution=\"asymptotic\")\n\n\n    Asymptotic Kruskal-Wallis Test\n\ndata:  alpha$WPM by\n     alpha$Alphabet (EdgeWrite, Graffiti, Unistrokes)\nchi-squared = 9.7019, df = 2, p-value = 0.007821\n\n\nConduct manual post hoc Mann-Whitney pairwise comparisons and adjust the \\(p\\)-values to take into account the possibility of false discovery.\n\newgf&lt;-wilcox.test(alpha$WPM[alpha$Alphabet==\"EdgeWrite\"],alpha$WPM[alpha$Alphabet==\"Graffiti\"],paired=FALSE,exact=FALSE)\newun&lt;-wilcox.test(alpha$WPM[alpha$Alphabet==\"EdgeWrite\"],alpha$WPM[alpha$Alphabet==\"Unistrokes\"],paired=FALSE,exact=FALSE)\ngfun&lt;-wilcox.test(alpha$WPM[alpha$Alphabet==\"Graffiti\"],alpha$WPM[alpha$Alphabet==\"Unistrokes\"],paired=FALSE,exact=FALSE)\np.adjust(c(ewgf$p.value,ewun$p.value,gfun$p.value),method=\"holm\")\n\n[1] 0.20358147 0.01810677 0.04146919"
  },
  {
    "objectID": "week13.html#same-person-using-two-different-tools-paired-samples-t-test",
    "href": "week13.html#same-person-using-two-different-tools-paired-samples-t-test",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.4 Same person using two different tools (Paired samples \\(t\\)-test)",
    "text": "13.4 Same person using two different tools (Paired samples \\(t\\)-test)\nIs it better to search or scroll for contacts in a smartphone contacts manager? Which takes more time? Which takes more effort? Which is more error-prone? Start by reading in data, converting to factors, and summarizing.\n\nsrchscrl &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/srchscrl.csv\"))\nsrchscrl$Subject &lt;- factor(srchscrl$Subject)\nsrchscrl$Order   &lt;- factor(srchscrl$Order)\nsrchscrl$Technique   &lt;- factor(srchscrl$Technique)\n#. srchscrl$Errors   &lt;- factor(srchscrl$Errors,ordered=TRUE,levels=c(0,1,2,3,4))\nsummary(srchscrl)\n\n    Subject    Technique  Order       Time           Errors         Effort \n 1      : 2   Scroll:20   1:20   Min.   : 49.0   Min.   :0.00   Min.   :1  \n 2      : 2   Search:20   2:20   1st Qu.: 94.5   1st Qu.:0.75   1st Qu.:3  \n 3      : 2                      Median :112.5   Median :1.50   Median :4  \n 4      : 2                      Mean   :117.0   Mean   :1.60   Mean   :4  \n 5      : 2                      3rd Qu.:148.2   3rd Qu.:2.25   3rd Qu.:5  \n 6      : 2                      Max.   :192.0   Max.   :4.00   Max.   :7  \n (Other):28                                                                \n\n\nlibrary(xtable)\noptions(xtable.comment=FALSE)\noptions(xtable.booktabs=TRUE)\nxtable(head(srchscrl),caption=\"First rows of data\")\nView descriptive statistics by Technique. There are several ways to do this. The following uses the plyr package. ::: {.cell}\nplyr::ddply(srchscrl, ~ Technique,\n      function(data) summary(data$Time))\n\n  Technique Min. 1st Qu. Median  Mean 3rd Qu. Max.\n1    Scroll   49  123.50  148.5 137.2     161  192\n2    Search   50   86.75   99.5  96.8     106  147\n\nplyr::ddply(srchscrl, ~ Technique,\n      summarise, Time.mean=mean(Time), Time.sd=sd(Time))\n\n  Technique Time.mean  Time.sd\n1    Scroll     137.2 35.80885\n2    Search      96.8 23.23020\n\n:::\nAnother approach is to use the dplyr package. Be aware that it conflicts with plyr so you should try to avoid using both. If you must use both, as I did above, it may make the most sense to call particular functions from the plyr package rather than load the package. This is what I did with plyr::ddply() above.\n\nsrchscrl |&gt;\n  group_by(Technique) |&gt;\n  summarize(mean=mean(Time),sd=sd(Time))\n\n# A tibble: 2 × 3\n  Technique  mean    sd\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Scroll    137.   35.8\n2 Search     96.8  23.2\n\n\nYou can explore the Time response by making histograms or boxplots. One approach is to use the ggplot2 package and put the histograms together in one frame. The ggplot2 package allows for a remarkable variety of options.\n\nggplot(srchscrl,aes(Time,fill=Technique)) +\n  geom_histogram(bins=30,alpha=0.9,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n\n\n\n\nWe can use the same package for boxplots. Boxplots show the median as a bold line in the middle of the box. The box itself ranges from the first quartile (starting at the 25th percentile) to the third quartile (terminating at the 75th percentile). The whiskers run from the minimum to the maximum, where these are defined as the 25th percentile minus 1.5 times the interquartile range and the 75th percentile plus 1.5 times the interquartile range. The interquartile range is the width of the box. Dots outside the whiskers show outliers.\n\nggplot(srchscrl,aes(Technique,Time,fill=Technique)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n\n\n\n\nWe would rather use parametric statistics if ANOVA assumptions are met. Recall that we can test for normality, normality of residuals, and homoscedasticity. In the case of a within-subjects experiment, we can also test for order effects which is one way to test the independence assumption. First test whether these times seem to be drawn from a normal distribution.\n\nshapiro.test(srchscrl[srchscrl$Technique == \"Search\",]$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  srchscrl[srchscrl$Technique == \"Search\", ]$Time\nW = 0.96858, p-value = 0.7247\n\nshapiro.test(srchscrl[srchscrl$Technique == \"Scroll\",]$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  srchscrl[srchscrl$Technique == \"Scroll\", ]$Time\nW = 0.91836, p-value = 0.09213\n\n\nIn both cases we fail to reject the null hypothesis, which is that the Time data are drawn from a normal distribution. Note that we fail to reject at \\(\\alpha=0.05\\) but that in the case of the Scroll technique we would reject at \\(\\alpha=0.1\\).\nFit a model for testing residuals—the Error function is used to indicate within-subject effects, i.e., each Subject was exposed to all levels of Technique. generally, Error(S/(ABC)) means each S was exposed to every level of A, B, C and S is a column encoding subject ids.\n\nm &lt;- aov(Time ~ Technique + Error(Subject/Technique),\n    data=srchscrl)\n\nThe above-specified model has residuals—departures of the observed data from the data that would be expected if the model were accurate.\nNow we can test the residuals of this model for normality and also examine a QQ plot for normality. The QQ plot shows the theoretical line to which the residuals should adhere if they are normally distributed. Deviations from that line are indications of non-normality. First test by Subject.\n\nshapiro.test(residuals(m$Subject))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m$Subject)\nW = 0.9603, p-value = 0.5783\n\nqqnorm(residuals(m$Subject)) \nqqline(residuals(m$Subject))\n\n\n\n\nWe fail to reject the null hypothesis of normality and the QQ plot looks normal. So far, so good.\nNext test by Subject:Technique.\n\nshapiro.test(residuals(m$'Subject:Technique'))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m$\"Subject:Technique\")\nW = 0.97303, p-value = 0.8172\n\nqqnorm(residuals(m$'Subject:Technique'))\nqqline(residuals(m$'Subject:Technique'))\n\n\n\n\nWe fail to reject the null hypothesis of normality and the QQ plot looks normal. We’re getting there.\nWe’re still checking the ANOVA assumptions. Next thing to test is homoscedasticity, the assumption of equal variance. For this we use the Brown-Forsythe test, a variant of Levene’s test that uses the median instead of the mean, providing greater robustness against non-normal data.\n\nleveneTest(Time ~ Technique, data=srchscrl, center=median)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  2.0088 0.1645\n      38               \n\n\nThis experiment used counterbalancing to ward off the possibility of an order effect. An order effect results from learning or fatigue or some other factor based on the order in which the tests were run. We would like to not have that happen and one solution is to have half the subjects do task A first and half the subjects do task B first. This is the simplest form of counterbalancing. It becomes more problematic if there are more than two tasks.\nFor a paired-samples \\(t\\)-test we must use a wide-format table; most R functions do not require a wide-format table, but the dcast() function offers a quick way to translate long-format into wide-format when we need it.\nA wide-format table has one subject in every row. A long-format table has one observation in every row. Most R functions use long-format tables.\n\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\nsrchscrl.wide.order &lt;- dcast(srchscrl, Subject ~ Order,\n                 value.var=\"Time\")\n\nxtable(head(srchscrl.wide.order),\n       caption=\"First rows of wide order\")\nNow conduct a \\(t\\)-test to see if order has an effect. ::: {.cell}\nt.test(srchscrl.wide.order$\"1\", srchscrl.wide.order$\"2\",\n       paired=TRUE, var.equal=TRUE)\n\n\n    Paired t-test\n\ndata:  srchscrl.wide.order$\"1\" and srchscrl.wide.order$\"2\"\nt = -1.3304, df = 19, p-value = 0.1991\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -47.34704  10.54704\nsample estimates:\nmean difference \n          -18.4 \n\n:::\nWe fail to reject the null hypothesis that the responses do not differ according to order. To phrase this in a more readable (!) way, we have evidence that the order does not matter.\n\n13.4.1 Running the paired \\(t\\)-test\nIt now makes sense to use a paired \\(t\\)-test since the ANOVA assumptions have been satisfied. This is a parametric test of Time where we pair subjects by technique. Again, we need the wide-format table to conduct a paired test. The wide-format table has one row for each subject rather than one row for each observation.\n\nsrchscrl.wide.tech = dcast(srchscrl, Subject ~ Technique,\n               value.var=\"Time\")\n\nxtable(head(srchscrl.wide.tech),\n       caption=\"First rows of wide technique\")\n\nt.test(srchscrl.wide.tech$Search, srchscrl.wide.tech$Scroll,\n       paired=TRUE, var.equal=TRUE)\n\n\n    Paired t-test\n\ndata:  srchscrl.wide.tech$Search and srchscrl.wide.tech$Scroll\nt = -3.6399, df = 19, p-value = 0.001743\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -63.63083 -17.16917\nsample estimates:\nmean difference \n          -40.4 \n\n\nThis supports the intuition we developed doing the histogram and boxplots only now we have a valid statistical test to support this intuition.\nSuppose we did not satisfy the ANOVA assumptions. Then we would conduct the nonparametric equivalent of paired-samples t-test.\n\n\n13.4.2 Exploring a Poisson-distributed factor\nExplore the Errors response; error counts are often Poisson-distributed.\n\nplyr::ddply(srchscrl, ~ Technique, function(data)\n      summary(data$Errors))\n\n  Technique Min. 1st Qu. Median Mean 3rd Qu. Max.\n1    Scroll    0       0    0.5  0.7       1    2\n2    Search    1       2    2.5  2.5       3    4\n\nplyr::ddply(srchscrl, ~ Technique, summarise,\n      Errors.mean=mean(Errors), Errors.sd=sd(Errors))\n\n  Technique Errors.mean Errors.sd\n1    Scroll         0.7 0.8013147\n2    Search         2.5 1.0513150\n\n\n\nggplot(srchscrl,aes(Errors,fill=Technique)) +\n  geom_histogram(bins=20,alpha=0.9,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n\n\n\nggplot(srchscrl,aes(Technique,Errors,fill=Technique)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n\n\n\n\nTry to fit a Poisson distribution for count data. Note that ks.test() only works for continuous distributions, but Poisson distributions are discrete, so use fitdist, not fitdistr, and test with gofstat.\n\nlibrary(fitdistrplus)\nfit = fitdist(srchscrl[srchscrl$Technique == \"Search\",]$Errors,\n          \"pois\", discrete=TRUE)\ngofstat(fit) # goodness-of-fit test\n\nChi-squared statistic:  1.522231 \nDegree of freedom of the Chi-squared distribution:  2 \nChi-squared p-value:  0.4671449 \n   the p-value may be wrong with some theoretical counts &lt; 5  \nChi-squared table:\n     obscounts theocounts\n&lt;= 1  4.000000   5.745950\n&lt;= 2  6.000000   5.130312\n&lt;= 3  6.000000   4.275260\n&gt; 3   4.000000   4.848477\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   65.61424\nBayesian Information Criterion   66.60997\n\nfit = fitdist(srchscrl[srchscrl$Technique == \"Scroll\",]$Errors,\n          \"pois\", discrete=TRUE)\ngofstat(fit) # goodness-of-fit test\n\nChi-squared statistic:  0.3816087 \nDegree of freedom of the Chi-squared distribution:  1 \nChi-squared p-value:  0.5367435 \n   the p-value may be wrong with some theoretical counts &lt; 5  \nChi-squared table:\n     obscounts theocounts\n&lt;= 0 10.000000   9.931706\n&lt;= 1  6.000000   6.952194\n&gt; 1   4.000000   3.116100\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   45.53208\nBayesian Information Criterion   46.52781\n\n\nConduct a Wilcoxon signed-rank test on Errors. ::: {.cell}\nwilcoxsign_test(Errors ~ Technique | Subject,\n        data=srchscrl, distribution=\"exact\")\n\n\n    Exact Wilcoxon-Pratt Signed-Rank Test\n\ndata:  y by x (pos, neg) \n     stratified by block\nZ = -3.6701, p-value = 6.104e-05\nalternative hypothesis: true mu is not equal to 0\n\n:::\nNote: the term afer the “|” indicates the within-subjects blocking term for matched pairs.\n\n\n13.4.3 Examining a Likert scale response item\nNow also examine Effort, the ordinal Likert scale response (1-7).\n\nplyr::ddply(srchscrl, ~ Technique, function(data)\n      summary(data$Effort))\n\n  Technique Min. 1st Qu. Median Mean 3rd Qu. Max.\n1    Scroll    1       3      4  4.4    6.00    7\n2    Search    1       3      4  3.6    4.25    5\n\nplyr::ddply(srchscrl, ~ Technique, summarise,\n      Effort.mean=mean(Effort), Effort.sd=sd(Effort))\n\n  Technique Effort.mean Effort.sd\n1    Scroll         4.4  1.698296\n2    Search         3.6  1.187656\n\n\n\nggplot(srchscrl,aes(Effort,fill=Technique)) +\n  geom_histogram(bins=20,alpha=0.9,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n\n\n\nggplot(srchscrl,aes(Technique,Effort,fill=Technique)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Set3\") +\n  geom_dotplot(show.legend=FALSE,binaxis='y',stackdir='center',dotsize=1) +\n  theme_tufte(base_size=8)\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\nOur response is ordinal within-subjects, so use nonparametric Wilcoxon signed-rank.\n\nwilcoxsign_test(Effort ~ Technique | Subject,\n        data=srchscrl, distribution=\"exact\")\n\n\n    Exact Wilcoxon-Pratt Signed-Rank Test\n\ndata:  y by x (pos, neg) \n     stratified by block\nZ = 1.746, p-value = 0.08746\nalternative hypothesis: true mu is not equal to 0"
  },
  {
    "objectID": "week13.html#people-doing-tasks-on-different-phones-in-different-postures-factorial-anova",
    "href": "week13.html#people-doing-tasks-on-different-phones-in-different-postures-factorial-anova",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.5 People doing tasks on different phones in different postures (Factorial ANOVA)",
    "text": "13.5 People doing tasks on different phones in different postures (Factorial ANOVA)\nThe scenario is text entry on smartphone keyboards: iPhone and Galaxy, in different postures: sitting, walking, standing.\nThe statistics employed include Factorial ANOVA, repeated measures ANOVA, main effects, interaction effects, the Aligned Rank Transform for nonparametric ANOVAs.\nThis is a \\(3 \\times 2\\) mixed factorial design. It is mixed in the sense that there is a between-subjects factor (Keyboard) and a within-subjects factor (Posture). It is balanced in the sense that there are twelve persons using each Keyboard and they are each examined for all three levels of Posture.\n\n13.5.1 Read and describe the data\n\nmbltxt &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/mbltxt.csv\"))\nhead(mbltxt)\n\n# A tibble: 6 × 6\n  Subject Keyboard Posture Posture_Order   WPM Error_Rate\n    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1       1 iPhone   Sit                 1  20.2     0.022 \n2       1 iPhone   Stand               2  23.7     0.03  \n3       1 iPhone   Walk                3  20.8     0.0415\n4       2 iPhone   Sit                 1  20.9     0.022 \n5       2 iPhone   Stand               3  23.3     0.0255\n6       2 iPhone   Walk                2  19.1     0.0355\n\nmbltxt &lt;- within(mbltxt, Subject &lt;- as.factor(Subject))\nmbltxt &lt;- within(mbltxt, Keyboard &lt;- as.factor(Keyboard))\nmbltxt &lt;- within(mbltxt, Posture &lt;- as.factor(Posture))\nmbltxt &lt;- within(mbltxt, Posture_Order &lt;- as.factor(Posture_Order))\nsummary(mbltxt)\n\n    Subject     Keyboard   Posture   Posture_Order      WPM        \n 1      : 3   Galaxy:36   Sit  :24   1:24          Min.   : 9.454  \n 2      : 3   iPhone:36   Stand:24   2:24          1st Qu.:19.091  \n 3      : 3               Walk :24   3:24          Median :21.032  \n 4      : 3                                        Mean   :20.213  \n 5      : 3                                        3rd Qu.:23.476  \n 6      : 3                                        Max.   :25.380  \n (Other):54                                                        \n   Error_Rate     \n Min.   :0.01500  \n 1st Qu.:0.02200  \n Median :0.03050  \n Mean   :0.03381  \n 3rd Qu.:0.04000  \n Max.   :0.06950  \n                  \n\n\n\n\n13.5.2 Explore the WPM (words per minute) data\n\ns &lt;- mbltxt |&gt;\n  group_by(Keyboard,Posture) |&gt;\n  summarize(\n    WPM.median=median(WPM),\n    WPM.mean=mean(WPM),\n    WPM.sd=sd(WPM)\n  )\n\n`summarise()` has grouped output by 'Keyboard'. You can override using the\n`.groups` argument.\n\ns\n\n# A tibble: 6 × 5\n# Groups:   Keyboard [2]\n  Keyboard Posture WPM.median WPM.mean WPM.sd\n  &lt;fct&gt;    &lt;fct&gt;        &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Galaxy   Sit           23.8     23.9  0.465\n2 Galaxy   Stand         21.2     21.2  0.810\n3 Galaxy   Walk          12.2     12.1  1.26 \n4 iPhone   Sit           20.9     21.0  0.701\n5 iPhone   Stand         23.8     23.9  0.834\n6 iPhone   Walk          19.1     19.2  1.35 \n\n\n\n\n13.5.3 Histograms for both factors\n\nggplot(mbltxt,aes(WPM,fill=Keyboard)) +\n  geom_histogram(bins=20,alpha=0.9,position=\"dodge\",show.legend=FALSE) +\n  scale_color_brewer() +\n  scale_fill_brewer() +\n  facet_grid(Keyboard~Posture) +\n  theme_tufte(base_size=8)\n\n\n\n\n\n\n13.5.4 Boxplot of both factors\n\nggplot(mbltxt,aes(Keyboard,WPM,fill=Keyboard)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  facet_wrap(~Posture) +\n  theme_tufte(base_size=7)\n\n\n\n\n\n\n13.5.5 An interaction plot\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(mbltxt,\n     interaction.plot(Posture, Keyboard, WPM,\n                      ylim=c(0, max(mbltxt$WPM))))\n\n\n\n\n\n\n13.5.6 Test for a Posture order effect\nThis is to ensure that counterbalancing worked.\n\nlibrary(ez)\nm &lt;- ezANOVA(dv=WPM,\n            between=Keyboard,\n            within=Posture_Order,\n            wid=Subject,\n            data=mbltxt)\nm$Mauchly\n\n                  Effect         W         p p&lt;.05\n3          Posture_Order 0.9912922 0.9122583      \n4 Keyboard:Posture_Order 0.9912922 0.9122583      \n\n\nWikipedia tells us that “Sphericity is an important assumption of a repeated-measures ANOVA. It refers to the condition where the variances of the differences between all possible pairs of within-subject conditions (i.e., levels of the independent variable) are equal. The violation of sphericity occurs when it is not the case that the variances of the differences between all combinations of the conditions are equal. If sphericity is violated, then the variance calculations may be distorted, which would result in an \\(F\\)-ratio that would be inflated.” (from the Wikipedia article on Mauchly’s sphericity test)\nMauchly’s test of sphericity above tells us that there is not a significant departure from sphericity, so we can better rely on the \\(F\\)-statistic in the following ANOVA, the purpose of which is to detect any order effect that would interfere with our later results.\n\nm$ANOVA\n\n                  Effect DFn DFd            F            p p&lt;.05          ges\n2               Keyboard   1  22 1.244151e+02 1.596641e-10     * 0.0794723123\n3          Posture_Order   2  44 5.166254e-02 9.497068e-01       0.0023071128\n4 Keyboard:Posture_Order   2  44 2.830819e-03 9.971734e-01       0.0001266932\n\n\nThe \\(F\\)-statistic for Posture_Order is very small, indicating that there is not an order effect. That gives us the confidence to run the ANOVA test we wanted to run all along."
  },
  {
    "objectID": "week13.html#differences-between-peoples-performance-and-within-a-persons-performance-two-way-mixed-factorial-anova",
    "href": "week13.html#differences-between-peoples-performance-and-within-a-persons-performance-two-way-mixed-factorial-anova",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.6 Differences between people’s performance and within a person’s performance (Two-way mixed factorial ANOVA)",
    "text": "13.6 Differences between people’s performance and within a person’s performance (Two-way mixed factorial ANOVA)\nSince a mixed factorial design by definition has both a between-subjects and a within-subjects factor, we don’t need to also mention that this is a repeated measures test.\n\nm &lt;- ezANOVA(dv=WPM,\n            between=Keyboard,\n            within=Posture,\n            wid=Subject,\n            data=mbltxt)\nm$Mauchly\n\n            Effect         W           p p&lt;.05\n3          Posture 0.6370236 0.008782794     *\n4 Keyboard:Posture 0.6370236 0.008782794     *\n\n\nIn this case, sphericity is violated, so we need to additionally apply the Greenhouse-Geisser correction or the less conservative Huyn-Feldt correction. Nevertheless, let’s look at the uncorrected ANOVA table. Later, we’ll compare it with the uncorrected version provided by the aov() function.\n\nm$ANOVA\n\n            Effect DFn DFd        F            p p&lt;.05       ges\n2         Keyboard   1  22 124.4151 1.596641e-10     * 0.6151917\n3          Posture   2  44 381.4980 1.602465e-28     * 0.9255880\n4 Keyboard:Posture   2  44 157.1600 9.162076e-21     * 0.8367128\n\n\nNote that “ges” in the ANOVA table is the generalized eta-squared measure of effect size, \\(\\eta^2_G\\), preferred to eta-squared or partial eta-squared. See Roger Bakeman (2005) “Recommended effect size statistics for repeated measures designs”, Behavior Research Methods, 37 (3) pages 379–384. There, he points out that the usual \\(\\eta^2\\) is the ratio of effect to total variance:\n\\[\\eta^2=\\frac{SS_{\\text{effect}}}{SS_{\\text{total}}}\\]\nwhere \\(SS\\) is sum of squares. This is similar to the \\(R^2\\) measure typically reported for regression results. The generalized version is alleged to compensate for the deficiencies that \\(\\eta^2\\) shares with \\(R^2\\), mainly that it can be improved by simply adding more predictors. The generalized version looks like this:\n\\[\\eta^2_G=\\frac{SS_{\\text{effect}}}{\\delta \\times SS_{\\text{effect}} + \\sum SS_{\\text{measured}}}\\]\nHere \\(\\delta=0\\) if the effect involves one or more measured factors and \\(\\delta=1\\) if the effect involves only manipulated factors. (Actually it is a little more complicated—here I’m just trying to convey a crude idea that \\(\\eta^2_G\\) ranges between 0 and 1 and that, as it approaches 1, the size of the effect is greater. Oddly enough, it is common to report effect sizes as simply small, medium, or large.)\nNow compute the corrected degrees of freedom for each corrected effect.\n\npos &lt;- match(m$'Sphericity Corrections'$Effect,\n            m$ANOVA$Effect) # positions of within-Ss efx in m$ANOVA\nm$Sphericity$GGe.DFn &lt;- m$Sphericity$GGe * m$ANOVA$DFn[pos] # Greenhouse-Geisser\nm$Sphericity$GGe.DFd &lt;- m$Sphericity$GGe * m$ANOVA$DFd[pos]\nm$Sphericity$HFe.DFn &lt;- m$Sphericity$HFe * m$ANOVA$DFn[pos] # Huynh-Feldt\nm$Sphericity$HFe.DFd &lt;- m$Sphericity$HFe * m$ANOVA$DFd[pos]\nm$Sphericity\n\n            Effect       GGe        p[GG] p[GG]&lt;.05       HFe        p[HF]\n3          Posture 0.7336884 1.558280e-21         * 0.7731517 1.432947e-22\n4 Keyboard:Posture 0.7336884 7.800756e-16         * 0.7731517 1.447657e-16\n  p[HF]&lt;.05  GGe.DFn  GGe.DFd  HFe.DFn  HFe.DFd\n3         * 1.467377 32.28229 1.546303 34.01868\n4         * 1.467377 32.28229 1.546303 34.01868\n\n\nThe above table shows the Greenhouse Geisser correction to the numerator (GGe.DFn) and denominator (GGe.DFd) degrees of freedom and the resulting \\(p\\)-values (p[GG]). The Greenhouse Geiser epsilon statistic (\\(\\epsilon\\)) is shown as GGe. There is an analogous set of measures for the less conservative Huynh-Feldt correction. Note that you could calculate a more conservative \\(F\\)-statistic using the degrees of freedom given even though a corrected \\(F\\)-statistic is not shown for some reason."
  },
  {
    "objectID": "week13.html#anova-results-from-aov",
    "href": "week13.html#anova-results-from-aov",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.7 ANOVA results from aov()",
    "text": "13.7 ANOVA results from aov()\nThe uncorrected results from the ez package are the same as the aov() function in base R, shown below.\n\nm &lt;- aov(WPM ~ Keyboard * Posture + Error(Subject/Posture),\n        data=mbltxt) # fit model\nsummary(m)\n\n\nError: Subject\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nKeyboard   1  96.35   96.35   124.4 1.6e-10 ***\nResiduals 22  17.04    0.77                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Subject:Posture\n                 Df Sum Sq Mean Sq F value Pr(&gt;F)    \nPosture           2  749.6   374.8   381.5 &lt;2e-16 ***\nKeyboard:Posture  2  308.8   154.4   157.2 &lt;2e-16 ***\nResiduals        44   43.2     1.0                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n13.7.1 Manual post hoc pairwise comparisons\nBecause the ANOVA table showed a significant interaction effect and the significance of that interaction effect was borne out by the small p[GG] value, it makes sense to conduct post hoc pairwise comparisons. These require reshaping the data to a wide format because the \\(t\\) test expects data in that format.\n\nmbltxt.wide &lt;- dcast(mbltxt, Subject + Keyboard ~ Posture,\n                    value.var=\"WPM\")\nhead(mbltxt.wide)\n\n  Subject Keyboard     Sit   Stand    Walk\n1       1   iPhone 20.2145 23.7485 20.7960\n2       2   iPhone 20.8805 23.2595 19.1305\n3       3   iPhone 21.2635 23.4945 20.8545\n4       4   iPhone 20.7080 23.9220 18.2575\n5       5   iPhone 21.0075 23.4700 17.7105\n6       6   iPhone 19.9115 24.2975 19.8550\n\nsit &lt;- t.test(mbltxt.wide$Sit ~ Keyboard, data=mbltxt.wide)\nstd &lt;- t.test(mbltxt.wide$Stand ~ Keyboard, data=mbltxt.wide)\nwlk &lt;- t.test(mbltxt.wide$Walk ~ Keyboard, data=mbltxt.wide)\np.adjust(c(sit$p.value, std$p.value, wlk$p.value), method=\"holm\")\n\n[1] 3.842490e-10 4.622384e-08 1.450214e-11\n\n\nThe above \\(p\\)-values indicate significant differences for all three.\n\n\n13.7.2 Compare iPhone ‘sit’ and ‘walk’\n\npar(pin=c(2.75,1.25),cex=0.5)\ntst&lt;-t.test(mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\",]$Sit,\n       mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\",]$Walk,\n       paired=TRUE)\ntst\n\n\n    Paired t-test\n\ndata:  mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\", ]$Sit and mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\", ]$Walk\nt = 3.6259, df = 11, p-value = 0.003985\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.6772808 2.7695525\nsample estimates:\nmean difference \n       1.723417 \n\npar(pin=c(2.75,1.25),cex=0.5)\nboxplot(mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\",]$Sit,\n        mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\",]$Walk,\n        xlab=\"iPhone.Sit vs. iPhone.Walk\", ylab=\"WPM\")"
  },
  {
    "objectID": "week13.html#what-if-anova-assumptions-arent-met-nonparametric-approach-to-factorial-anova",
    "href": "week13.html#what-if-anova-assumptions-arent-met-nonparametric-approach-to-factorial-anova",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.8 What if ANOVA assumptions aren’t met? (Nonparametric approach to factorial ANOVA)",
    "text": "13.8 What if ANOVA assumptions aren’t met? (Nonparametric approach to factorial ANOVA)\nThe rest of this section concerns a nonparametric approach developed at the University of Washington.\n\n13.8.1 The Aligned Rank Transform (ART) procedure\nhttp://depts.washington.edu/aimgroup/proj/art/\n\n\n13.8.2 Explore the Error_Rate data\n\ns &lt;- mbltxt |&gt;\n  group_by(Keyboard,Posture) |&gt;\n  summarize(\n    WPM.median=median(Error_Rate),\n    WPM.mean=mean(Error_Rate),\n    WPM.sd=sd(Error_Rate)\n  )\n\n`summarise()` has grouped output by 'Keyboard'. You can override using the\n`.groups` argument.\n\ns\n\n# A tibble: 6 × 5\n# Groups:   Keyboard [2]\n  Keyboard Posture WPM.median WPM.mean  WPM.sd\n  &lt;fct&gt;    &lt;fct&gt;        &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 Galaxy   Sit         0.019    0.0194 0.00243\n2 Galaxy   Stand       0.0305   0.0307 0.00406\n3 Galaxy   Walk        0.0658   0.0632 0.00575\n4 iPhone   Sit         0.0205   0.0199 0.00248\n5 iPhone   Stand       0.0302   0.0298 0.00258\n6 iPhone   Walk        0.04     0.0399 0.00405\n\n\n\n\n13.8.3 Histograms of Error_Rate\n\nggplot(mbltxt,aes(Error_Rate,fill=Keyboard)) +\n  geom_histogram(bins=20,alpha=0.9,position=\"dodge\",show.legend=FALSE) +\n  scale_color_brewer() +\n  scale_fill_brewer() +\n  facet_grid(Keyboard~Posture) +\n  theme_tufte(base_size=8)\n\n\n\n\n\n\n13.8.4 Box plots of Error_Rate\n\nggplot(mbltxt,aes(Keyboard,Error_Rate,fill=Keyboard)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Greens\") +\n  facet_wrap(~Posture) +\n  theme_tufte(base_size=7)\n\n\n\n\n\n\n13.8.5 Interaction plot of Error_Rate\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(mbltxt,\n     interaction.plot(Posture, Keyboard, Error_Rate,\n                      ylim=c(0, max(mbltxt$Error_Rate))))\n\n\n\n\n\n\n13.8.6 Aligned Rank Transform on Error_Rate\n\nlibrary(ARTool) # for art, artlm\nm &lt;- art(Error_Rate ~ Keyboard * Posture + (1|Subject), data=mbltxt) # uses LMM\nanova(m) # report anova\n\nboundary (singular) fit: see help('isSingular')\n\n\nAnalysis of Variance of Aligned Rank Transformed Data\n\nTable Type: Analysis of Deviance Table (Type III Wald F tests with Kenward-Roger df) \nModel: Mixed Effects (lmer)\nResponse: art(Error_Rate)\n\n                         F Df Df.res     Pr(&gt;F)    \n1 Keyboard          89.450  1     22 3.2959e-09 ***\n2 Posture          274.704  2     44 &lt; 2.22e-16 ***\n3 Keyboard:Posture  78.545  2     44 3.0298e-15 ***\n---\nSignif. codes:   0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\n\n\n\n13.8.7 Examine the normality assumption\n\npar(pin=c(2.75,1.25),cex=0.5)\nshapiro.test(residuals(m)) # normality?\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.98453, p-value = 0.5227\n\nqqnorm(residuals(m)); qqline(residuals(m)) # seems to conform\n\n\n\n\n\n\n13.8.8 Interaction plot\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(mbltxt,\n     interaction.plot(Posture, Keyboard, Error_Rate,\n              ylim=c(0, max(mbltxt$Error_Rate)))) # for convenience\n\n\n\n\n\n\n13.8.9 Conduct post hoc pairwise comparisons within each factor\n\n#. library(emmeans) # instead of lsmeans\n#. for backward compatibility, emmeans provides an lsmeans() function\nlsmeans(artlm(m, \"Keyboard\"), pairwise ~ Keyboard)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n$lsmeans\n Keyboard lsmean   SE df lower.CL upper.CL\n Galaxy     52.3 2.36 22     47.4     57.2\n iPhone     20.7 2.36 22     15.8     25.6\n\nResults are averaged over the levels of: Posture \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n$contrasts\n contrast        estimate   SE df t.ratio p.value\n Galaxy - iPhone     31.6 3.34 22   9.458  &lt;.0001\n\nResults are averaged over the levels of: Posture \nDegrees-of-freedom method: kenward-roger \n\nlsmeans(artlm(m, \"Posture\"), pairwise ~ Posture)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n$lsmeans\n Posture lsmean   SE   df lower.CL upper.CL\n Sit       12.5 1.47 65.9     9.57     15.4\n Stand     36.5 1.47 65.9    33.57     39.4\n Walk      60.5 1.47 65.9    57.57     63.4\n\nResults are averaged over the levels of: Keyboard \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n$contrasts\n contrast     estimate   SE df t.ratio p.value\n Sit - Stand       -24 2.05 44 -11.720  &lt;.0001\n Sit - Walk        -48 2.05 44 -23.439  &lt;.0001\n Stand - Walk      -24 2.05 44 -11.720  &lt;.0001\n\nResults are averaged over the levels of: Keyboard \nDegrees-of-freedom method: kenward-roger \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n#. Warning: don't do the following in ART!\n#lsmeans(artlm(m, \"Keyboard : Posture\"), pairwise ~ Keyboard : Posture)\n\nThe above contrast-testing method is invalid for cross-factor pairwise comparisons in ART. and you can’t just grab aligned-ranks for manual \\(t\\)-tests. instead, use testInteractions() from the phia package to perform “interaction contrasts.” See vignette(\"art-contrasts\").\n\nlibrary(phia)\ntestInteractions(artlm(m, \"Keyboard:Posture\"),\n                 pairwise=c(\"Keyboard\", \"Posture\"), adjustment=\"holm\")\n\nboundary (singular) fit: see help('isSingular')\n\n\nChisq Test: \nP-value adjustment method: holm\n                             Value Df    Chisq Pr(&gt;Chisq)    \nGalaxy-iPhone :  Sit-Stand  -5.083  1   0.5584     0.4549    \nGalaxy-iPhone :   Sit-Walk -76.250  1 125.6340     &lt;2e-16 ***\nGalaxy-iPhone : Stand-Walk -71.167  1 109.4412     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the output, A-B : C-D is interpreted as a difference-of-differences, i.e., the difference between (A-B | C) and (A-B | D). In words, is the difference between A and B significantly different in condition C from condition D?"
  },
  {
    "objectID": "week13.html#experiments-with-interaction-effects",
    "href": "week13.html#experiments-with-interaction-effects",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.9 Experiments with interaction effects",
    "text": "13.9 Experiments with interaction effects\nThis section reports on three experiments with possible interaction effects: Avatars, Notes, and Social media value. To work through the questions, you need the three csv files containing the data: avatars.csv, notes.csv, and socialvalue.csv.\nThese experiments may be between-subjects, within-subjects, or mixed. To be a mixed factorial design, there would have to be at least two independent variables and at least one within-subjects factor and at least one between-subjects factor.\n\n13.9.1 Sentiments about Avatars among males and females (Interaction effects)\nThirty males and thirty females were shown an avatar that was either male or female and asked to write a story about that avatar. The number of positive sentiments in the story were summed. What kind of experimental design is this? [Answer: It is a \\(2\\times 2\\) between-subjects design with factors for Sex (M, F) and Avatar (M, F).]\n\navatars &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/avatars.csv\"))\navatars$Subject &lt;- factor(avatars$Subject)\nsummary(avatars)\n\n    Subject       Sex               Avatar            Positives    \n 1      : 1   Length:60          Length:60          Min.   : 32.0  \n 2      : 1   Class :character   Class :character   1st Qu.: 65.0  \n 3      : 1   Mode  :character   Mode  :character   Median : 84.0  \n 4      : 1                                         Mean   : 85.1  \n 5      : 1                                         3rd Qu.:104.2  \n 6      : 1                                         Max.   :149.0  \n (Other):54                                                        \n\n\nWhat’s the average number of positive sentiments for the most positive combination of Sex and Avatar?\n\nplyr::ddply(avatars,~Sex*Avatar,summarize,\n      Pos.mean=mean(Positives),\n      Pos.sd=sd(Positives))\n\n     Sex Avatar  Pos.mean   Pos.sd\n1 Female Female  63.13333 17.48414\n2 Female   Male  85.20000 25.31008\n3   Male Female 100.73333 18.72152\n4   Male   Male  91.33333 19.66384\n\n\nCreate an interaction plot with Sex on the X-Axis and Avatar as the traces. Do the lines cross? Do the same for reversed axes.\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(avatars,interaction.plot(Sex,Avatar,Positives,\n                  ylim=c(0,max(avatars$Positives))))\n\n\n\nwith(avatars,interaction.plot(Avatar,Sex,Positives,\n                  ylim=c(0,max(avatars$Positives))))\n\n\n\n\nConduct a factorial ANOVA on Positives by Sex and Avatar and report the largest \\(F\\)-statistic. Report which effects are significant.\n\nm&lt;-ezANOVA(dv=Positives,between=c(Sex,Avatar),\n       wid=Subject,data=avatars)\n\nWarning: Converting \"Sex\" to factor for ANOVA.\n\n\nWarning: Converting \"Avatar\" to factor for ANOVA.\n\n\nCoefficient covariances computed by hccm()\n\nm$ANOVA\n\n      Effect DFn DFd         F            p p&lt;.05        ges\n1        Sex   1  56 17.041756 0.0001228287     * 0.23331526\n2     Avatar   1  56  1.429598 0.2368686270       0.02489305\n3 Sex:Avatar   1  56  8.822480 0.0043757511     * 0.13610216\n\n\nConduct planned pairwise comparisons using independent-samples \\(t\\)-tests. Ask whether females produced different numbers of positive sentiments for male vs female avatars. Then ask whether males did the same. Assume equal variances and use Holm’s sequential Bonferroni procedure to correct for multiple comparisons.\n\nf&lt;-t.test(avatars[avatars$Sex==\"Female\" & avatars$Avatar==\"Male\",]$Positives,\n      avatars[avatars$Sex==\"Female\" & avatars$Avatar==\"Female\",]$Positives,\n      var.equal=TRUE)\nf\n\n\n    Two Sample t-test\n\ndata:  avatars[avatars$Sex == \"Female\" & avatars$Avatar == \"Male\", ]$Positives and avatars[avatars$Sex == \"Female\" & avatars$Avatar == \"Female\", ]$Positives\nt = 2.7782, df = 28, p-value = 0.009647\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  5.796801 38.336533\nsample estimates:\nmean of x mean of y \n 85.20000  63.13333 \n\nm&lt;-t.test(avatars[avatars$Sex==\"Male\" & avatars$Avatar==\"Male\",]$Positives,\n      avatars[avatars$Sex==\"Male\" & avatars$Avatar==\"Female\",]$Positives,\n      var.equal=TRUE)\nm\n\n\n    Two Sample t-test\n\ndata:  avatars[avatars$Sex == \"Male\" & avatars$Avatar == \"Male\", ]$Positives and avatars[avatars$Sex == \"Male\" & avatars$Avatar == \"Female\", ]$Positives\nt = -1.3409, df = 28, p-value = 0.1907\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -23.759922   4.959922\nsample estimates:\nmean of x mean of y \n 91.33333 100.73333 \n\np.adjust(c(f$p.value,m$p.value),method=\"holm\")\n\n[1] 0.01929438 0.19073468\n\n\n\n\n13.9.2 Writing notes with builtin or addon apps on two phones (mixed factorial design)\nThe notes.csv file describes a study in which iPhone and Android owners used a built-in note-taking app then a third-party note-taking app or vice versa. What kind of experimental design is this? (Answer: A \\(2 \\times 2\\) mixed factorial design with a between-subjects factor for Phone (iPhone, Android) and a within-subjects factor for Notes (Built-in, Add-on).)\n\nnotes &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/notes.csv\"))\nnotes$Subject&lt;-factor(notes$Subject)\nnotes$Order&lt;-factor(notes$Order)\nsummary(notes)\n\n    Subject      Phone              Notes           Order      Words      \n 1      : 2   Length:40          Length:40          1:20   Min.   :259.0  \n 2      : 2   Class :character   Class :character   2:20   1st Qu.:421.8  \n 3      : 2   Mode  :character   Mode  :character          Median :457.0  \n 4      : 2                                                Mean   :459.2  \n 5      : 2                                                3rd Qu.:518.5  \n 6      : 2                                                Max.   :598.0  \n (Other):28                                                               \n\n\nWhat’s the average number of words recorded for the most heavily used combination of Phone and Notes?\n\nplyr::ddply(notes, ~Phone*Notes,summarize,\n         Words.mean=mean(Words),Words.sd=sd(Words))\n\n    Phone    Notes Words.mean Words.sd\n1 Android   Add-on      388.1 42.38828\n2 Android Built-in      410.9 77.49043\n3  iPhone   Add-on      504.2 47.29529\n4  iPhone Built-in      533.7 48.04176\n\n\nCreate an interaction plot with Phone on the X-Axis and Notes as the traces. Do the lines cross? Do the same for reversed axes.\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(notes,interaction.plot(Phone,Notes,Words,\n                ylim=c(0,max(notes$Words))))\n\n\n\nwith(notes,interaction.plot(Notes,Phone,Words,\n                ylim=c(0,max(notes$Words))))\n\n\n\n\nTest for an order effect in the presentation of order of the Notes factor. Report the \\(p\\)-value.\n\nm&lt;-ezANOVA(dv=Words,between=Phone,within=Order,wid=Subject,data=notes)\n\nWarning: Converting \"Phone\" to factor for ANOVA.\n\nm$ANOVA\n\n       Effect DFn DFd          F            p p&lt;.05        ges\n2       Phone   1  18 43.5625695 3.375888e-06     * 0.56875437\n3       Order   1  18  0.5486763 4.684126e-01       0.01368098\n4 Phone:Order   1  18  3.0643695 9.705122e-02       0.07189858\n\n\nConduct a factorial ANOVA on Words by Phone and Notes. Report the largest \\(F\\)-statistic.\n\nm&lt;-ezANOVA(dv=Words,between=Phone,within=Notes,wid=Subject,data=notes)\n\nWarning: Converting \"Notes\" to factor for ANOVA.\n\n\nWarning: Converting \"Phone\" to factor for ANOVA.\n\nm$ANOVA\n\n       Effect DFn DFd           F            p p&lt;.05         ges\n2       Phone   1  18 43.56256949 3.375888e-06     * 0.562185697\n3       Notes   1  18  2.35976941 1.418921e-01       0.057972811\n4 Phone:Notes   1  18  0.03872717 8.461951e-01       0.001008948\n\n\nConduct paired-samples \\(t\\)-tests to answer two questions. First, did iPhone user enter different numbers of words using the built-in notes app versus the add-on notes app? Second, same for Android. Assume equal variances and use Holm’s sequential Bonferroni procedure to correct for multiple comparisons. Report the lowest adjusted \\(p\\)-value.\n\nnotes.wide&lt;-dcast(notes,Subject+Phone~Notes,value.var=\"Words\")\nhead(notes.wide)\n\n  Subject   Phone Add-on Built-in\n1       1  iPhone    464      561\n2       2 Android    433      428\n3       3  iPhone    598      586\n4       4 Android    347      448\n5       5  iPhone    478      543\n6       6 Android    365      445\n\ni&lt;-t.test(notes.wide[notes.wide$Phone==\"iPhone\",]$'Add-on',\n      notes.wide[notes.wide$Phone==\"iPhone\",]$'Built-in',\n      paired=TRUE,var.equal=TRUE)\ni\n\n\n    Paired t-test\n\ndata:  notes.wide[notes.wide$Phone == \"iPhone\", ]$\"Add-on\" and notes.wide[notes.wide$Phone == \"iPhone\", ]$\"Built-in\"\nt = -1.8456, df = 9, p-value = 0.09804\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -65.658758   6.658758\nsample estimates:\nmean difference \n          -29.5 \n\na&lt;-t.test(notes.wide[notes.wide$Phone==\"Android\",]$'Add-on',\n      notes.wide[notes.wide$Phone==\"Android\",]$'Built-in',\n      paired=TRUE,var.equal=TRUE)\na\n\n\n    Paired t-test\n\ndata:  notes.wide[notes.wide$Phone == \"Android\", ]$\"Add-on\" and notes.wide[notes.wide$Phone == \"Android\", ]$\"Built-in\"\nt = -0.75847, df = 9, p-value = 0.4676\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -90.80181  45.20181\nsample estimates:\nmean difference \n          -22.8 \n\np.adjust(c(i$p.value,a$p.value),method=\"holm\")\n\n[1] 0.1960779 0.4675674\n\n\n\n\n13.9.3 Social media value judged by people after watching clips (two-by-two within subject design)\nThe file socialvalue.csv describes a study of people viewing a pos or neg film clip then going onto social media and judging the value of the first 100 posts they see. The number of valued posts was recorded. What kind of experimental design is this? [Answer: A \\(2\\times 2\\) within-subject design with factors for Clip (positive, negative) and Social (Facebook, Twitter).]\n\nsv &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/socialvalue.csv\"))\nsv$Subject&lt;-factor(sv$Subject)\nsv$Clip&lt;-factor(sv$Clip)\nsv$Social&lt;-factor(sv$Social)\nsv$ClipOrder&lt;-factor(sv$ClipOrder)\nsv$SocialOrder&lt;-factor(sv$SocialOrder)\nsummary(sv)\n\n    Subject         Clip    ClipOrder      Social   SocialOrder     Valued    \n 1      : 4   negative:32   1:32      Facebook:32   1:32        Min.   :13.0  \n 2      : 4   positive:32   2:32      Twitter :32   2:32        1st Qu.:52.0  \n 3      : 4                                                     Median :56.0  \n 4      : 4                                                     Mean   :57.3  \n 5      : 4                                                     3rd Qu.:67.0  \n 6      : 4                                                     Max.   :95.0  \n (Other):40                                                                   \n\n\nWhat’s the average number of valued posts for the most valued combination of Clip and Social?\n\nplyr::ddply(sv, ~Clip*Social,summarize, Valued.mean=mean(Valued),\n      Valued.sd=sd(Valued))\n\n      Clip   Social Valued.mean Valued.sd\n1 negative Facebook     46.3125 22.285178\n2 negative  Twitter     55.5625  5.032809\n3 positive Facebook     68.7500 21.151832\n4 positive  Twitter     58.5625  5.656486\n\n\nCreate an interaction plot with Social on the \\(X\\)-Axis and Clip as the traces. Do the lines cross? Do the same for reversed axes.\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(sv,interaction.plot(Social,Clip,Valued,\n             ylim=c(0,max(sv$Valued))))\n\n\n\nwith(sv,interaction.plot(Clip,Social,Valued,\n             ylim=c(0,max(sv$Valued))))\n\n\n\n\nTest for an order effect in the presentation of order of the ClipOrder or SocialOrder factor. Report the \\(p\\)-values.\n\nm&lt;-ezANOVA(dv=Valued,within=c(ClipOrder,SocialOrder),wid=Subject,data=sv)\nm$ANOVA\n\n                 Effect DFn DFd          F         p p&lt;.05          ges\n2             ClipOrder   1  15 0.93707354 0.3483818       0.0253831509\n3           SocialOrder   1  15 0.81236528 0.3816660       0.0143842018\n4 ClipOrder:SocialOrder   1  15 0.01466581 0.9052172       0.0001913088\n\n\nConduct a factorial ANOVA on Valued by Clip and Social. Report the largest \\(F\\)-statistic.\n\nm&lt;-ezANOVA(dv=Valued,within=c(Clip,Social),wid=Subject,data=sv)\nm$ANOVA\n\n       Effect DFn DFd          F          p p&lt;.05          ges\n2        Clip   1  15 6.99533219 0.01837880     * 0.1469889054\n3      Social   1  15 0.01466581 0.90521722       0.0002340033\n4 Clip:Social   1  15 6.11355984 0.02586779     * 0.0914169000\n\n\nConduct paired-samples \\(t\\)-tests to answer two questions. First, on Facebook, were the number of valued posts different after watching a positive or negative clip. Second, same on Twitter. Assume equal variances and use Holm’s sequential Bonferroni procedure to correct for multiple comparisons. Report the lowest adjusted \\(p\\)-value.\n\nsv.wide&lt;-dcast(sv,Subject+Social~Clip,value.var=\"Valued\")\nhead(sv.wide)\n\n  Subject   Social negative positive\n1       1 Facebook       38       85\n2       1  Twitter       52       53\n3       2 Facebook       73       25\n4       2  Twitter       52       54\n5       3 Facebook       25       95\n6       3  Twitter       53       70\n\nf&lt;-t.test(sv.wide[sv.wide$Social==\"Facebook\",]$positive,\n      sv.wide[sv.wide$Social==\"Facebook\",]$negative,\n      paired=TRUE,var.equal=TRUE)\nf\n\n\n    Paired t-test\n\ndata:  sv.wide[sv.wide$Social == \"Facebook\", ]$positive and sv.wide[sv.wide$Social == \"Facebook\", ]$negative\nt = 2.5929, df = 15, p-value = 0.02039\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n  3.993 40.882\nsample estimates:\nmean difference \n        22.4375 \n\nt&lt;-t.test(sv.wide[sv.wide$Social==\"Twitter\",]$positive,\n      sv.wide[sv.wide$Social==\"Twitter\",]$negative,\n      paired=TRUE,var.equal=TRUE)\nt\n\n\n    Paired t-test\n\ndata:  sv.wide[sv.wide$Social == \"Twitter\", ]$positive and sv.wide[sv.wide$Social == \"Twitter\", ]$negative\nt = 1.9926, df = 15, p-value = 0.06482\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.2089939  6.2089939\nsample estimates:\nmean difference \n              3 \n\np.adjust(c(f$p.value,t$p.value),method=\"holm\")\n\n[1] 0.04077153 0.06482275\n\n\nConduct a nonparametric Aligned Rank Transform Procedure on Valued by Clip and Social.\n\nm&lt;-art(Valued~Clip*Social+(1|Subject),data=sv)\nanova(m)\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nAnalysis of Variance of Aligned Rank Transformed Data\n\nTable Type: Analysis of Deviance Table (Type III Wald F tests with Kenward-Roger df) \nModel: Mixed Effects (lmer)\nResponse: art(Valued)\n\n                     F Df Df.res     Pr(&gt;F)    \n1 Clip        17.13224  1     45 0.00015089 ***\n2 Social       0.49281  1     45 0.48629341    \n3 Clip:Social 11.31751  1     45 0.00157736  **\n---\nSignif. codes:   0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\n\nConduct interaction contrasts to discover whether the difference on Facebook was itself different from the difference on Twitter. Report the \\(\\chi^2\\) statistic.\n\ntestInteractions(artlm(m,\"Clip:Social\"),\n         pairwise=c(\"Clip\",\"Social\"),adjustment=\"holm\")\n\nboundary (singular) fit: see help('isSingular')\n\n\nChisq Test: \nP-value adjustment method: holm\n                                      Value Df  Chisq Pr(&gt;Chisq)    \nnegative-positive : Facebook-Twitter -29.25  1 11.318  0.0007678 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "week13.html#what-if-errors-are-not-normally-distributed-generalized-linear-models",
    "href": "week13.html#what-if-errors-are-not-normally-distributed-generalized-linear-models",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.10 What if errors are not normally distributed? (Generalized linear models)",
    "text": "13.10 What if errors are not normally distributed? (Generalized linear models)\nHere are three examples of generalized linear models. The first is analyzed using nominal logistic regression, the second is analyzed via ordinal logistic regression, and the third is analyzed via Poisson regression.\nAs Wikipedia tells us, a generalized linear model or GLM is a flexible generalization of ordinary linear regression that allows for response variables with error distribution models other than a normal distribution. There is also something called a general linear model but it is not the same thing as a generalized linear model. It is just the general form of the ordinary linear regression model: \\(\\mathbfit{Y=X\\beta+\\epsilon}\\).\nGLMs that we examine here are good for between-subjects studies so we’ll actually recode one of our fictitious data sets to be between subjects just to have an example to use.\n\n13.10.1 Preferences among websites by males and females (GLM 1: Nominal logistic regression for preference responses)\n\n\n13.10.2 Multinomial distribution with logit link function\nThe prefsABCsex.csv file records preferences among three websites A, B, and C expressed by males and females. The subject number, preference and sex were recorded.\nThe logit link function is the log odds function, generally \\(\\text{logit}(p)=\\ln \\frac{p}{1-p}\\), where \\(p\\) is the probability of an event such as choosing website A. The form of the link function is \\(\\mathbfit{X\\beta}=\\ln\\frac{\\mu}{1-\\mu}\\). This is just the relationship of a matrix of predictors times a vector of parameters \\(\\mathbfit{\\beta}\\) to the logit of the mean of the distribution.\n\nprefsABCsex &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/prefsABCsex.csv\"))\nhead(prefsABCsex)\n\n# A tibble: 6 × 3\n  Subject Pref  Sex  \n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1       1 C     F    \n2       2 C     M    \n3       3 B     M    \n4       4 C     M    \n5       5 C     M    \n6       6 B     F    \n\nprefsABCsex$Subject&lt;-factor(prefsABCsex$Subject)\nprefsABCsex$Sex&lt;-factor(prefsABCsex$Sex)\nsummary(prefsABCsex)\n\n    Subject       Pref           Sex   \n 1      : 1   Length:60          F:29  \n 2      : 1   Class :character   M:31  \n 3      : 1   Mode  :character         \n 4      : 1                            \n 5      : 1                            \n 6      : 1                            \n (Other):54                            \n\nggplot(prefsABCsex[prefsABCsex$Sex == \"M\",],aes(Pref)) +\n  theme_tufte() +\n  geom_bar(width=0.25,fill=\"gray\") +\n  geom_hline(yintercept=seq(0, 20, 5), col=\"white\", lwd=1) +\n  annotate(\"text\", x = 1.5, y = 18, adj=1,  family=\"serif\",\n    label = c(\"Males prefer\\nwebsite C\"))\n\n\n\nggplot(prefsABCsex[prefsABCsex$Sex == \"F\",],aes(Pref)) +\n  theme_tufte() +\n  geom_bar(width=0.25,fill=\"gray\") +\n  geom_hline(yintercept=seq(0, 20, 5), col=\"white\", lwd=1) +\n  annotate(\"text\", x = 1.5, y = 18, adj=1,  family=\"serif\",\n    label = c(\"Females dislike\\nwebsite A\"))\n\n\n\n\nThese histograms lead us to suspect that C is preferred by males and that A is disliked by females, but we should still run tests to be convinced that the variability observed is not due to chance.\nAnalyze Pref by Sex using multinomial logistic regression, aka nominal logistic regression. Here we are testing for whether there is a difference between the sexes regarding their preferences.\nThe annotation type=3 is borrowed from SAS and refers to one of three ways of handling an unbalanced design. This experimental design is unbalanced because there are more males than females being tested. This way of handling the unbalanced design is only valid if there are significant interactions, as hinted by the gross differences between the preceding histograms.\n\nlibrary(nnet) # provides multinom()\n#. library(car) # provides Anova()\n#. set sum-to-zero contrasts for the Anova call\ncontrasts(prefsABCsex$Sex) &lt;- \"contr.sum\"\nm&lt;-multinom(Pref~Sex, data=prefsABCsex)\n\n# weights:  9 (4 variable)\ninitial  value 65.916737 \niter  10 value 55.099353\niter  10 value 55.099353\nfinal  value 55.099353 \nconverged\n\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Pref\n    LR Chisq Df Pr(&gt;Chisq)  \nSex   7.0744  2    0.02909 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Analysis of Deviance table tells us that there is a significant main effect for Sex. It does not tell us more detail but motivates pairwise tests to get more detail. If there were no significant effect, pairwise tests would not be warranted.\nPairwise tests tell which of the bins are over or under populated based on the assumption that each bin should contain one third of the observations (hence p=1/3). When making multiple comparisons we would overstate the significance of the differences so we use Holm’s sequential Bonferroni procedure to correct this.\n\nma&lt;-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"A\"),\n           nrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\nmb&lt;-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"B\"),\n           nrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\nmc&lt;-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"C\"),\n           nrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\n#. correct for multiple comparisons\np.adjust(c(ma$p.value, mb$p.value, mc$p.value), method=\"holm\")\n\n[1] 0.109473564 0.126622172 0.001296754\n\nfa&lt;-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"A\"),\n           nrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\nfb&lt;-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"B\"),\n           nrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\nfc&lt;-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"C\"),\n           nrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\n#. correct for multiple comparisons\np.adjust(c(fa$p.value, fb$p.value, fc$p.value), method=\"holm\")\n\n[1] 0.02703274 0.09447821 0.69396951\n\n\nThe preceding tests confirm what we suspected from looking at histograms: males prefer C and females dislike A. We see this by looking at the adjusted \\(p\\)-values, where the first row, third value is significant and the second row, first value is significant.\nHow would we write this up in a report? We could make the following claim. We tested the main effect for sex and found a significant result, \\(\\chi^2_2=7.1, p&lt;0.05\\). An exact binomial test found the preference among males for website C greater than chance, \\(p&lt;0.01\\). An exact binomial test found the preference among females against website A greater than chance, \\(p&lt;0.05\\). No other significant differences were found.\n\n\n13.10.3 Judgments of perceived effort (GLM 2: Ordinal logistic regression for Likert responses)\n\n\n13.10.4 Multinomial distribution with cumulative logit link function\nIn this example, users are either searching, scrolling or using voice to find contacts in a smartphone address book. The time it takes to find a certain number of contacts, the perceived effort, and the number of errors are all recorded. Of interest now is the perceived effort, recorded on a Likert scale. A Likert scale can not be normally distributed because of the restrictions on the ends and is not likely to even look vaguely normal.\nThe cumulative logit link function is like the logit link function:\n\\[\\text{logit}(P(Y\\leqslant j|x))=\\ln\\frac{P(Y\\leqslant j|x)}{1-P(Y\\leqslant j|x)} \\text{ where }Y=1,2,\\ldots,J\\]\nIn this case \\(J\\) ranges from 1 to 7.\nRead in the data and examine it. We see that it is a within-subjects study but it is a fictitious study anyway so we will recode it as if it were a between-subjects study. Then we will be able to apply the following techniques, which we would have to modify for a within-subjects study.\n\nsrchscrlvce &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/srchscrlvce.csv\"))\nhead(srchscrlvce)\n\n# A tibble: 6 × 6\n  Subject Technique Order  Time Errors Effort\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1       1 Search        1    98      4      5\n2       1 Scroll        2   152      0      6\n3       2 Search        2    57      2      2\n4       2 Scroll        1   148      0      3\n5       3 Search        1    86      3      2\n6       3 Scroll        2   160      0      4\n\nsrchscrlvce$Subject&lt;-(1:nrow(srchscrlvce)) # recode as between-subjects\nsrchscrlvce$Subject&lt;-factor(srchscrlvce$Subject)\nsrchscrlvce$Technique&lt;-factor(srchscrlvce$Technique)\nsrchscrlvce$Order&lt;-NULL # drop order, n/a for between-subjects\nhead(srchscrlvce) # verify\n\n# A tibble: 6 × 5\n  Subject Technique  Time Errors Effort\n  &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1       Search       98      4      5\n2 2       Scroll      152      0      6\n3 3       Search       57      2      2\n4 4       Scroll      148      0      3\n5 5       Search       86      3      2\n6 6       Scroll      160      0      4\n\nsummary(srchscrlvce)\n\n    Subject    Technique       Time           Errors         Effort    \n 1      : 1   Scroll:20   Min.   : 49.0   Min.   :0.00   Min.   :1.00  \n 2      : 1   Search:20   1st Qu.: 86.0   1st Qu.:1.00   1st Qu.:3.00  \n 3      : 1   Voice :20   Median : 97.0   Median :2.00   Median :4.00  \n 4      : 1               Mean   :106.2   Mean   :2.75   Mean   :4.15  \n 5      : 1               3rd Qu.:128.0   3rd Qu.:4.00   3rd Qu.:5.00  \n 6      : 1               Max.   :192.0   Max.   :9.00   Max.   :7.00  \n (Other):54                                                            \n\n\nA good description of Effort is the median and quantiles. Another good description is the mean and standard deviation.\n\nplyr::ddply(srchscrlvce, ~ Technique,\n       function(data) summary(data$Effort))\n\n  Technique Min. 1st Qu. Median Mean 3rd Qu. Max.\n1    Scroll    1    3.00      4 4.40    6.00    7\n2    Search    1    3.00      4 3.60    4.25    5\n3     Voice    1    3.75      5 4.45    5.25    6\n\nplyr::ddply(srchscrlvce, ~ Technique,\n       summarize, Effort.mean=mean(Effort), Effort.sd=sd(Effort))\n\n  Technique Effort.mean Effort.sd\n1    Scroll        4.40  1.698296\n2    Search        3.60  1.187656\n3     Voice        4.45  1.356272\n\npar(cex=0.6)\nggplot(srchscrlvce,aes(Effort,fill=Technique)) +\n  geom_histogram(bins=7,alpha=0.8,position=\"dodge\") +\n  scale_color_grey() +\n  scale_fill_grey() +\n  theme_tufte()\n\n\n\nggplot(srchscrlvce,aes(Technique,Effort,fill=Technique)) +\n  geom_tufteboxplot(show.legend=FALSE) +\n  theme_tufte()\n\nWarning: The following aesthetics were dropped during statistical transformation: y\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\nWarning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\nThe boxplots (these are Tufte-style boxplots) are not encouraging. We may not find a significant difference among these three techniques but let us try anyway. We analyze Effort Likert ratings by Technique using ordinal logistic regression.\n\n#. library(MASS) # provides polr()\n#. library(car) # provides Anova()\nsrchscrlvce$Effort &lt;- ordered(srchscrlvce$Effort)\n#. set sum-to-zero contrasts for the Anova call\ncontrasts(srchscrlvce$Technique) &lt;- \"contr.sum\"\nm &lt;- polr(Effort ~ Technique, data=srchscrlvce, Hess=TRUE) # ordinal logistic\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Effort\n          LR Chisq Df Pr(&gt;Chisq)\nTechnique   4.5246  2     0.1041\n\n\nPost hoc pairwise comparisons are NOT justified due to lack of significance but here’s how we would do them, just for completeness. Tukey means to compare all pairs and holm is the adjustment due to the double-counting that overstates the significance.\n\nsummary(glht(m, mcp(Technique=\"Tukey\")), test=adjusted(type=\"holm\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: polr(formula = Effort ~ Technique, data = srchscrlvce, Hess = TRUE)\n\nLinear Hypotheses:\n                      Estimate Std. Error z value Pr(&gt;|z|)\nSearch - Scroll == 0 -1.016610   0.584614  -1.739    0.191\nVoice - Scroll == 0   0.007397   0.587700   0.013    0.990\nVoice - Search == 0   1.024007   0.552298   1.854    0.191\n(Adjusted p values reported -- holm method)\n\n\nHow would we express this in a report? We would simply say that we found no significant differences between the three techniques.\n\n\n13.10.5 Counting errors in a task (GLM 3: Poisson regression for count responses)\n\n\n13.10.6 Poisson distribution with log link function\nUsing the same data but now focus on the Errors column instead of effort. Errors likely have a Poisson distribution. The log link function is just \\(\\mathbfit{X\\beta}=\\ln(\\mu)\\) rather than the more elaborate logit link function we saw before.\n\nplyr::ddply(srchscrlvce, ~ Technique,\n             function(data) summary(data$Errors))\n\n  Technique Min. 1st Qu. Median Mean 3rd Qu. Max.\n1    Scroll    0    0.00    0.5 0.70    1.00    2\n2    Search    1    2.00    2.5 2.50    3.00    4\n3     Voice    2    3.75    5.0 5.05    6.25    9\n\nplyr::ddply(srchscrlvce, ~ Technique, summarize,\n             Errors.mean=mean(Errors), Errors.sd=sd(Errors))\n\n  Technique Errors.mean Errors.sd\n1    Scroll        0.70 0.8013147\n2    Search        2.50 1.0513150\n3     Voice        5.05 1.9049796\n\npar(cex=0.6)\nggplot(srchscrlvce,aes(Errors,fill=Technique)) +\n  geom_histogram(bins=9,alpha=0.8,position=\"dodge\") +\n  scale_color_grey() +\n  scale_fill_grey() +\n  theme_tufte()\n\n\n\nggplot(srchscrlvce,aes(Technique,Errors,fill=Technique)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Greens\") +\n  theme_tufte()\n\n\n\n\nThese boxplots are very encouraging. There appears to be a clear difference between all three of these techniques. Notice that you could draw horizontal lines across the plot without intersecting the boxes. That represents a high degree of separation.\nNow verify that these data are Poisson-distributed with a goodness-of-fit test for each technique. If the results are not significant, we expect that the data do not deviate significantly from what we would expect of a Poisson distribution.\n\n#. library(fitdistrplus)\nfit&lt;-fitdist(srchscrlvce[srchscrlvce$Technique == \"Search\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n\nChi-squared statistic:  1.522231 \nDegree of freedom of the Chi-squared distribution:  2 \nChi-squared p-value:  0.4671449 \n   the p-value may be wrong with some theoretical counts &lt; 5  \nChi-squared table:\n     obscounts theocounts\n&lt;= 1  4.000000   5.745950\n&lt;= 2  6.000000   5.130312\n&lt;= 3  6.000000   4.275260\n&gt; 3   4.000000   4.848477\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   65.61424\nBayesian Information Criterion   66.60997\n\nfit&lt;-fitdist(srchscrlvce[srchscrlvce$Technique == \"Scroll\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n\nChi-squared statistic:  0.3816087 \nDegree of freedom of the Chi-squared distribution:  1 \nChi-squared p-value:  0.5367435 \n   the p-value may be wrong with some theoretical counts &lt; 5  \nChi-squared table:\n     obscounts theocounts\n&lt;= 0 10.000000   9.931706\n&lt;= 1  6.000000   6.952194\n&gt; 1   4.000000   3.116100\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   45.53208\nBayesian Information Criterion   46.52781\n\nfit&lt;-fitdist(srchscrlvce[srchscrlvce$Technique == \"Voice\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n\nChi-squared statistic:  0.1611327 \nDegree of freedom of the Chi-squared distribution:  3 \nChi-squared p-value:  0.9836055 \n   the p-value may be wrong with some theoretical counts &lt; 5  \nChi-squared table:\n     obscounts theocounts\n&lt;= 3  5.000000   5.161546\n&lt;= 4  4.000000   3.473739\n&lt;= 5  3.000000   3.508476\n&lt;= 6  3.000000   2.952967\n&gt; 6   5.000000   4.903272\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   84.19266\nBayesian Information Criterion   85.18839\n\n\nAll three of the above goodness of fit tests tell us that there is no evidence of deviation from a Poisson distribution. Since we are now convinced of the Poisson distribution for each of the three techniques, analyze the errors using Poisson regression.\nWe’ve been saying “set sum-to-zero contrasts for the Anova call” but what does that mean? Contrasts are linear combinations used in ANOVA. As Wikipedia defines it, a contrast is a linear combination \\(\\sum^t_{i=1}a_i\\theta_i\\), where each \\(\\theta_i\\) is a statistic and the \\(a_i\\) values sum to zero. Typically, the \\(a_i\\) values are \\(1\\) and \\(-1\\). A simple contrast represents a difference between means and is used in ANOVA. In R, they are invisible if you use Type I ANOVA, but have to be specified as follows if using a Type III ANOVA. The default anova() function is Type I but we’re using Type III, available from the Anova() function in the car package.\nA minor detail is that we don’t really need to use Anova() here instead of anova() because the study is balanced, meaning that it has the same number of observations in each condition. The only reason for using Anova() on this data is that it gives a better-looking output. The anova() function would just display the \\(\\chi^2\\) statistic without the associated \\(p\\)-value.\n\ncontrasts(srchscrlvce$Technique) &lt;- \"contr.sum\"\n#. family parameter identifies both distribution and link fn\nm &lt;- glm(Errors ~ Technique, data=srchscrlvce, family=poisson)\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Errors\n          LR Chisq Df Pr(&gt;Chisq)    \nTechnique    74.93  2  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBecause the Analysis of Deviance table shows a significant \\(\\chi^2\\) value and corresponding \\(p\\)-value, we are justified to conduct pairwise comparisons among levels of Technique.\n\n#. library(multcomp)\nsummary(glht(m, mcp(Technique=\"Tukey\")), test=adjusted(type=\"holm\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: glm(formula = Errors ~ Technique, family = poisson, data = srchscrlvce)\n\nLinear Hypotheses:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \nSearch - Scroll == 0   1.2730     0.3024   4.210 5.11e-05 ***\nVoice - Scroll == 0    1.9761     0.2852   6.929 1.27e-11 ***\nVoice - Search == 0    0.7031     0.1729   4.066 5.11e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\nWe see from the table that all three differences are significant. We could have guessed this result from glancing at the boxplot above, but it is valuable to have statistical evidence that this is not a chance difference."
  },
  {
    "objectID": "week13.html#more-experiments-without-normally-distributed-errors-more-generalized-linear-models",
    "href": "week13.html#more-experiments-without-normally-distributed-errors-more-generalized-linear-models",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.11 More experiments without normally distributed errors (More generalized linear models)",
    "text": "13.11 More experiments without normally distributed errors (More generalized linear models)\n\n13.11.1 Preference between touchpads vs trackballs by non / disabled people and males / females\nThis study examines whether participants of either sex with or without a disability prefer touchpads or trackballs. Start by examining the data and determining how many participants are involved.\n\ndps &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/deviceprefssex.csv\"))\ndps$Subject&lt;-as.factor(dps$Subject)\ndps$Disability&lt;-as.factor(dps$Disability)\ndps$Sex&lt;-as.factor(dps$Sex)\ndps$Pref&lt;-as.factor(dps$Pref)\nsummary(dps)\n\n    Subject   Disability Sex           Pref   \n 1      : 1   0:18       F:15   touchpad :21  \n 2      : 1   1:12       M:15   trackball: 9  \n 3      : 1                                   \n 4      : 1                                   \n 5      : 1                                   \n 6      : 1                                   \n (Other):24                                   \n\n\nUse binomial regression to examine Pref by Disability and Sex. Report the \\(p\\)-value of the interaction of Disability\\(\\times\\)Sex.\n\ncontrasts(dps$Disability) &lt;- \"contr.sum\"\ncontrasts(dps$Sex) &lt;- \"contr.sum\"\nm&lt;-glm(Pref ~ Disability*Sex, data=dps, family=binomial)\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Pref\n               LR Chisq Df Pr(&gt;Chisq)   \nDisability      10.4437  1   0.001231 **\nSex              2.8269  1   0.092695 . \nDisability:Sex   0.6964  1   0.403997   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow use multinomial regression for the same task and report the corresponding \\(p\\)-value.\n\n#. library(nnet)\ncontrasts(dps$Disability) &lt;- \"contr.sum\"\ncontrasts(dps$Sex) &lt;- \"contr.sum\"\nm&lt;-multinom(Pref~Disability*Sex, data=dps)\n\n# weights:  5 (4 variable)\ninitial  value 20.794415 \niter  10 value 13.023239\niter  20 value 13.010200\nfinal  value 13.010184 \nconverged\n\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Pref\n               LR Chisq Df Pr(&gt;Chisq)   \nDisability      10.4434  1   0.001231 **\nSex              2.8267  1   0.092710 . \nDisability:Sex   0.6961  1   0.404087   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow conduct post-hoc binomial tests for each Disability\\(\\times\\)Sex combination.\n\nm0&lt;-binom.test(sum(dps[dps$Sex == \"M\" & dps$Disability == \"0\",]$Pref == \"touchpad\"),\n               nrow(dps[dps$Sex == \"M\" & dps$Disability == \"0\",]),p=1/2)\nm1&lt;-binom.test(sum(dps[dps$Sex == \"M\" & dps$Disability == \"1\",]$Pref == \"touchpad\"),\n               nrow(dps[dps$Sex == \"M\" & dps$Disability == \"1\",]),p=1/2)\n\nf0&lt;-binom.test(sum(dps[dps$Sex == \"F\" & dps$Disability == \"0\",]$Pref == \"touchpad\"),\n               nrow(dps[dps$Sex == \"F\" & dps$Disability == \"0\",]),p=1/2)\nf1&lt;-binom.test(sum(dps[dps$Sex == \"F\" & dps$Disability == \"1\",]$Pref == \"touchpad\"),\n               nrow(dps[dps$Sex == \"F\" & dps$Disability == \"1\",]),p=1/2)\n\np.adjust(c(m0$p.value, m1$p.value, f0$p.value,f1$p.value), method=\"holm\")\n\n[1] 0.0625000 1.0000000 0.1962891 1.0000000\n\n\n\n\n13.11.2 Handwriting recognition speed between different tools and right-handed vs left-handed people\nThis study examined three handwriting recognizers, A, B, and C and participants who are either right-handed or left-handed. The response is the number of incorrectly recognized handwritten words out of every 100 handwritten words. Examine the data and tell how many participants were involved.\n\nhw &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/hwreco.csv\"))\nhw$Subject&lt;-factor(hw$Subject)\nhw$Recognizer&lt;-factor(hw$Recognizer)\nhw$Hand&lt;-factor(hw$Hand)\nsummary(hw)\n\n    Subject   Recognizer    Hand        Errors     \n 1      : 1   A:17       Left :25   Min.   : 1.00  \n 2      : 1   B:17       Right:25   1st Qu.: 3.00  \n 3      : 1   C:16                  Median : 4.00  \n 4      : 1                         Mean   : 4.38  \n 5      : 1                         3rd Qu.: 6.00  \n 6      : 1                         Max.   :11.00  \n (Other):44                                        \n\n\nCreate an interaction plot of Recognizer on the \\(x\\)-axis and Hand as the traces and tell how many times the traces cross.\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(hw,interaction.plot(Recognizer,Hand,Errors,\n                  ylim=c(0,max(hw$Errors))))\n\n\n\n\nTest whether the Errors of each Recognizer fit a Poisson distribution. First fit the Poisson distribution using fitdist(), then test the fit using gofstat(). The null hypothesis of this test is that the data do not deviate from a Poisson distribution.\n\n#. library(fitdistrplus)\nfit&lt;-fitdist(hw[hw$Recognizer == \"A\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n\nChi-squared statistic:  1.807852 \nDegree of freedom of the Chi-squared distribution:  2 \nChi-squared p-value:  0.4049767 \n   the p-value may be wrong with some theoretical counts &lt; 5  \nChi-squared table:\n     obscounts theocounts\n&lt;= 2  4.000000   3.627277\n&lt;= 3  5.000000   3.168895\n&lt;= 5  4.000000   6.072436\n&gt; 5   4.000000   4.131392\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   75.86792\nBayesian Information Criterion   76.70113\n\nfit&lt;-fitdist(hw[hw$Recognizer == \"B\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n\nChi-squared statistic:  0.9192556 \nDegree of freedom of the Chi-squared distribution:  2 \nChi-squared p-value:  0.6315187 \n   the p-value may be wrong with some theoretical counts &lt; 5  \nChi-squared table:\n     obscounts theocounts\n&lt;= 3  5.000000   3.970124\n&lt;= 5  4.000000   5.800601\n&lt;= 6  3.000000   2.588830\n&gt; 6   5.000000   4.640444\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   78.75600\nBayesian Information Criterion   79.58921\n\nfit&lt;-fitdist(hw[hw$Recognizer == \"C\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n\nChi-squared statistic:  0.3521272 \nDegree of freedom of the Chi-squared distribution:  2 \nChi-squared p-value:  0.8385647 \n   the p-value may be wrong with some theoretical counts &lt; 5  \nChi-squared table:\n     obscounts theocounts\n&lt;= 2  5.000000   4.600874\n&lt;= 3  4.000000   3.347372\n&lt;= 4  3.000000   3.085858\n&gt; 4   4.000000   4.965897\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   70.89042\nBayesian Information Criterion   71.66301\n\n\nNow use Poisson regression to examine Errors by Recommender and Hand. Report the \\(p\\)-value for the Recognizer\\(\\times\\)Hand interaction.\n\n#. library(car)\ncontrasts(hw$Recognizer) &lt;- \"contr.sum\"\ncontrasts(hw$Hand) &lt;- \"contr.sum\"\nm&lt;-glm(Errors ~ Recognizer*Hand, data=hw, family=poisson)\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Errors\n                LR Chisq Df Pr(&gt;Chisq)   \nRecognizer        4.8768  2   0.087299 . \nHand              3.1591  1   0.075504 . \nRecognizer:Hand  12.9682  2   0.001528 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConduct planned comparisons between left and right errors for each recognizer. Using glht() and lsm() will give all comparisons and we only want three so don’t correct for multiple comparisons automatically. That would overcorrect. Instead, extract the three relevant \\(p\\)-values manually and and use p.adjust() to correct for those.\n\n#. library(multcomp) # for glht\n#. library(lsmeans) # for lsm\nsummary(glht(m, lsm(pairwise ~ Recognizer * Hand)),\n        test=adjusted(type=\"none\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: glm(formula = Errors ~ Recognizer * Hand, family = poisson, data = hw)\n\nLinear Hypotheses:\n                         Estimate Std. Error z value Pr(&gt;|z|)    \nA Left - B Left == 0   -8.938e-01  2.611e-01  -3.423 0.000619 ***\nA Left - C Left == 0   -2.231e-01  3.000e-01  -0.744 0.456990    \nA Left - A Right == 0  -8.183e-01  2.638e-01  -3.102 0.001925 ** \nA Left - B Right == 0  -5.306e-01  2.818e-01  -1.883 0.059702 .  \nA Left - C Right == 0  -5.306e-01  2.818e-01  -1.883 0.059702 .  \nB Left - C Left == 0    6.707e-01  2.412e-01   2.780 0.005428 ** \nB Left - A Right == 0   7.551e-02  1.944e-01   0.388 0.697704    \nB Left - B Right == 0   3.632e-01  2.182e-01   1.665 0.095955 .  \nB Left - C Right == 0   3.632e-01  2.182e-01   1.665 0.095955 .  \nC Left - A Right == 0  -5.952e-01  2.441e-01  -2.438 0.014779 *  \nC Left - B Right == 0  -3.075e-01  2.635e-01  -1.167 0.243171    \nC Left - C Right == 0  -3.075e-01  2.635e-01  -1.167 0.243171    \nA Right - B Right == 0  2.877e-01  2.214e-01   1.299 0.193822    \nA Right - C Right == 0  2.877e-01  2.214e-01   1.299 0.193822    \nB Right - C Right == 0  3.331e-16  2.425e-01   0.000 1.000000    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- none method)\n\np.adjust(c(0.001925,0.095955,0.243171),method=\"holm\")\n\n[1] 0.005775 0.191910 0.243171\n\n\nThe above analyses suggest that the error counts were Poisson-distributed. The above analyses suggest that there was a significant Recognizer\\(\\times\\)Hand interaction. The above analyses suggest that for recognizer A, there were significantly more errors for right-handed participants than for left-handed participants.\n\n\n13.11.3 Ease of booking international or domestic flights on three different services\nThis study describes flight bookings using one of three services, Expedia, Orbitz, or Priceline. Each booking was either International or Domestic and the Ease of each interaction was recorded on a 7 point Likert scale where 7 was easiest. Examine the data and determine the number of participants in the study.\n\nbf &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/bookflights.csv\"))\nbf$Subject&lt;-factor(bf$Subject)\nbf$International&lt;-factor(bf$International)\nbf$Website&lt;-factor(bf$Website)\nbf$International&lt;-factor(bf$International)\nbf$Ease&lt;-as.ordered(bf$Ease)\nsummary(bf)\n\n    Subject         Website    International Ease   \n 1      :  1   Expedia  :200   0:300         1: 87  \n 2      :  1   Orbitz   :200   1:300         2: 58  \n 3      :  1   Priceline:200                 3:108  \n 4      :  1                                 4:107  \n 5      :  1                                 5: 95  \n 6      :  1                                 6: 71  \n (Other):594                                 7: 74  \n\n\nDraw an interaction plot with Website on the x-axis and International as the traces. Determine how many times the traces cross.\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(bf,interaction.plot(Website,International,as.numeric(Ease),\n                  ylim=c(0,max(as.numeric(bf$Ease)))))\n\n\n\n\nUse ordinal logistic regression to examine Ease by Website and International. Report the \\(p\\)-value of the Website main effect.\n\n#. library(MASS) # provides polr()\n#. library(car) # provides Anova()\n#. set sum-to-zero contrasts for the Anova call\ncontrasts(bf$Website) &lt;- \"contr.sum\"\ncontrasts(bf$International) &lt;- \"contr.sum\"\nm &lt;- polr(Ease ~ Website*International, data=bf, Hess=TRUE)\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Ease\n                      LR Chisq Df Pr(&gt;Chisq)    \nWebsite                  6.811  2    0.03319 *  \nInternational            0.668  1    0.41383    \nWebsite:International   90.590  2    &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConduct three pairwise comparisons of Ease between domestic and international for each service. Report the largest adjusted \\(p\\)-value. Use the same technique as above where you extracted the relevant unadjusted \\(p\\)-values manually and used p.adjust() to adjust them.\n\nsummary(as.glht(pairs(lsmeans(m, pairwise ~ Website * International))),\n        test=adjusted(type=\"none\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nLinear Hypotheses:\n                                                         Estimate Std. Error\nExpedia International0 - Orbitz International0 == 0       -2.1442     0.2619\nExpedia International0 - Priceline International0 == 0    -0.9351     0.2537\nExpedia International0 - Expedia International1 == 0      -1.6477     0.2570\nExpedia International0 - Orbitz International1 == 0       -0.3217     0.2490\nExpedia International0 - Priceline International1 == 0    -0.7563     0.2517\nOrbitz International0 - Priceline International0 == 0      1.2091     0.2555\nOrbitz International0 - Expedia International1 == 0        0.4965     0.2505\nOrbitz International0 - Orbitz International1 == 0         1.8225     0.2571\nOrbitz International0 - Priceline International1 == 0      1.3879     0.2546\nPriceline International0 - Expedia International1 == 0    -0.7126     0.2518\nPriceline International0 - Orbitz International1 == 0      0.6134     0.2497\nPriceline International0 - Priceline International1 == 0   0.1789     0.2501\nExpedia International1 - Orbitz International1 == 0        1.3260     0.2524\nExpedia International1 - Priceline International1 == 0     0.8914     0.2506\nOrbitz International1 - Priceline International1 == 0     -0.4345     0.2477\n                                                         z value Pr(&gt;|z|)    \nExpedia International0 - Orbitz International0 == 0       -8.189 2.22e-16 ***\nExpedia International0 - Priceline International0 == 0    -3.686 0.000228 ***\nExpedia International0 - Expedia International1 == 0      -6.411 1.44e-10 ***\nExpedia International0 - Orbitz International1 == 0       -1.292 0.196380    \nExpedia International0 - Priceline International1 == 0    -3.004 0.002663 ** \nOrbitz International0 - Priceline International0 == 0      4.732 2.22e-06 ***\nOrbitz International0 - Expedia International1 == 0        1.982 0.047498 *  \nOrbitz International0 - Orbitz International1 == 0         7.089 1.35e-12 ***\nOrbitz International0 - Priceline International1 == 0      5.452 4.99e-08 ***\nPriceline International0 - Expedia International1 == 0    -2.830 0.004659 ** \nPriceline International0 - Orbitz International1 == 0      2.457 0.014023 *  \nPriceline International0 - Priceline International1 == 0   0.715 0.474476    \nExpedia International1 - Orbitz International1 == 0        5.254 1.49e-07 ***\nExpedia International1 - Priceline International1 == 0     3.557 0.000375 ***\nOrbitz International1 - Priceline International1 == 0     -1.754 0.079408 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- none method)\n\np.adjust(c(1.44e-10,1.35e-12,0.474476),method=\"holm\")\n\n[1] 2.88000e-10 4.05000e-12 4.74476e-01\n\n\nThe above analyses indicate a significant main effect of Website on Ease. The above analyses indicate a significant interaction between Website and International. Expedia was perceived as significantly easier for booking international flights than domestic flights. Orbitz, on the other hand, was perceived as significantly easier for booking domestic flights than international flights."
  },
  {
    "objectID": "week13.html#same-person-using-different-tools-within-subjects-studies",
    "href": "week13.html#same-person-using-different-tools-within-subjects-studies",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.12 Same person using different tools (Within subjects studies)",
    "text": "13.12 Same person using different tools (Within subjects studies)\n\n13.12.1 Two search engines compared\n\nws &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/websearch2.csv\"))\n\nHow many subjects took part in this study?\n\nws$Subject&lt;-factor(ws$Subject)\nws$Engine&lt;-factor(ws$Engine)\nsummary(ws)\n\n    Subject      Engine       Order        Searches         Effort    \n 1      : 2   Bing  :30   Min.   :1.0   Min.   : 89.0   Min.   :1.00  \n 2      : 2   Google:30   1st Qu.:1.0   1st Qu.:135.8   1st Qu.:2.00  \n 3      : 2               Median :1.5   Median :156.5   Median :4.00  \n 4      : 2               Mean   :1.5   Mean   :156.9   Mean   :3.90  \n 5      : 2               3rd Qu.:2.0   3rd Qu.:175.2   3rd Qu.:5.25  \n 6      : 2               Max.   :2.0   Max.   :241.0   Max.   :7.00  \n (Other):48                                                           \n\ntail(ws)\n\n# A tibble: 6 × 5\n  Subject Engine Order Searches Effort\n  &lt;fct&gt;   &lt;fct&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 28      Google     2      131      4\n2 28      Bing       1      192      4\n3 29      Google     1      162      5\n4 29      Bing       2      163      3\n5 30      Google     2      146      5\n6 30      Bing       1      137      2\n\n\nThirty subjects participated.\nWhat is the average number of searches for the engine with the largest average number of searches?\n\nws |&gt;\n  group_by(Engine) |&gt;\n  summarize(avg=mean(Searches))\n\n# A tibble: 2 × 2\n  Engine   avg\n  &lt;fct&gt;  &lt;dbl&gt;\n1 Bing    166.\n2 Google  148.\n\n\nBing had 166 searches on average.\nWhat is the \\(p\\)-value (four digits) from a paired-samples \\(t\\)-test of order effect?\n\n#. library(reshape2)\nws.wide.order &lt;- dcast(ws,Subject ~ Order, value.var=\"Searches\")\ntst&lt;-t.test(ws.wide.order$\"1\",ws.wide.order$\"2\",paired=TRUE,var.equal=TRUE)\ntst\n\n\n    Paired t-test\n\ndata:  ws.wide.order$\"1\" and ws.wide.order$\"2\"\nt = 0.34273, df = 29, p-value = 0.7343\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -13.57786  19.04453\nsample estimates:\nmean difference \n       2.733333 \n\n\nThe \\(p\\)-value is 0.7343\nWhat is the \\(t\\)-statistic (two digits) for a paired-samples \\(t\\)-test of Searches by Engine?\n\n#. library(reshape2)\nws.wide.engine &lt;- dcast(ws,Subject ~ Engine, value.var=\"Searches\")\ntst&lt;-t.test(ws.wide.engine$\"Bing\",ws.wide.engine$\"Google\",paired=TRUE,var.equal=TRUE)\ntst\n\n\n    Paired t-test\n\ndata:  ws.wide.engine$Bing and ws.wide.engine$Google\nt = 2.5021, df = 29, p-value = 0.01824\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n  3.310917 32.955750\nsample estimates:\nmean difference \n       18.13333 \n\n\nThe \\(t\\)-statistic is 2.50.\nWhat is the \\(p\\)-value (four digits) from a Wilcoxon signed-rank test on Effort?\n\n#. library(coin)\nwilcoxsign_test(Effort~Engine|Subject,data=ws,distribution=\"exact\")\n\n\n    Exact Wilcoxon-Pratt Signed-Rank Test\n\ndata:  y by x (pos, neg) \n     stratified by block\nZ = 0.68343, p-value = 0.5016\nalternative hypothesis: true mu is not equal to 0\n\n\nThe \\(p\\)-value is 0.5016.\n\n\n13.12.2 Same but with three search engines\n\nws3 &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/websearch3.csv\"))\n\nHow many subjects took part in this study? ::: {.cell}\nsummary(ws3)\n\n    Subject        Engine              Order      Searches         Effort     \n Min.   : 1.0   Length:90          Min.   :1   Min.   : 92.0   Min.   :1.000  \n 1st Qu.: 8.0   Class :character   1st Qu.:1   1st Qu.:139.0   1st Qu.:3.000  \n Median :15.5   Mode  :character   Median :2   Median :161.0   Median :4.000  \n Mean   :15.5                      Mean   :2   Mean   :161.6   Mean   :4.256  \n 3rd Qu.:23.0                      3rd Qu.:3   3rd Qu.:181.8   3rd Qu.:6.000  \n Max.   :30.0                      Max.   :3   Max.   :236.0   Max.   :7.000  \n\nws3$Subject&lt;-as.factor(ws3$Subject)\nws3$Order&lt;-as.factor(ws3$Order)\nws3$Engine&lt;-as.factor(ws3$Engine)\ntail(ws3)\n\n# A tibble: 6 × 5\n  Subject Engine Order Searches Effort\n  &lt;fct&gt;   &lt;fct&gt;  &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 29      Google 3          157      5\n2 29      Bing   1          195      4\n3 29      Yahoo  2          182      5\n4 30      Google 3          152      1\n5 30      Bing   2          188      7\n6 30      Yahoo  1          131      3\n\n:::\nAgain, thirty subjects participated.\nWhat is the average number of searches for the engine with the largest average number of searches?\n\nplyr::ddply(ws3,~ Engine,summarize, Mean=mean(Searches))\n\n  Engine     Mean\n1   Bing 159.8333\n2 Google 152.6667\n3  Yahoo 172.4000\n\n\nYahoo required 172.40 searches on average.\nFind Mauchly’s \\(W\\) criterion (four digits) as a value of violation of sphericity.\n\n#. library(ez)\nm = ezANOVA(dv=Searches, within=Order, wid=Subject, data=ws3)\nm$Mauchly\n\n  Effect         W         p p&lt;.05\n2  Order 0.9416469 0.4309561      \n\n\nMauchly’s \\(W = 0.9416\\), indicating that there is no violation of sphericity.\nConduct the appropriate ANOVA and give the \\(p\\)-value of the \\(F\\)-test (four digits).\n\nm$ANOVA\n\n  Effect DFn DFd        F        p p&lt;.05       ges\n2  Order   2  58 1.159359 0.320849       0.0278629\n\n\nThe relevant \\(p\\)-value is 0.3208.\nConduct a repeated measures ANOVA on Searches by Engine and give the Mauchly’s \\(W\\) criterion (four digits).\n\n#. library(ez)\nm &lt;- ezANOVA(dv=Searches, within=Engine, wid=Subject, data=ws3)\nm$Mauchly\n\n  Effect         W         p p&lt;.05\n2 Engine 0.9420316 0.4334278      \n\n\nMauchly’s \\(W = 0.9420\\), indicating that there is no violation of sphericity.\nConduct the appropriate ANOVA and give the \\(p\\)-value of the \\(F\\)-test (four digits).\n\nm$ANOVA\n\n  Effect DFn DFd        F          p p&lt;.05        ges\n2 Engine   2  58 2.856182 0.06560302       0.06498641\n\n\nThe relevant \\(p\\)-value is 0.0656.\nConduct post-hoc paired sample \\(t\\)-tests among levels of Engine, assuming equal variances and using “holm” to correct for multiple comparisons. What is the smallest \\(p\\)-value (four digits)? ::: {.cell}\n#. library(reshape2)\nws3.wide.engine &lt;- dcast(ws3,Subject~Engine,value.var=\"Searches\")\nbi.go&lt;-t.test(ws3.wide.engine$Bing,ws3.wide.engine$Google,paired=TRUE,var.equal=TRUE)\nbi.ya&lt;-t.test(ws3.wide.engine$Bing,ws3.wide.engine$Yahoo,paired=TRUE,var.equal=TRUE)\ngo.ya&lt;-t.test(ws3.wide.engine$Google,ws3.wide.engine$Yahoo,paired=TRUE,var.equal=TRUE)\np.adjust(c(bi.go$p.value,bi.ya$p.value,go.ya$p.value),method=\"holm\")\n\n[1] 0.37497103 0.37497103 0.05066714\n\n::: The smallest \\(p\\)-value is 0.0507.\nConduct a Friedman (nonparametric) test on Effort. Find the \\(\\chi^2\\) statistic (four digits).\n\n#. library(coin)\nfriedman_test(Effort~Engine|Subject,data=ws3,distribution=\"asymptotic\")\n\n\n    Asymptotic Friedman Test\n\ndata:  Effort by\n     Engine (Bing, Google, Yahoo) \n     stratified by Subject\nchi-squared = 8.0182, df = 2, p-value = 0.01815\n\n\n\\(\\chi^2=8.0182\\)\nConduct post hoc pairwise Wilcoxon signed-rank tests on Effort by Engine with “holm” for multiple comparison correction. Give the smallest \\(p\\)-value (four digits).\n\n#. library(reshape2)\nws3.wide.effort &lt;- dcast(ws3,Subject~Engine,value.var=\"Effort\")\nbi.go&lt;-wilcox.test(ws3.wide.effort$Bing,ws3.wide.effort$Google,paired=TRUE,exact=FALSE)\nbi.ya&lt;-wilcox.test(ws3.wide.effort$Bing,ws3.wide.effort$Yahoo,paired=TRUE,exact=FALSE)\ngo.ya&lt;-wilcox.test(ws3.wide.effort$Google,ws3.wide.effort$Yahoo,paired=TRUE,exact=FALSE)\np.adjust(c(bi.go$p.value,bi.ya$p.value,go.ya$p.value),method=\"holm\")\n\n[1] 0.69319567 0.03085190 0.04533852\n\n\nThe smallest \\(p\\)-value is 0.0309."
  },
  {
    "objectID": "week13.html#experiments-with-people-in-groups-doing-tasks-with-different-tools-mixed-models",
    "href": "week13.html#experiments-with-people-in-groups-doing-tasks-with-different-tools-mixed-models",
    "title": "13  Human Computer Interaction Experiments",
    "section": "13.13 Experiments with people in groups doing tasks with different tools (Mixed models)",
    "text": "13.13 Experiments with people in groups doing tasks with different tools (Mixed models)\nMixed models contain both fixed effects and random effects. Following are linear mixed models and generalized linear mixed models examples. Recall that linear models have normally distributed residuals while generalized linear models may have residuals following other distributions.\n\n13.13.1 Searching to find facts and effort of searching (A linear mixed model)\nLoad websearch3.csv. It describes a test of the number of searches required to find out a hundred facts and the perceived effort of searching. How many subjects participated?\n\nws &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/websearch3.csv\"))\nws&lt;-within(ws,Subject&lt;-factor(Subject))\nws&lt;-within(ws,Order&lt;-factor(Order))\nws&lt;-within(ws,Engine&lt;-factor(Engine))\ntail(ws)\n\n# A tibble: 6 × 5\n  Subject Engine Order Searches Effort\n  &lt;fct&gt;   &lt;fct&gt;  &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 29      Google 3          157      5\n2 29      Bing   1          195      4\n3 29      Yahoo  2          182      5\n4 30      Google 3          152      1\n5 30      Bing   2          188      7\n6 30      Yahoo  1          131      3\n\nsummary(ws)\n\n    Subject      Engine   Order     Searches         Effort     \n 1      : 3   Bing  :30   1:30   Min.   : 92.0   Min.   :1.000  \n 2      : 3   Google:30   2:30   1st Qu.:139.0   1st Qu.:3.000  \n 3      : 3   Yahoo :30   3:30   Median :161.0   Median :4.000  \n 4      : 3                      Mean   :161.6   Mean   :4.256  \n 5      : 3                      3rd Qu.:181.8   3rd Qu.:6.000  \n 6      : 3                      Max.   :236.0   Max.   :7.000  \n (Other):72                                                     \n\n\nWhat was the average number of Search instances for each Engine?\n\nws |&gt;\n  group_by(Engine) |&gt;\n  summarize(median=median(Searches),\n        avg=mean(Searches),\n        sd=sd(Searches))\n\n# A tibble: 3 × 4\n  Engine median   avg    sd\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bing     162.  160.  30.6\n2 Google   152   153.  24.6\n3 Yahoo    170.  172.  37.8\n\nplyr::ddply(ws,~Engine,summarize,avg=mean(Searches))\n\n  Engine      avg\n1   Bing 159.8333\n2 Google 152.6667\n3  Yahoo 172.4000\n\n\nConduct a linear mixed model analysis of variance on Search by Engine and report the \\(p\\)-value.\n\nlibrary(lme4) # for lmer\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\n\nAttaching package: 'lme4'\n\n\nThe following object is masked from 'package:RVAideMemoire':\n\n    dummy\n\nlibrary(lmerTest)\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\n#. library(car) # for Anova\ncontrasts(ws$Engine) &lt;- \"contr.sum\"\nm &lt;- lmer(Searches ~ Engine + (1|Subject), data=ws)\n\nboundary (singular) fit: see help('isSingular')\n\nAnova(m, type=3, test.statistic=\"F\")\n\nAnalysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)\n\nResponse: Searches\n                    F Df Df.res  Pr(&gt;F)    \n(Intercept) 2374.8089  1     29 &lt; 2e-16 ***\nEngine         3.0234  2     58 0.05636 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConduct simultaneous pairwise comparisons among all levels of Engine, despite the previous \\(p\\)-value. Report the adjusted(by Holm’s sequential Bonferroni procedure) \\(p\\)-values.\n\n#. library(multcomp)\nsummary(glht(m, mcp(Engine=\"Tukey\")),\n    test=adjusted(type=\"holm\")) # Tukey means compare all pairs\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lmer(formula = Searches ~ Engine + (1 | Subject), data = ws)\n\nLinear Hypotheses:\n                    Estimate Std. Error z value Pr(&gt;|z|)  \nGoogle - Bing == 0    -7.167      8.124  -0.882   0.3777  \nYahoo - Bing == 0     12.567      8.124   1.547   0.2438  \nYahoo - Google == 0   19.733      8.124   2.429   0.0454 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\n\n\n13.13.2 People judging social media posts after viewing clips (Another linear mixed model)\nThe file socialvalue.csv describes a study of people viewing a positive or negative film clip then going onto social media and judging the value (1 or 0) of the first hundred posts they see. The number of valued posts was recorded. Load the file and tell how many participated.\n\nsv &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/socialvalue.csv\"))\nsv&lt;-within(sv,Subject&lt;-factor(Subject))\nsv&lt;-within(sv,Clip&lt;-factor(Clip))\nsv&lt;-within(sv,Social&lt;-factor(Social))\nsv&lt;-within(sv,ClipOrder&lt;-factor(ClipOrder))\nsv&lt;-within(sv,SocialOrder&lt;-factor(SocialOrder))\nsummary(sv)\n\n    Subject         Clip    ClipOrder      Social   SocialOrder     Valued    \n 1      : 4   negative:32   1:32      Facebook:32   1:32        Min.   :13.0  \n 2      : 4   positive:32   2:32      Twitter :32   2:32        1st Qu.:52.0  \n 3      : 4                                                     Median :56.0  \n 4      : 4                                                     Mean   :57.3  \n 5      : 4                                                     3rd Qu.:67.0  \n 6      : 4                                                     Max.   :95.0  \n (Other):40                                                                   \n\ntail(sv)\n\n# A tibble: 6 × 6\n  Subject Clip     ClipOrder Social   SocialOrder Valued\n  &lt;fct&gt;   &lt;fct&gt;    &lt;fct&gt;     &lt;fct&gt;    &lt;fct&gt;        &lt;dbl&gt;\n1 15      negative 2         Facebook 2               60\n2 15      negative 2         Twitter  1               62\n3 16      positive 2         Facebook 2               34\n4 16      positive 2         Twitter  1               61\n5 16      negative 1         Facebook 1               59\n6 16      negative 1         Twitter  2               70\n\n\nHow many more posts were valued on Facebook than on Twitter after seeing a positive clip?\n\nout&lt;-plyr::ddply(sv,~Clip*Social,summarize,ValuedAvg=mean(Valued))\nout\n\n      Clip   Social ValuedAvg\n1 negative Facebook   46.3125\n2 negative  Twitter   55.5625\n3 positive Facebook   68.7500\n4 positive  Twitter   58.5625\n\n68.75-58.5625\n\n[1] 10.1875\n\n\nConduct a linear mixed model analysis of variance on Valued by Social and Clip. Report the \\(p\\)-value of the interaction effect.\n\n#. library(lme4) # for lmer\n#. library(lmerTest)\n#. library(car) # for Anova\ncontrasts(sv$Social) &lt;- \"contr.sum\"\ncontrasts(sv$Clip) &lt;- \"contr.sum\"\nm &lt;- lmer(Valued ~ (Social * Clip) + (1|Subject), data=sv)\n\nboundary (singular) fit: see help('isSingular')\n\nAnova(m, type=3, test.statistic=\"F\")\n\nAnalysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)\n\nResponse: Valued\n                   F Df Df.res    Pr(&gt;F)    \n(Intercept) 839.2940  1     15 1.392e-14 ***\nSocial        0.0140  1     45  0.906195    \nClip         10.3391  1     45  0.002413 ** \nSocial:Clip   6.0369  1     45  0.017930 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConduct planned pairwise comparisons of how the clips may have influenced judgments about the value of social media. Report whether the number of valued posts differed after seeing a positive versus negative clip.\n\n#. library(multcomp) # for glht\n#. library(emmeans) # for lsm\nsummary(glht(m, lsm(pairwise ~ Social * Clip)),test=adjusted(type=\"none\"))\n\nNote: df set to 45\n\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: lmer(formula = Valued ~ (Social * Clip) + (1 | Subject), data = sv)\n\nLinear Hypotheses:\n                                           Estimate Std. Error t value Pr(&gt;|t|)\nFacebook negative - Twitter negative == 0    -9.250      5.594  -1.654 0.105175\nFacebook negative - Facebook positive == 0  -22.438      5.594  -4.011 0.000225\nFacebook negative - Twitter positive == 0   -12.250      5.594  -2.190 0.033759\nTwitter negative - Facebook positive == 0   -13.188      5.594  -2.357 0.022810\nTwitter negative - Twitter positive == 0     -3.000      5.594  -0.536 0.594397\nFacebook positive - Twitter positive == 0    10.188      5.594   1.821 0.075234\n                                              \nFacebook negative - Twitter negative == 0     \nFacebook negative - Facebook positive == 0 ***\nFacebook negative - Twitter positive == 0  *  \nTwitter negative - Facebook positive == 0  *  \nTwitter negative - Twitter positive == 0      \nFacebook positive - Twitter positive == 0  .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- none method)\n\np.adjust(c(0.00017,0.59374),method=\"holm\")\n\n[1] 0.00034 0.59374\n\n\n\n\n13.13.3 People watching teasers in different orders and judging (Yet another linear mixed model)\nThe file teaser.csv describes a study in which people watched teasers for different genres and reported whether they liked them. Load the file and tell the number of participants.\n\nte &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/teaser.csv\"))\nte&lt;-within(te,Subject&lt;-factor(Subject))\nte&lt;-within(te,Order&lt;-factor(Order))\nte&lt;-within(te,Teaser&lt;-factor(Teaser))\ntail(te)\n\n# A tibble: 6 × 4\n  Subject Teaser   Order Liked\n  &lt;fct&gt;   &lt;fct&gt;    &lt;fct&gt; &lt;dbl&gt;\n1 19      thriller 4         1\n2 20      action   3         1\n3 20      comedy   2         0\n4 20      horror   4         0\n5 20      romance  1         0\n6 20      thriller 5         1\n\npar(pin=c(2.75,1.25),cex=0.5)\nboxplot(Liked~Teaser,data=te)\n\n\n\n\nInvestigate order effects.\n\ncontrasts(te$Order) &lt;- \"contr.sum\"\nm &lt;- glmer(Liked ~ Order + (1|Subject), data=te, family=binomial, nAGQ=0)\n\nboundary (singular) fit: see help('isSingular')\n\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: Liked\n             Chisq Df Pr(&gt;Chisq)\n(Intercept) 1.0392  1     0.3080\nOrder       3.9205  4     0.4169\n\n\nConduct a linear mixed model analysis of variance.\n\ncontrasts(te$Teaser) &lt;- \"contr.sum\"\nm &lt;- glmer(Liked ~ Teaser + (1|Subject), data=te, family=binomial, nAGQ=0)\n\nboundary (singular) fit: see help('isSingular')\n\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: Liked\n             Chisq Df Pr(&gt;Chisq)    \n(Intercept)  1.209  1     0.2715    \nTeaser      26.695  4  2.291e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n#. library(multcomp)\nsummary(glht(m, mcp(Teaser=\"Tukey\")),\n    test=adjusted(type=\"holm\")) # Tukey means compare all pairs\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: glmer(formula = Liked ~ Teaser + (1 | Subject), data = te, family = binomial, \n    nAGQ = 0)\n\nLinear Hypotheses:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \ncomedy - action == 0     -3.7917     1.1361  -3.337 0.006763 ** \nhorror - action == 0     -5.1417     1.2681  -4.054 0.000502 ***\nromance - action == 0    -2.5390     1.1229  -2.261 0.118786    \nthriller - action == 0   -1.5581     1.1684  -1.334 0.389097    \nhorror - comedy == 0     -1.3499     0.8909  -1.515 0.389097    \nromance - comedy == 0     1.2528     0.6682   1.875 0.243191    \nthriller - comedy == 0    2.2336     0.7420   3.010 0.018279 *  \nromance - horror == 0     2.6027     0.8740   2.978 0.018279 *  \nthriller - horror == 0    3.5835     0.9317   3.846 0.001080 ** \nthriller - romance == 0   0.9808     0.7217   1.359 0.389097    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\n\n\n13.13.4 Finding number of unique words used in posts by males and females (A generalized linear mixed model)\nThe file vocab.csv describes a study in which 50 posts by males and females were analyzed for the number of unique words used. Load the file and tell the number of participants.\n\nvo &lt;- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vocab.csv\"))\nvo&lt;-within(vo,Subject&lt;-factor(Subject))\nvo&lt;-within(vo,Sex&lt;-factor(Sex))\nvo&lt;-within(vo,Order&lt;-factor(Order))\nvo&lt;-within(vo,Social&lt;-factor(Social))\ntail(vo)\n\n# A tibble: 6 × 5\n  Subject Sex   Social   Order Vocab\n  &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt; &lt;dbl&gt;\n1 29      M     Facebook 3        46\n2 29      M     Twitter  1        38\n3 29      M     Gplus    2        22\n4 30      F     Facebook 3       103\n5 30      F     Twitter  2        97\n6 30      F     Gplus    1        92\n\n\nCreate an interaction plot and see how often the lines cross.\n\npar(pin=c(2.75,1.25),cex=0.5)\nwith(vo,interaction.plot(Social,Sex,Vocab,ylim=c(0,max(vo$Vocab))))\n\n\n\n\nPerform Kolmogorov-Smirnov goodness-of-fit tests on Vocab for each level of Social using exponential distributions.\n\n#. library(MASS)\nfit = fitdistr(vo[vo$Social == \"Facebook\",]$Vocab, \"exponential\")$estimate\nks.test(vo[vo$Social == \"Facebook\",]$Vocab, \"pexp\", rate=fit[1], exact=TRUE)\n\nWarning in ks.test.default(vo[vo$Social == \"Facebook\", ]$Vocab, \"pexp\", : ties\nshould not be present for the Kolmogorov-Smirnov test\n\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  vo[vo$Social == \"Facebook\", ]$Vocab\nD = 0.17655, p-value = 0.2734\nalternative hypothesis: two-sided\n\nfit = fitdistr(vo[vo$Social == \"Twitter\",]$Vocab, \"exponential\")$estimate\nks.test(vo[vo$Social == \"Twitter\",]$Vocab, \"pexp\", rate=fit[1], exact=TRUE)\n\nWarning in ks.test.default(vo[vo$Social == \"Twitter\", ]$Vocab, \"pexp\", rate =\nfit[1], : ties should not be present for the Kolmogorov-Smirnov test\n\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  vo[vo$Social == \"Twitter\", ]$Vocab\nD = 0.099912, p-value = 0.8966\nalternative hypothesis: two-sided\n\nfit = fitdistr(vo[vo$Social == \"Gplus\",]$Vocab, \"exponential\")$estimate\nks.test(vo[vo$Social == \"Gplus\",]$Vocab, \"pexp\", rate=fit[1], exact=TRUE)\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  vo[vo$Social == \"Gplus\", ]$Vocab\nD = 0.14461, p-value = 0.5111\nalternative hypothesis: two-sided\n\n\nUse a generallized linear mixed model to conduct a test of order effects on Vocab.\n\ncontrasts(vo$Sex) &lt;- \"contr.sum\"\ncontrasts(vo$Order) &lt;- \"contr.sum\"\nm &lt;- glmer(Vocab ~ Sex*Order + (1|Subject), data=vo, family=Gamma(link=\"log\"))\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: Vocab\n                Chisq Df Pr(&gt;Chisq)    \n(Intercept) 1179.0839  1     &lt;2e-16 ***\nSex            1.3687  1     0.2420    \nOrder          0.7001  2     0.7047    \nSex:Order      2.0655  2     0.3560    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConduct a test of Vocab by Sex and Social using a generalized linear mixed model.\n\ncontrasts(vo$Sex) &lt;- \"contr.sum\"\ncontrasts(vo$Social) &lt;- \"contr.sum\"\nm = glmer(Vocab ~ Sex*Social + (1|Subject), data=vo, family=Gamma(link=\"log\"))\nAnova(m, type=3)\n\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: Vocab\n                Chisq Df Pr(&gt;Chisq)    \n(Intercept) 1172.6022  1  &lt; 2.2e-16 ***\nSex            0.7925  1     0.3733    \nSocial        26.2075  2  2.038e-06 ***\nSex:Social     0.3470  2     0.8407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPerform post hoc pairwise comparisons among levels of Social adjusted with Holm’s sequential Bonferroni procedure.\n\n#. library(multcomp)\nsummary(glht(m, mcp(Social=\"Tukey\")),\n    test=adjusted(type=\"holm\")) # Tukey means compare all pairs\n\nWarning in mcp2matrix(model, linfct = linfct): covariate interactions found --\ndefault contrast might be inappropriate\n\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: glmer(formula = Vocab ~ Sex * Social + (1 | Subject), data = vo, \n    family = Gamma(link = \"log\"))\n\nLinear Hypotheses:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \nGplus - Facebook == 0     0.8676     0.2092   4.148 6.72e-05 ***\nTwitter - Facebook == 0  -0.1128     0.2026  -0.556    0.578    \nTwitter - Gplus == 0     -0.9803     0.2071  -4.734 6.60e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\n\n\n13.13.5 Judging search effort among different search engines (Another generalized linear mixed model)\nRecode Effort from websearch3.csv as an ordinal response.\n\nws&lt;-within(ws,Effort&lt;-factor(Effort))\nws&lt;-within(ws,Effort&lt;-ordered(Effort))\nsummary(ws)\n\n    Subject      Engine   Order     Searches     Effort\n 1      : 3   Bing  :30   1:30   Min.   : 92.0   1: 6  \n 2      : 3   Google:30   2:30   1st Qu.:139.0   2: 8  \n 3      : 3   Yahoo :30   3:30   Median :161.0   3:19  \n 4      : 3                      Mean   :161.6   4:16  \n 5      : 3                      3rd Qu.:181.8   5:16  \n 6      : 3                      Max.   :236.0   6:15  \n (Other):72                                      7:10  \n\n\nConduct an ordinal logistic regression to determine Effort by Engine, using a generalized linear mixed model.\n\nlibrary(ordinal) # provides clmm\n\n\nAttaching package: 'ordinal'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n#. library(RVAideMemoire) # provides Anova.clmm\nws2&lt;-data.frame(ws)\nm&lt;-clmm(Effort~Engine + (1|Subject),data=ws2)\nAnova.clmm(m,type=3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Effort\n       LR Chisq Df Pr(&gt;Chisq)  \nEngine    8.102  2     0.0174 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPerform pairwise comparisons of Engine on Effort.\n\npar(pin=c(2.75,1.25),cex=0.5)\nplot(as.numeric(Effort) ~ Engine,data=ws2)\n\n\n\n#. library(lme4)\n#. library(multcomp)\nm &lt;- lmer(as.numeric(Effort)~Engine + (1|Subject), data=ws2)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(glht(m,mcp(Engine=\"Tukey\")),test=adjusted(type=\"holm\"))\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lmer(formula = as.numeric(Effort) ~ Engine + (1 | Subject), data = ws2)\n\nLinear Hypotheses:\n                    Estimate Std. Error z value Pr(&gt;|z|)  \nGoogle - Bing == 0  -0.03333    0.42899  -0.078   0.9381  \nYahoo - Bing == 0    1.10000    0.42899   2.564   0.0247 *\nYahoo - Google == 0  1.13333    0.42899   2.642   0.0247 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ash, Carol. 1993. The Probability Tutoring Book. New York, NY:\nIEEE Press.\n\n\nDiez, David, Mine Çetinkaya-Rundel, and Cristopher D Barr. 2019.\nOpenIntro Statistics, Fourth Edition. self-published. https://openintro.org/os.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning, 2nd Edition.\nSpringer New York.\n\n\nMendenhall, William, and Terry Sincich. 2012. A Second Course in\nStatistics, Regression Analysis, Seventh Edition. Boston, MA, USA:\nPrentice Hall.\n\n\nvan Buuren, Stef. 2018. Flexible Imputation of Missing Data.\nBoca Raton, FL: CRC Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualize, and Model\nData. 2nd ed. O’Reilly Media, Inc. https://r4ds.hadley.nz/."
  }
]