---
title: "Stats: Probability"
author: Mick McQuaid
date: today
bibliography: master.bib
monofont: JetBrainsMono Nerd Font
format:
  revealjs:
    logo: iSchoolLogo.png
    theme: statstheme.scss
    css: style.css
    transition: slide
    background-transition: fade
    preview-links: auto
    controls: true
    controls-layout: bottom-right
    center: true
---

::: {.r-fit-text}
Week FOUR
:::

# Probability is hard!

## Example of probability
Rolling a pair of dice

- You know a general rule
- You want to know a single outcome (or a small set of outcomes)

## Example of stats
A/B testing a website

- You know a single outcome (or a small set of outcomes)
- You want to know a general rule (or an appropriate model)

# In a way, stats and probability are inverses of each other

# Approaches to probability

## Bayesian interpretation
There are two competing schools of thought about what probability is. The Bayesian approach is that probability is *quantified belief or reasonable expectation of the outcomes of events based on a state of knowledge*. This approach is recently taught in graduate schools. It requires a lot of math background and is not widely studied in undergraduate courses. We will not extensively study this approach, but I want you to know that it exists and is rising in academic popularity.

## Frequentist interpretation
Our textbook takes a *frequentist* approach to probability, one of the two main approaches to probability and the one usually taught in undergraduate courses in the USA. This approach models probability of an *outcome* as the number of times the outcome would occur if we observed the *random process* that produced it an infinite number of times. For example, if we flip a fair coin an infinite number of times, it comes up heads half the time, so the probability of heads is 0.5.

# Law of large numbers
This law claims that, as more outcomes are observed, the proportion of outcomes converges to the probability of the outcome. For example, if we flip a fair coin a hundred times, the probability of heads coming up half the time is greater than if we only flip it ten times.

## Example of law of large numbers

```{r}
#. number of flips
num_flips <- 10000

#. flips simulation
coin <- c('heads', 'tails')
flips <- sample(coin, size = num_flips, replace = TRUE)

#. number of heads and tails
freqs <- table(flips)
freqs
heads_freq <- cumsum(flips == 'heads') / 1:num_flips
plot(heads_freq,      # vector
     type = 'l',      # line type
     lwd = 2,         # width of line
     col = 'tomato',  # color of line
     las = 1,         # orientation of tick-mark labels
     ylim = c(0, 1),  # range of y-axis
     xlab = "number of tosses",    # x-axis label
     ylab = "relative frequency")  # y-axis label
abline(h = 0.5, col = 'gray50')
```

# Outcomes---disjoint and otherwise

## Disjoint outcomes
These are outcomes that can not both happen. For example, in the fair coin flipping case, the outcome cannot be both heads and tails. But [the sum of all the disjoint probabilities is always 1]{.emph}.

## Probabilities when outcomes are not disjoint
The textbook uses playing cards to illustrate concepts like *cards that are neither diamonds nor face cards*. You have to familiarize yourself with playing cards to understand these examples. The textbook uses the following Venn diagram to illustrate the above example.

##
![](fiVennDiagramFace+Diamonds.png)

## General addition rule
The textbook gives a general rule for multiple outcomes, whether they are disjoint or not.

If *A* and *B* are any two events, disjoint or not, then the probability that at least one of them
will occur is

$$
P (A\text{ or }B) = P (A) + P (B) − P (A\text{ and }B)
$$

where $P (A\text{ and }B)$ is the probability that both events occur.

## Counting Permutations
Permutations can be thought of as lineups. For instance, suppose you have five people to put in a line. There are five people to choose from to be first in line, then four people remain to be second in line, and so on. You can count this up as $5 \times 4 \times 3 \times 2 \times 1 = 5!$ or five factorial. This holds true for as many objects as you wish to line up.

## Counting Combinations

Combinations can be thought of as committees. There is no order as in a lineup. You're either a member or you're not. Suppose you want to form a committee of five people from among twenty people. It doesn't matter what order they come in so you can't use the factorial method to count them. Another method is shown in @Ash1993. The main result is that, to choose a committee of 5 from among 20 people, use

$$
\binom{20}{5} = \frac{20!}{5!(20-5)!}
$$

This is read as *twenty choose 5*.

## The binomial coefficient
The definition of the binomial coefficient is

$$
\binom{n}{r} = \frac{n!}{r!(n-r)!}
$$

## Rules for combinations

$$
\binom{n}{r}=\binom{n}{n-r}
$$

$$
\binom{n}{1}=n
$$

$$
\binom{n}{n}=\binom{n}{0}=1
$$

This last result is because $0!=1$ by definition.

## Example of a combination

@Ash1993 gives the examples of finding and not finding the Queen of Spades ($Q_s$) in a poker hand. You can think of a poker hand as a committee of 5 cards drawn from 52, so the total number of poker hands is given by $\binom{52}{5}$. Finding hands containing the $Q_s$ amounts to choosing a committee of size four (the remainder of the hand, from among 51 cards (the remainder of the deck. So there are $\binom{51}{4}$ such hands.

$$
P(Q_s)=\dfrac{\binom{51}{4}}{\binom{52}{5}}=\frac{5}{52}
$$

## Simplifying stacked fractions
You need to do this when solving a combination by hand.
$$
\dfrac{\frac{a}{b}}{\frac{c}{d}}=\dfrac{a \cdot d}{b \cdot c}
$$

# Probability distributions

## Probability distribution definition
A probability distribution is a list of the possible
outcomes with corresponding probabilities that satisfies
three rules:

1. The outcomes listed must be disjoint.
2. Each probability must be between 0 and 1.
3. The probabilities must total 1.

## Probability distribution for two fair dice

Francis DiTraglia shows the following example of plotting the probability distribution for rolling two fair dice on his [website](https://ditraglia.com/Econ103Public/Rtutorials/Rtutorial4.html).

```{r}
two.dice <- function() {
  dice <- sample(1:6, size = 2, replace = TRUE)
  return(sum(dice))
}
sims <- replicate(10000, two.dice())
plot(table(sims), xlab = 'Sum', ylab = 'Frequency', main = '10,000 Rolls of 2 Fair Dice')
```

Why is 7 the most likely outcome?

## Terms from set theory
Set theory is a mathematical discipline that uses tools like Venn diagrams to describes sets of objects. We can think of outcomes from random processes as objects, too, with the following terms.

- sample space: the set of all possible outcomes
- event: a particular outcome
- complement of an event: outcomes in the sample space outside a given event or events

## Complementary events
The complement of event $A$ is denoted $A^c$, and $A^c$ represents all outcomes not in $A$. $A$ and $A^c$
are mathematically related:

$$
P (A) + P (A^c) = 1, \text{ i.e., } P (A) = 1 − P (A^c)
$$

## Independence
Just as variables and observations can be independent, random processes can be independent,
too. Two processes are *independent* if knowing the outcome of one provides no useful information
about the outcome of the other. For instance, flipping a coin and rolling a die are two independent
processes---knowing the coin was heads does not help determine the outcome of a die roll. On the
other hand, stock prices usually move up or down together, so they are not independent.

## Multiplication rule for independent processes
If $A$ and $B$ represent events from two different and independent processes, then the probability
that both $A$ and $B$ occur can be calculated as the product of their separate probabilities:

$$
P (A\text{ and }B) = P (A) × P (B)
$$

Similarly, if there are $k$ events $A_1, \ldots, A_k$ from $k$ independent processes, then the probability
they all occur is

$$
P (A_1) × P (A_2) × \cdots × P (A_k)
$$

# Conditional probability

## Conditional probability example
This is where probability gets interesting. Some things depend on other things! The textbook uses a contingency table of the `photos_classify` data frame, which you can download from OpenIntro Stats, to explore this concept.

```{r}
load(paste0(Sys.getenv("STATS_DATA_DIR"),"/photo_classify.rda"))
#str(photo_classify)
addmargins(table(photo_classify))
```

##

We can use the entries in the contingency table to make statements about probability.

- probability that a fashion photo is correctly classified by ML (machine learning): 197/309
- probability that a given photo is about fashion when predicted by ML to be not: 112/1603

## Marginal probability
Marginal probability is the probability in the margins of table (right column and bottom row), e.g., ML predicts fashion photo at all: 219/1822.

## Joint probability
Joint probability is the probabillity of two (or more) things being true, e.g., ML predicts fashion and truth is fashion: 197/1822.
A joint probability would be any of the four interior cells divided by the lower right cell.

## Conditional probability
Conditional probability is the probability of some outcome given the condition of another outcome, such as the probability that ML predicts fashion when the photo is truly about fashion: 197/219.

## Conditional probability's importance
Conditional probability is very important. It's useful to know the general formula for conditional probability.
The conditional probability of outcome $A$ given condition $B$ is computed as the following:

$$
P (A|B) = \frac{P (A\text{ and }B)}{P (B)}
$$

## General multiplication rule
We already saw a specific multiplication rule for independent events. But there is a more general rule, applicable whether independence is true or not.
If $A$ and $B$ represent two outcomes or events, then

$$
P (A\text{ and }B) = P (A|B) × P (B)
$$

It is useful to think of $A$ as the outcome of interest and $B$ as the condition.

## Tree diagrams

```{r, engine = 'tikz',engine.opts=list(extra.preamble=c("\\usetikzlibrary{trees}","\\usepackage{pgfplots,stix,pagecolor}","\\definecolor{qopage}{HTML}{0E2A35}")),echo=FALSE}
% Set the overall layout of the tree
\tikzstyle{level 1}=[level distance=3.5cm, sibling distance=3.5cm]
\tikzstyle{level 2}=[level distance=3.5cm, sibling distance=2cm]

% Define styles for bags and leafs
\tikzstyle{bag} = [text width=4em, text centered]
\tikzstyle{end} = [circle, minimum width=3pt,fill, inner sep=0pt]

% The sloped option gives rotated edge labels. Personally
% I find sloped labels a bit difficult to read. Remove the sloped options
% to get horizontal labels. 
\begin{tikzpicture}[grow=right, sloped]
\node[bag] {Bag 1 $4W, 3B$}
    child {
        node[bag] {Bag 2 $4W, 5B$}        
            child {
                node[end, label=right:
                    {$P(W_1\cap W_2)=\frac{4}{7}\cdot\frac{4}{9}$}] {}
                edge from parent
                node[above] {$W$}
                node[below]  {$\frac{4}{9}$}
            }
            child {
                node[end, label=right:
                    {$P(W_1\cap B_2)=\frac{4}{7}\cdot\frac{5}{9}$}] {}
                edge from parent
                node[above] {$B$}
                node[below]  {$\frac{5}{9}$}
            }
            edge from parent 
            node[above] {$W$}
            node[below]  {$\frac{4}{7}$}
    }
    child {
        node[bag] {Bag 2 $3W, 6B$}        
        child {
                node[end, label=right:
                    {$P(B_1\cap B_2)=\frac{3}{7}\cdot\frac{3}{9}$}] {}
                edge from parent
                node[above] {$B$}
                node[below]  {$\frac{3}{9}$}
            }
            child {
                node[end, label=right:
                    {$P(B_1\cap W_2)=\frac{3}{7}\cdot\frac{6}{9}$}] {}
                edge from parent
                node[above] {$W$}
                node[below]  {$\frac{6}{9}$}
            }
        edge from parent         
            node[above] {$B$}
            node[below]  {$\frac{3}{7}$}
    };
\end{tikzpicture}
```

## Textbook tree diagram

![](fiProbTree.png)

::: {.notes}
This probability tree is from the OpenIntro Stats textbook, page 102.
:::

